{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils.helpers import md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: English (United States)\n",
      "Language: en-US\n",
      "Generated automatically: False\n",
      "Translation available: True\n",
      "-----\n",
      "Language: English (auto-generated)\n",
      "Language: en\n",
      "Generated automatically: True\n",
      "Translation available: True\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "video_id = \"139UPjoq7Kw&t=25s&ab_channel=JaneStreet\"\n",
    "transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n",
    "\n",
    "for transcript in transcript_list:\n",
    "        print(f\"Language: {transcript.language}\")\n",
    "        print(f\"Language: {transcript.language_code}\")\n",
    "        print(f\"Generated automatically: {transcript.is_generated}\")\n",
    "        print(f\"Translation available: {transcript.is_translatable}\")\n",
    "        print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highest priority: manually generated English transcript\n",
    "\n",
    "Secondary: automatically generated English transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: English (United States)\n",
      "Language: en-US\n",
      "Generated automatically: False\n",
      "Translation available: True\n",
      "-----\n",
      "Language: English (auto-generated)\n",
      "Language: en\n",
      "Generated automatically: True\n",
      "Translation available: True\n",
      "-----\n",
      "0.18s - - So it's my great pleasure\n",
      "1.65s - to introduce our speaker\n",
      "tonight, Horace He.\n",
      "4.38s - Horace He graduated from Cornell in 2020,\n",
      "8.1s - and he works at Meta on the PyTorch team,\n",
      "10.62s - specifically at the intersection\n",
      "12.75s - of compilers and machine learning.\n",
      "16.14s - If you've used things like torch.compile,\n",
      "18.66s - which is a thing in PyTorch\n",
      "19.86s - that makes your model goes\n",
      "21.24s - like 2-4x faster in just one line of code.\n",
      "23.79s - Or if you've used FlexAttention,\n",
      "25.44s - which is something that lets researcher\n",
      "27.99s - design fast kernel for attention\n",
      "30.21s - without leaving the world of Python.\n",
      "32.673s - He's the person responsible\n",
      "for both of those things.\n",
      "35.58s - You should also check his\n",
      "blog, which is pretty awesome,\n",
      "37.98s - and in particular the blog post\n",
      "40.207s - \"Making Deep Learning go\n",
      "Brrrr From First Principles.\"\n",
      "43.59s - And without further ado,\n",
      "I'll give it to Horace.\n",
      "46.71s - - Thanks for the intro.\n",
      "47.97s - Today I'm gonna give a talk about\n",
      "49.23s - building machine learning systems\n",
      "50.67s - for a trillion, trillion\n",
      "floating point operations.\n",
      "53.91s - My name is Horace He,\n",
      "54.99s - and I'm on the PyTorch\n",
      "compilers team at Meta.\n",
      "58.83s - So, you know, I think we live\n",
      "in pretty unprecedented times\n",
      "62.19s - in terms of an infrastructure build out,\n",
      "64.56s - which is, I think, you know,\n",
      "65.46s - nicely reflected in NVIDIA's stock price.\n",
      "68.82s - I feel like, you know,\n",
      "basically every month,\n",
      "70.59s - we see like a different headline\n",
      "72.21s - about like a new nuclear power plant\n",
      "73.89s - from like Microsoft, or, you know,\n",
      "75.882s - like a massive 300K GPU\n",
      "data cluster from xAI.\n",
      "79.56s - You know, people like,\n",
      "80.835s - I feel like when Amazon\n",
      "first like built like a,\n",
      "84.09s - or like bought a nuclear power center,\n",
      "85.8s - it was like a big news.\n",
      "87.3s - But now practically like, you know,\n",
      "88.38s - every cool AI company\n",
      "90.06s - has their like own nuclear data center.\n",
      "94.68s - And you know, of course,\n",
      "95.513s - this like kind of really\n",
      "massive (indistinct) build out\n",
      "97.56s - has kind of also resulted in\n",
      "pretty ludicrous fundraisers\n",
      "100.86s - from startups, I think.\n",
      "102.48s - Like, I remember back in like 2016, like,\n",
      "104.077s - you know, if you like\n",
      "made it as a startup,\n",
      "106.62s - it would be like if you were\n",
      "worth a billion dollars, right?\n",
      "109.064s - It was like a unicorn.\n",
      "109.897s - It was like the mark of\n",
      "like really making it\n",
      "111.36s - as a startup beyond your wildest dreams.\n",
      "113.91s - But you know, in 2024, you know,\n",
      "115.32s - you gotta raise a billion\n",
      "dollars just like get started.\n",
      "117.81s - You know, it's like just\n",
      "to play the game, (laughs)\n",
      "120.3s - you need like a billion dollars, you know,\n",
      "122.18s - of which most of it goes\n",
      "to NVIDIA (chuckles).\n",
      "125.186s - (audience laughs)\n",
      "127.89s - And so I think like, it's\n",
      "kind of crazy to think that,\n",
      "131.737s - you know, all of this, like, you know,\n",
      "132.57s - billions of dollars is really just to do\n",
      "134.07s - like absolutely insane amount\n",
      "136.05s - of floating point operations.\n",
      "137.82s - Here's like a nice\n",
      "chart from like Epoch AI\n",
      "140.1s - showing like the growth\n",
      "of compute over time.\n",
      "143.73s - And like, these floating point operations\n",
      "147.24s - are really just like big matmuls\n",
      "148.74s - done like over and over, like, you know,\n",
      "150.99s - for millions of iterations\n",
      "over like months.\n",
      "153.84s - And nowadays,\n",
      "155.07s - like the leading edge\n",
      "models are currently trained\n",
      "157.05s - with about like one E26\n",
      "floating point operations,\n",
      "160.2s - which is approximately 100 trillion\n",
      "164.4s - trillion floating point operations.\n",
      "165.96s - So like a trillion trillion,\n",
      "like a trillion TeraFLOPS\n",
      "170.58s - worth of floating point operations.\n",
      "172.92s - And so, you know, back, you\n",
      "know, another kind of effect\n",
      "175.68s - that you might've noticed\n",
      "is like back prior to 2016,\n",
      "178.74s - if you search on like Hacker news like ML,\n",
      "180.99s - you'd often get like a lot of\n",
      "people asking about, you know,\n",
      "182.73s - the ML family of languages.\n",
      "185.37s - But nowadays, you know, you\n",
      "search ML on a hacker news\n",
      "188.7s - and you get like a very\n",
      "different like type of article.\n",
      "195.06s - And so I think one of the things\n",
      "196.47s - that kind of is missed\n",
      "here is like, you know,\n",
      "198.12s - with all these like billions of dollars\n",
      "199.59s - and like, you know,\n",
      "yottaflop of operations,\n",
      "201.75s - it's kind of easy to\n",
      "forget that, you know,\n",
      "203.34s - like these operations needed\n",
      "to actually run somehow\n",
      "207.03s - on the like machines.\n",
      "209.82s - And so, you know, a modern\n",
      "stack might involve, you know,\n",
      "213.022s - like, you know, call like an LLM API\n",
      "214.65s - and then it's called like\n",
      "PyTorch, like NCCL, Triton, CUDA\n",
      "217.56s - like NVCC, like all these like\n",
      "different layers in the stack\n",
      "220.71s - that like somebody was\n",
      "involved in writing.\n",
      "224.31s - And I'm often reminded of this\n",
      "like XK CD comic, you know,\n",
      "227.97s - showing like the state of like\n",
      "modern digital infrastructure\n",
      "231.06s - and so, you know, you kind of, you know,\n",
      "232.62s - often have forget about\n",
      "all the infrastructure\n",
      "234.33s - that was involved for you\n",
      "to like get where you are,\n",
      "237.54s - but like really we're\n",
      "just kind of building\n",
      "239.04s - like layers on top of layers.\n",
      "243.21s - And so, you know, if you\n",
      "work in a systems, you know,\n",
      "245.13s - like I do and I suspect\n",
      "many of you do, you kind of,\n",
      "247.68s - I think oftentimes think about\n",
      "your work a little bit like,\n",
      "251.13s - like a benevolent dictator of sorts.\n",
      "254.04s - Where kind of you imagine\n",
      "like what you're doing here\n",
      "255.87s - is that like a lot of people\n",
      "build on top of your work.\n",
      "258.36s - And so if you can kind of, you know,\n",
      "259.8s - give like a small amount\n",
      "improvement to, you know,\n",
      "262.08s - like millions of people, you know,\n",
      "263.67s - your small improvements\n",
      "can lead to like, you know,\n",
      "265.65s - significant impacts on the world.\n",
      "268.14s - And so for example, I kind\n",
      "of imagine like Guido,\n",
      "270.87s - with Python, you just like,\n",
      "you know, kind of sits on top\n",
      "272.7s - of this cloud and you know,\n",
      "eventually like gives us like\n",
      "274.62s - NoGil or like faster CPython\n",
      "or like the Walrus Operator\n",
      "278.4s - and I feel like this is\n",
      "kind of oftentimes like\n",
      "280.56s - how I imagine like\n",
      "infrastructure work did.\n",
      "283.56s - And it's kind of a lot of\n",
      "why I got into infra work\n",
      "286.17s - in the first place.\n",
      "288.57s - And so on the other hand, I\n",
      "feel like if you work in ML,\n",
      "291.42s - things can sometimes feel\n",
      "a little bit different.\n",
      "293.37s - I originally came across\n",
      "this a post on threads\n",
      "295.62s - where this person said,\n",
      "you know, my main gripe\n",
      "297.63s - with like working on top of like LM APIs\n",
      "300.15s - is that you're not really like,\n",
      "301.23s - no one is like engineering anything.\n",
      "303.09s - You're just like chanting like a prayer\n",
      "306.09s - to this like manmade demon\n",
      "deity to like do your bidding.\n",
      "309.72s - And this like very similar\n",
      "to this kind of like\n",
      "311.67s - Shoggoth metaphor that has\n",
      "become like very popular\n",
      "314.1s - in deep learning circles.\n",
      "315.99s - Where the idea here is\n",
      "that like we really like\n",
      "317.7s - put all these like matmuls\n",
      "319.317s - and like computation into\n",
      "producing this like very weird\n",
      "322.65s - alien intelligence.\n",
      "324.15s - and then we like kind of\n",
      "like RLHF it, you know,\n",
      "327.66s - and like provide it in this\n",
      "like nice convenient interface\n",
      "330.42s - to people with like, you know, ChatGPT\n",
      "332.7s - or something like that.\n",
      "334.89s - But you know, I think\n",
      "if you think about it,\n",
      "337.29s - I think we're kind of\n",
      "working with Shoggoths\n",
      "339.45s - like all the way down.\n",
      "342.27s - Like even like if you're\n",
      "working like in systems\n",
      "345.72s - and you're not calling like ML models,\n",
      "348.03s - you still have this kinda like, you know,\n",
      "349.23s - massive amount of infrastructure\n",
      "that you presumably\n",
      "351.3s - don't really understand written by people\n",
      "353.55s - you've probably never talked to.\n",
      "355.17s - And like the end result\n",
      "know they try to expose\n",
      "358.14s - is like some kind of a simple interface\n",
      "360.45s - where you can just like import torch\n",
      "361.74s - and then, you know, hopefully\n",
      "run your code across like,\n",
      "363.829s - you know, a 100K GPUs.\n",
      "368.01s - And so I think as a result there are some,\n",
      "370.68s - I think, interesting ways\n",
      "in which I think ML models\n",
      "372.633s - that feel kind of different\n",
      "from regular systems.\n",
      "376.32s - One of those ways is that like ML models\n",
      "380.19s - are extremely simple and as a result\n",
      "383.52s - we have like very high expectations\n",
      "385.35s - from the performance of like the code.\n",
      "388.86s - I think it kind of like\n",
      "trace this all the way back\n",
      "391.08s - to this very nice article\n",
      "called the \"Bitter Lesson,\"\n",
      "393.93s - which I'd really recommend\n",
      "reading if you guys\n",
      "395.4s - haven't come across it before.\n",
      "397.11s - Where their main observation\n",
      "here was that like,\n",
      "399.66s - clever ideas in machine\n",
      "learning have basically\n",
      "402.72s - throughout like its 50 year\n",
      "history always lost out\n",
      "405.63s - to just simple ideas that\n",
      "like scaled really well\n",
      "408.96s - with Moore's law.\n",
      "411.66s - And I'm not really like\n",
      "joking here when I say\n",
      "413.61s - that like machine learning\n",
      "logic is exceedingly simple.\n",
      "416.55s - There's like this cool\n",
      "project from someone\n",
      "418.74s - called like Andre Pathy called llama2.c.\n",
      "421.65s - And basically here he's like implemented\n",
      "424.59s - like llama2 in about like 973 lines of C.\n",
      "428.34s - With like no other dependencies.\n",
      "429.63s - Like, you know, every single loop,\n",
      "431.01s - every single like matmul is\n",
      "just implemented from scratch.\n",
      "434.46s - So with 973 lines, you\n",
      "can't like, you know,\n",
      "436.47s - do it very fast, but it does run\n",
      "439.05s - and I think it does kind\n",
      "of indicate just like\n",
      "440.73s - how fundamentally simple the models\n",
      "443.43s - that like we're spending all\n",
      "this compute end up being.\n",
      "447.63s - And so the end result like\n",
      "449.16s - although like the problem\n",
      "themselves are extremely simple\n",
      "451.98s - and like very like easy\n",
      "to optimize in some sense.\n",
      "457.2s - The expectations are very high\n",
      "458.97s - for how well we can\n",
      "optimize these matmuls.\n",
      "461.49s - So one example here is that\n",
      "like the predominant metric\n",
      "464.04s - for measuring your like model's\n",
      "performance in deep learning\n",
      "467.75s - is called model FLOP utilization.\n",
      "470.1s - And so this is basically\n",
      "the like percentage\n",
      "472.17s - of the theoretical max flops\n",
      "that your GPU is able to do.\n",
      "477.3s - And so if you kind of think about this,\n",
      "478.65s - this is actually like a\n",
      "very absurd metric to hit.\n",
      "481.74s - Like if on a CPU, like you\n",
      "measured any of your code\n",
      "484.14s - by this metric, like\n",
      "you can only hit a 100%\n",
      "486.84s - if like at every single\n",
      "time every single core\n",
      "490.26s - of your CPU is always issuing\n",
      "max width SIMD instructions.\n",
      "494.37s - That's like the only way you\n",
      "can hit a 100% utilization.\n",
      "497.61s - And so, you know, if you take\n",
      "a look at any of the code\n",
      "499.77s - that you guys presumably write,\n",
      "501.96s - like almost no CPU code is\n",
      "like anywhere near like,\n",
      "504.919s - you know, a 100% flop.\n",
      "506.07s - It's probably like way under\n",
      "like 1% most of the time.\n",
      "510.15s - On the other hand, in\n",
      "like machine learning\n",
      "511.71s - for like large scale training,\n",
      "we're typically often hitting\n",
      "514.29s - around like 50% of like the peak flops.\n",
      "517.68s - And this is I think like this\n",
      "is kinda like a indicative\n",
      "522.18s - that like even though the overall problem\n",
      "523.373s - is like very simple, the kind\n",
      "of corresponding difficulty\n",
      "526.32s - just goes into like making\n",
      "your like models hit\n",
      "530.07s - like this, like very high perf barrier.\n",
      "534.75s - Another ind of a interesting observation\n",
      "537.48s - in like machine learning is that,\n",
      "539.76s - that has kind of exacerbated this a bit.\n",
      "541.95s - Is that the field has\n",
      "consolidated significantly\n",
      "544.05s - over the last five to 10 years.\n",
      "546.57s - So one of them is that like, you know,\n",
      "548.25s - like maybe 10 years ago you had\n",
      "like a lot more architecture\n",
      "551.04s - is a lot more variance\n",
      "and like different things\n",
      "552.69s - that people were trying.\n",
      "554.07s - But nowadays people really\n",
      "just like transformers are like\n",
      "557.61s - the dominant architecture for everything.\n",
      "558.96s - Like you have transformers for\n",
      "vision, you have transformers\n",
      "560.94s - for language, you have\n",
      "transformers for like, you know,\n",
      "563.25s - audio, it's kind of like all transformers.\n",
      "566.46s - And the other way like\n",
      "things have kind of changed\n",
      "568.47s - is that instead of like, you\n",
      "know, many different people\n",
      "571.5s - training SOTA models.\n",
      "572.88s - You oftentimes just have a few companies\n",
      "574.89s - that are training SOTA models.\n",
      "576.42s - And so we've kind of gone\n",
      "from a bit of like a monopoly\n",
      "578.82s - where like, you know,\n",
      "previously there was this like,\n",
      "581.271s - you know, one person providing the infra\n",
      "582.57s - and like many people using the infra,\n",
      "584.7s - to in some ways it feels a\n",
      "little bit more like monopsony\n",
      "588.06s - where you have like many\n",
      "people trying to provide infra\n",
      "590.04s - and then you know, only one\n",
      "person's actually training\n",
      "591.96s - the job at the end of the day.\n",
      "597.015s - And so as a kind of\n",
      "result, I think that like\n",
      "600.3s - there's kind of two\n",
      "general ways to think about\n",
      "602.46s - like getting performance\n",
      "from like your systems.\n",
      "605.73s - And so one of the ways\n",
      "generally is basically\n",
      "607.95s - like optimizations is\n",
      "like you have a compiler,\n",
      "609.9s - you like make the\n",
      "compiler faster, you know,\n",
      "612.57s - you improve like the performance\n",
      "613.86s - of everybody using your compiler.\n",
      "616.692s - and so the other way though\n",
      "617.525s - is kind of like programming models.\n",
      "619.89s - And so like this is kind of analogous\n",
      "621.27s - as opposed to like a\n",
      "system whose responsibility\n",
      "623.97s - is to like chop down a tree\n",
      "625.56s - and then you know, you're\n",
      "just like optimizing\n",
      "627.09s - how fast you can chop down the tree.\n",
      "628.86s - And the other alternative is\n",
      "like you're providing tools\n",
      "631.17s - for people to cut down\n",
      "the trees themselves.\n",
      "635.13s - And so to kind of like talk a little bit\n",
      "636.72s - about programming models, I\n",
      "think it's like illustrative\n",
      "640.44s - to kind of talk about\n",
      "how like ML frameworks\n",
      "642.42s - have kind of evolved in\n",
      "terms of what program model\n",
      "644.82s - they've exposed to users.\n",
      "647.04s - So originally like I think like 2010,\n",
      "651.23s - like 2011, 2012, the\n",
      "first like ML framework\n",
      "654.15s - that kind of got a lot of\n",
      "popularity was this framework\n",
      "656.19s - called Caffe.\n",
      "657.33s - And the way you like express\n",
      "neural networks in Caffe\n",
      "659.46s - is like a very declarative nature.\n",
      "662.04s - And by that I mean you like\n",
      "edited a protobuf file I think.\n",
      "665.88s - Where like the protobuf like\n",
      "specified all the things\n",
      "668.04s - you needed like care\n",
      "about your neural network.\n",
      "671.25s - And so as you can imagine, you know,\n",
      "672.42s - programming protobufs is\n",
      "like not very fun, you know,\n",
      "674.82s - there's a lot of things\n",
      "you might want to do\n",
      "677.011s - that you can't do in protobufs.\n",
      "678.36s - and so a like natural\n",
      "net thing that people did\n",
      "681.3s - was kind of these kind of\n",
      "graph builder type APIs.\n",
      "684.15s - And so this is kind of,\n",
      "you know, how TensorFlow 1\n",
      "686.19s - kinda of look like where the\n",
      "idea is that oh, you know,\n",
      "688.59s - like programming like, you\n",
      "know, programming protobufs\n",
      "691.26s - or like no human should need\n",
      "to write protobufs by hand.\n",
      "694.38s - And so ideally, you know,\n",
      "695.73s - you should just like\n",
      "write like a DSL of sorts\n",
      "698.61s - that allows you to generate the protobufs\n",
      "700.74s - like from this DSL.\n",
      "703.47s - However, like even this\n",
      "DSL still kind of has\n",
      "706.02s - like a lot of confusion in it.\n",
      "708.18s - Like it is not super clear\n",
      "that like given this DSL\n",
      "710.31s - like how code actually\n",
      "executes on your GPU.\n",
      "714.06s - And so kind of finally\n",
      "like around 2016 or 2017,\n",
      "717.78s - PyTorch started to become\n",
      "like really successful.\n",
      "720.06s - And kind of the like the feature\n",
      "722.7s - that like was most emblematic\n",
      "of PyTorch was basically\n",
      "726.33s - what was called imperative\n",
      "or eager execution.\n",
      "728.73s - And so what this means is that like,\n",
      "733.86s - or like, yeah, so PyTorch\n",
      "you know, was very successful\n",
      "736.29s - and I think it's like worth talking about\n",
      "737.46s - like why eager execution\n",
      "was so successful.\n",
      "740.31s - And I think that the main\n",
      "reason it was successful,\n",
      "742.05s - it just comes down to like\n",
      "what the programming model\n",
      "744.24s - of the execution looked like.\n",
      "745.6s - Where in a imperative\n",
      "slash like eager execution\n",
      "748.35s - is basically like, you\n",
      "know, you call a function,\n",
      "750.96s - the GPU runs a function\n",
      "752.73s - and then the function finishes\n",
      "and you know, that's it.\n",
      "755.656s - Like, you know, like that's\n",
      "basically all you do.\n",
      "757.35s - you call like torch.matmul\n",
      "and this is basically\n",
      "759.54s - the sequence of operations\n",
      "that happens but you know,\n",
      "762.51s - with kind of like a\n",
      "graph mode type approach\n",
      "765.06s - or you know, where kind\n",
      "of this pilot interjects.\n",
      "767.4s - You first like define the function,\n",
      "769.29s - the function gets converted\n",
      "into some like intermediate IR,\n",
      "772.552s - you know, a bunch of who knows\n",
      "what happens to your function\n",
      "775.86s - and then eventually like the\n",
      "function eventually executes\n",
      "778.53s - on the GPU.\n",
      "780.39s - And like the top one\n",
      "was like a very simple\n",
      "783.6s - execution model for people to understand.\n",
      "786.81s - And I think another thing\n",
      "that's kind of interesting\n",
      "788.58s - to notice about this is\n",
      "that this kind of also just,\n",
      "791.28s - like the top half also kind of describes\n",
      "793.83s - how Python executes.\n",
      "796.56s - And I think this is kind of\n",
      "a pretty illustrative to me\n",
      "798.81s - of like why Python has been so successful\n",
      "801.11s - in machine learning.\n",
      "802.62s - It's basically that like,\n",
      "like a funny statement\n",
      "806.31s - that you can make about Python\n",
      "is that like if you tried\n",
      "808.95s - to like train a model\n",
      "today, like you know,\n",
      "812.25s - I gave you like a day\n",
      "to like train a model,\n",
      "814.68s - it would run faster if\n",
      "you ran it in Python\n",
      "816.51s - compared to doing in C++.\n",
      "818.67s - And you might argue that this\n",
      "is like a unfair comparison.\n",
      "822.78s - Because you know, like you\n",
      "know all the infra like,\n",
      "824.947s - you know, all the\n",
      "frameworks that people built\n",
      "825.9s - are in Python.\n",
      "827.25s - But I think the reason why so many\n",
      "828.78s - of these frameworks have\n",
      "been built in Python\n",
      "830.67s - is that Python is like an exceedingly like\n",
      "833.31s - a simple language.\n",
      "834.63s - And so it's also like\n",
      "a very global language.\n",
      "837.78s - And what I mean by like global\n",
      "is that like it's very easy\n",
      "840.39s - for people to build\n",
      "their own infrastructure\n",
      "842.04s - on top of Python without\n",
      "really needing to fight\n",
      "844.68s - with like anything the\n",
      "Python language does\n",
      "846.93s - because the Python language\n",
      "itself does basically nothing.\n",
      "851.175s - Like it, you know, it doesn't\n",
      "do any optimizations for you.\n",
      "853.38s - It just like, you know, takes\n",
      "your function and runs it.\n",
      "856.98s - And so I think that PyTorch\n",
      "basically historically\n",
      "859.83s - is at like a very similar\n",
      "point in the design space.\n",
      "862.86s - Where PyTorch is like execution\n",
      "model is like so very simple\n",
      "867.93s - and although this like\n",
      "doesn't really give you a lot\n",
      "870.3s - in terms of like, it\n",
      "doesn't like automatically\n",
      "871.95s - do a lot of things for you, it\n",
      "does mean that it's very easy\n",
      "874.71s - for people to like build\n",
      "their own infrastructure\n",
      "878.37s - and frameworks on top of PyTorch.\n",
      "886.523s - I think another important\n",
      "detail to realize\n",
      "890.16s - especially about like PyTorch\n",
      "when it first came out.\n",
      "892.62s - Is that like this kind\n",
      "of unoptimized execution\n",
      "895.74s - didn't actually even sacrifice\n",
      "any performance at all.\n",
      "899.07s - Like, you know when\n",
      "people kind of benchmarked\n",
      "900.45s - PyTorch versus like, you\n",
      "know, TensorFlow at Cafe,\n",
      "903.03s - PyTorch oftentimes wasn't even\n",
      "slower than those frameworks.\n",
      "907.08s - And I think there's like a\n",
      "two main reasons for this.\n",
      "909.45s - So the first reason is\n",
      "that back in the day,\n",
      "911.85s - like you know about like\n",
      "90% plus of your time\n",
      "914.04s - was spent in matmuls.\n",
      "915.3s - And so there's basically nothing\n",
      "else you need to optimize.\n",
      "917.7s - And so matmuls here are\n",
      "like matrix implications\n",
      "920.1s - and so they're often provided\n",
      "by these like vendor libraries\n",
      "922.44s - like cuBLAS or like QDNN.\n",
      "925.289s - And like you know, they're\n",
      "provided by NVIDIA.\n",
      "926.76s - and they're like very hand optimized\n",
      "928.65s - and so, you know, if 90% of\n",
      "your time is spent in matmuls,\n",
      "931.65s - then like what else can you\n",
      "even do to like optimize\n",
      "934.2s - the performance of your neural network?\n",
      "937.92s - And I think another\n",
      "kind of interesting like\n",
      "942.87s - important piece here for like\n",
      "why PyTorch's performance\n",
      "945.15s - was like quite good is\n",
      "that it had this like\n",
      "947.37s - Async execution model.\n",
      "949.5s - Where basically the idea here\n",
      "is that like you kind of have\n",
      "952.23s - like a parallel work queue on your GPU.\n",
      "954.93s - And so what the CPU does is\n",
      "that it's only responsible\n",
      "957.84s - for scheduling work on your work queue\n",
      "959.91s - and then you know, the GPU\n",
      "executes work from the work queue\n",
      "963.36s - and I think of it\n",
      "generally as like this GIF\n",
      "966.21s - is what usually comes to mind\n",
      "and basically you can imagine\n",
      "968.76s - that like the dog is like,\n",
      "970.98s - or Gromit is like Python, you\n",
      "know, they're like, you know,\n",
      "973.56s - trying to put down the train\n",
      "track in front of the train,\n",
      "976.14s - which is the GPU.\n",
      "977.49s - And so, you know, as long\n",
      "as like Gromit is able\n",
      "980.49s - to put down the train\n",
      "tracks faster than the train\n",
      "983.01s - actually rolls along the train tracks,\n",
      "985.11s - you can actually kind of view Python\n",
      "986.88s - as like having zero overhead.\n",
      "988.86s - Like it doesn't provide\n",
      "any extra cost compared to,\n",
      "992.61s - you know, if Python was like\n",
      "in a more efficient language\n",
      "994.68s - like C++.\n",
      "997.65s - And so in this way like you know,\n",
      "999.167s - eager execution not only\n",
      "had like a much easier\n",
      "1001.7s - to understand program model for users,\n",
      "1003.68s - it was also like basically just as fast\n",
      "1006.62s - as like non eager execution.\n",
      "1010.4s - Unfortunately, you know,\n",
      "good things that never last.\n",
      "1013.19s - And in 2017 like NVIDIA introduced\n",
      "1016.13s - what are called like tensor cores.\n",
      "1018.67s - And if you guys are\n",
      "unfamiliar with tensor cores,\n",
      "1020.57s - they're basically like\n",
      "hardware units on the GPUs\n",
      "1023.96s - that only do matmul operations.\n",
      "1026.15s - Like, and I don't mean this\n",
      "like figuratively in the sense\n",
      "1029.87s - that like people often\n",
      "say that GPUs are like\n",
      "1031.49s - well suited for matmuls.\n",
      "1033.05s - I mean this like very literally\n",
      "1034.85s - in that like there's actually\n",
      "an assembly instruction\n",
      "1036.77s - that just does like a mini matmul.\n",
      "1038.99s - And this is how you like\n",
      "interact with the tensor cores.\n",
      "1041.84s - And so if you look at this\n",
      "like plot of like the amount\n",
      "1044.96s - of like matmul flops\n",
      "versus non-matmul flops,\n",
      "1048.14s - you can really see like\n",
      "when NVIDIA realized\n",
      "1051.71s - that like deep learning was a big deal.\n",
      "1053.93s - Because like all, all\n",
      "of a sudden, you know,\n",
      "1055.46s - you kind of had this\n",
      "like massive like 10x gap\n",
      "1057.89s - and so there's like a log scale.\n",
      "1059.57s - And you had this kinda\n",
      "like massive like 10x gap\n",
      "1062.06s - between how fast matmuls were on the GPU\n",
      "1065.15s - and how fast like literally anything else\n",
      "1066.98s - you wanted to run on the GPU was.\n",
      "1071.213s - And so the end result is,\n",
      "you know, previously we said\n",
      "1073.19s - that like, you know, matmuls\n",
      "took like 90% of the time\n",
      "1075.68s - and so if the, if like, you\n",
      "know, NVIDIA has fed up matmuls\n",
      "1079.21s - by like 10x but then\n",
      "everything else like stayed\n",
      "1081.74s - the same amount of speed,\n",
      "1083.0s - then all of a sudden, you\n",
      "know, like you're spending\n",
      "1085.88s - a lot more of your time doing\n",
      "like non-matmul operations.\n",
      "1090.41s - And so as a result we've kind\n",
      "of gotten like ML compilers\n",
      "1095.307s - I think largely due to this change.\n",
      "1097.31s - And so one of the, I think\n",
      "the important details\n",
      "1098.87s - about ML compilers like, you know,\n",
      "1100.61s - in terms of like how they\n",
      "differ from the frameworks\n",
      "1102.44s - that came before is that\n",
      "ML frameworks still keep\n",
      "1105.71s - like the eager programming\n",
      "model and that like the code\n",
      "1108.62s - that you write like\n",
      "logically the program model\n",
      "1110.93s - exposed to users is that\n",
      "you're still just writing\n",
      "1112.82s - Python code and executes\n",
      "like line by line.\n",
      "1115.52s - The only difference now is that instead\n",
      "1116.9s - of actually executing line by line,\n",
      "1118.67s - we kind of captured into\n",
      "like a graph in some manner.\n",
      "1122.51s - And so Torch compile I think\n",
      "actually kind of does this\n",
      "1124.55s - in a pretty interesting way.\n",
      "1126.77s - And that torch compile\n",
      "actually like intercepts\n",
      "1131.03s - at like the Python by\n",
      "code interpreter level.\n",
      "1133.64s - Where Python kind of exposes these APIs\n",
      "1135.53s - where you can kind of insert\n",
      "1136.88s - your like own frame interpreter.\n",
      "1138.8s - And so this looks very much\n",
      "just like a traditional\n",
      "1140.96s - like you know, Git for\n",
      "like any kind of other VM.\n",
      "1143.99s - except this Git is kind of, you know,\n",
      "1145.19s - only meant for like PyTorch programs.\n",
      "1150.08s - And so if you kind of\n",
      "look at like, you know\n",
      "1151.76s - how like things have evolved over time.\n",
      "1154.58s - Originally you kind of had\n",
      "like frameworks like TensorFlow\n",
      "1157.13s - or Caffe before that.\n",
      "1158.72s - Where both the user like\n",
      "program model that users wrote\n",
      "1162.11s - was like a graph builder type attraction.\n",
      "1165.05s - But then the execution programming model\n",
      "1166.49s - was also a graph\n",
      "execution type extraction.\n",
      "1169.49s - And then like after that you\n",
      "kind of had PyTorch, you know,\n",
      "1172.16s - one type like stuff.\n",
      "1174.62s - Where the user program model now switched\n",
      "1176.27s - to like an eager style execution,\n",
      "but the execution program\n",
      "1179.45s - and the execution program model\n",
      "was also eager where like,\n",
      "1182.42s - you know, each operator's\n",
      "executed one at a time.\n",
      "1185.78s - But kind of now finally, you know,\n",
      "1187.07s - modern ML frameworks like,\n",
      "1188.93s - pretty much all ML frameworks nowadays\n",
      "1191.45s - use like an imperative\n",
      "eager programming model.\n",
      "1194.45s - But almost all ML frameworks\n",
      "now also have some way\n",
      "1197.21s - to like capture this program\n",
      "model into a graph of some kind\n",
      "1200.63s - so they can perform optimizations.\n",
      "1202.4s - And so this is like, you\n",
      "know, JAX.git, Tonnegrad,\n",
      "1205.391s - this is like MLX.\n",
      "1206.51s - They kind of all have their\n",
      "like different approaches\n",
      "1208.19s - for capturing the graph to optimize.\n",
      "1215.72s - And so I think next I kinda\n",
      "wanna talk about, you know,\n",
      "1218.03s - we've kind of discussed how\n",
      "we've gotten to ML compilers\n",
      "1221.96s - in the first place.\n",
      "1223.13s - And so I think next I\n",
      "wanna talk about like\n",
      "1224.78s - what ML compilers are\n",
      "actually doing for you\n",
      "1227.51s - and what kind of optimizations\n",
      "they're performing.\n",
      "1231.32s - And so generally speaking,\n",
      "the way I think about\n",
      "1233.66s - deep learning performance\n",
      "or like performance on GPUs\n",
      "1235.94s - in general is that there's\n",
      "basically three things\n",
      "1238.1s - you can be spending your time on.\n",
      "1240.05s - The first one is compute,\n",
      "so this is time on our GPU\n",
      "1243.32s - competing like actual\n",
      "floating point operations.\n",
      "1245.9s - The next one is a memory\n",
      "which is, you know,\n",
      "1248.06s - time spent transferring\n",
      "your tensors within a GPU.\n",
      "1251.63s - So this is like, you know,\n",
      "across various memory subsystems\n",
      "1254.594s - in your GPU.\n",
      "1256.367s - And so finally like overhead,\n",
      "1257.78s - which is like everything\n",
      "else like you know,\n",
      "1259.308s - it's like time your GPU\n",
      "spending idle and so on.\n",
      "1263.12s - And so first we're gonna\n",
      "talk about compute.\n",
      "1265.7s - And so I think to a sum of\n",
      "approximation you can say\n",
      "1269.9s - that all runtime on your\n",
      "GPU is either compute\n",
      "1272.45s - or it's a shuffling data.\n",
      "1274.25s - And that like, you know, data movement\n",
      "1276.95s - is like not like a real operation, right?\n",
      "1280.07s - it's like a no op from like\n",
      "the theoretical point of view.\n",
      "1283.67s - All it's doing is it's\n",
      "moving data from one place\n",
      "1285.74s - where it's convenient to another place\n",
      "1286.88s - where it's convenient.\n",
      "1288.11s - And so basically a\n",
      "floating point operation\n",
      "1289.6s - is like the only real thing a GPU can do.\n",
      "1294.02s - But you can actually, I\n",
      "think simplify this even more\n",
      "1296.3s - and say that in reality actually\n",
      "nowadays like all runtime\n",
      "1299.72s - is either like matmuls or\n",
      "essentially shuffling data.\n",
      "1303.23s - And so this is like, because\n",
      "if you look at the actual like\n",
      "1306.05s - flop chart on like an Edge\n",
      "100 like GPU, you can see here\n",
      "1310.49s - that like the FP32 flops is like\n",
      "1312.86s - you only have 67\n",
      "teraFLOPS of FB32 compute,\n",
      "1316.79s - but you actually have\n",
      "like a 1,00 teraFLOPS\n",
      "1318.95s - of a TF32 compute.\n",
      "1320.93s - Which is basically like\n",
      "matrix multiplication compute.\n",
      "1323.9s - And so what this essentially...\n",
      "1325.61s - And sometimes like what this means\n",
      "1327.23s - is that if you're not doing\n",
      "measurable (indistinct)\n",
      "1329.18s - on your GPU, you're really\n",
      "only like getting like 7%\n",
      "1333.74s - of like your peak FLOP utilization.\n",
      "1336.11s - And so like, you know by like the metric\n",
      "1337.43s - that I mentioned before\n",
      "like model FLOP utilization,\n",
      "1340.01s - even if your GPU was\n",
      "fully occupied doing stuff\n",
      "1342.86s - that wasn't a matmul you\n",
      "could only ever get like 7%\n",
      "1346.478s - of FLOP utilization, which\n",
      "is like much lower than,\n",
      "1348.89s - you know, our theoretical peak.\n",
      "1355.307s - I did have a brief interlude about like,\n",
      "1356.66s - I think an interesting case where like\n",
      "1358.61s - these kind of abstractions\n",
      "do break down even more.\n",
      "1362.42s - And so I do have like a\n",
      "kind of a fun question.\n",
      "1365.42s - Which is like, do the matrix contents\n",
      "1367.07s - affect your matrix\n",
      "multiplication performance?\n",
      "1369.86s - And so I think, you know,\n",
      "if you kind of like,\n",
      "1373.28s - you know, are familiar\n",
      "with like, you know,\n",
      "1374.93s - general performance, there are\n",
      "like a lot of things that...\n",
      "1377.63s - A lot of ways where like data\n",
      "can impact your performance,\n",
      "1380.27s - but in this case matmuls\n",
      "actually avoid a lot of them.\n",
      "1382.73s - So for example, they have\n",
      "identical memory access patterns\n",
      "1385.22s - regardless of like what\n",
      "data is in your tensor,\n",
      "1388.01s - there's like no control\n",
      "flow in the matmul.\n",
      "1391.192s - And the GPU also don't\n",
      "have like the denormals.\n",
      "1393.14s - So like that's like not\n",
      "a possibility as well.\n",
      "1396.53s - So if you like, you know,\n",
      "1397.597s - and in this case where\n",
      "like taking three tensors,\n",
      "1400.28s - we're initializing them\n",
      "with like all zeros.\n",
      "1402.89s - Like a tensor initialized\n",
      "from the Gaussian distribution\n",
      "1406.07s - and then a tensor initialized\n",
      "1407.63s - from like a uniform distribution,\n",
      "1408.89s - which is like from zero to one.\n",
      "1410.45s - And so funnily enough,\n",
      "if you benchmark this,\n",
      "1413.12s - you actually find that there\n",
      "is a performance difference\n",
      "1415.94s - depending on the actual data\n",
      "that is within your tensors.\n",
      "1421.01s - And so there's this tweet from\n",
      "some of you guys might know,\n",
      "1426.03s - a long time ago that really I think\n",
      "1428.27s - like when I first saw this.\n",
      "1429.243s - I actually was very much\n",
      "reminded of this tweet,\n",
      "1431.69s - where, you know, I thought I\n",
      "knew how like a GPU worked,\n",
      "1435.05s - but then I was like very confused about\n",
      "1437.12s - what could possibly be causing\n",
      "1438.56s - these like performance\n",
      "differences like, you know,\n",
      "1441.17s - between the different data.\n",
      "1443.42s - And so the actual cause\n",
      "here is something called\n",
      "1446.27s - like leakage power or dynamic\n",
      "power where I think, you know,\n",
      "1449.42s - most of you're probably\n",
      "familiar that you know,\n",
      "1452.068s - like when a CPU or GPU is under load,\n",
      "1453.95s - it uses more like power\n",
      "1456.2s - and at some point it can\n",
      "like throttle, you know,\n",
      "1458.36s - it's like using the maximum\n",
      "amount of power can use\n",
      "1460.88s - or the max amount of\n",
      "like a heat it's allowed.\n",
      "1464.87s - But the actual thing\n",
      "is that like, you know,\n",
      "1466.19s - this power doesn't just\n",
      "like come from nowhere.\n",
      "1469.43s - It actually largely\n",
      "comes from what's called\n",
      "1471.47s - like dynamic or switching power.\n",
      "1473.87s - And what this means is like\n",
      "every time a transistor\n",
      "1476.21s - like on the GPU switches\n",
      "from like zero to one\n",
      "1479.03s - or like, you know, high\n",
      "to low or low to high,\n",
      "1481.16s - it like loses a little bit of this power.\n",
      "1483.89s - And so the actual like\n",
      "power usage on your GPU\n",
      "1486.35s - is kind of like a like sum\n",
      "across the total amount\n",
      "1489.35s - of like switching that\n",
      "goes on in your GPU.\n",
      "1493.37s - And so this is why like\n",
      "if you're multiplying\n",
      "1495.14s - with all zeros, you can imagine\n",
      "1496.43s - that like your GPU ends up not,\n",
      "1498.08s - like a lot of transistors\n",
      "don't end up switching at all.\n",
      "1500.63s - And so like it doesn't actually\n",
      "consume that much power\n",
      "1503.12s - and it's much less a throttle.\n",
      "1505.64s - And so if you actually like look at this,\n",
      "1508.64s - you can actually get like\n",
      "very different performance\n",
      "1510.44s - for like all these different\n",
      "kind of fund distributions.\n",
      "1512.84s - Like whether it's like, you\n",
      "know, the normal distribution\n",
      "1516.17s - or whether it's like a\n",
      "checkerboard type like pattern\n",
      "1518.87s - or you know, it's like sparse or ternary.\n",
      "1520.88s - And the reason why is just like,\n",
      "1522.56s - it's like this kind of abstract thing\n",
      "1525.02s - where these different patterns\n",
      "lead to like more or less\n",
      "1527.48s - transistor flips and which\n",
      "leads to like more or less\n",
      "1531.35s - of power throttling, which\n",
      "leads to like more or less\n",
      "1533.86s - of performance.\n",
      "1535.28s - And so I remember actually\n",
      "one time somebody had told me\n",
      "1538.19s - a funny story where like\n",
      "they were like training\n",
      "1541.13s - the machine learning model\n",
      "and benchmarking performance.\n",
      "1543.5s - And then they like at some\n",
      "point their model would nad,\n",
      "1547.16s - and then they'd be like wow my performance\n",
      "1548.63s - just got way better (chuckles).\n",
      "1550.82s - And so I like wrote an article about this\n",
      "1553.1s - and they like messaged me and\n",
      "they were like, oh you know,\n",
      "1554.81s - that was very illustrative\n",
      "1557.27s - because you know, I was really confused\n",
      "1558.29s - why my performance\n",
      "would be getting better.\n",
      "1560.21s - But that's why, you know, like\n",
      "if all your tensors are NaN,\n",
      "1563.0s - your transistors also don't\n",
      "need to do a lot of flipping\n",
      "1565.04s - and so you know, you'll\n",
      "measure like a better perf.\n",
      "1569.99s - So that's kind of compute.\n",
      "1571.37s - And so the next thing that\n",
      "your GPU can be spending\n",
      "1574.49s - a lot of your time on is like memory,\n",
      "1576.5s - which is essentially a time\n",
      "spent transferring your tensors\n",
      "1579.05s - within a GPU.\n",
      "1584.54s - And so I think one thing to observe\n",
      "1586.94s - from this kind of empirical plot\n",
      "1589.91s - from a paper on like a data\n",
      "movement is that although like,\n",
      "1595.49s - so this paper kind of\n",
      "breaks down the operations\n",
      "1598.46s - on like I think a Bert type model.\n",
      "1601.04s - Into what it calls like\n",
      "tensor contractions,\n",
      "1602.9s - i.e memory locations.\n",
      "1605.15s - And then like, you know,\n",
      "normalization operations\n",
      "1607.07s - and element wise operations.\n",
      "1608.87s - And so you can see that\n",
      "although major (indistinct)\n",
      "1611.9s - are responsible for like\n",
      "99.8% of your FLOPS,\n",
      "1614.93s - they're only responsible\n",
      "for 61% of your runtime.\n",
      "1617.78s - And so you know where like\n",
      "why are we spending like,\n",
      "1621.247s - you know, 40% of our runtime\n",
      "doing like operations\n",
      "1623.78s - like cumulatively only\n",
      "take 0.2% of our FLOPS?\n",
      "1629.33s - The kind of a key thing\n",
      "here is what's called\n",
      "1632.09s - like a memory bandwidth cost.\n",
      "1634.25s - Where the way I typically\n",
      "think about this,\n",
      "1636.53s - is that like even like, and\n",
      "so here I'm talking like\n",
      "1639.5s - all of your data already lives\n",
      "on the GPU, like, you know,\n",
      "1641.81s - it's like, you know, it's like, you know,\n",
      "1643.43s - occupying your like GPU's VRAM.\n",
      "1648.05s - But the thing is that like your GPU's VRAM\n",
      "1650.36s - is not where it like the\n",
      "compute units are located.\n",
      "1653.54s - And in order to actually\n",
      "do operations on a GPU,\n",
      "1655.76s - you need to move your\n",
      "data from like the VRAM\n",
      "1659.15s - to like where the compute\n",
      "units are located,\n",
      "1662.24s - which is like your SRAM\n",
      "or like compute units.\n",
      "1665.03s - And so usually I kind\n",
      "of think this of like\n",
      "1666.98s - as like a factory where\n",
      "you have like a factory\n",
      "1669.26s - with like not that much space.\n",
      "1670.94s - And then you have a warehouse\n",
      "located like much further away\n",
      "1674.78s - and so you have like a lot\n",
      "more space in your warehouse,\n",
      "1678.2s - but now in order to do any operations\n",
      "1680.0s - like on those like supplies,\n",
      "1683.36s - you need to move them from your warehouse\n",
      "1685.22s - to your factory and then back.\n",
      "1688.28s - And so this cost of\n",
      "like moving data around\n",
      "1689.96s - is called the memory bandwidth cost.\n",
      "1693.17s - And so this is actually like responsible\n",
      "1695.15s - for like a lot of like what your GPU\n",
      "1697.39s - is spending its time doing.\n",
      "1699.23s - Where if you imagine like\n",
      "let's say we do like, you know,\n",
      "1701.42s - three operations like on a GPU,\n",
      "1703.52s - so maybe you're doing like an add,\n",
      "1704.609s - and then like a, you know, RELU,\n",
      "1706.67s - and then like a sign operation.\n",
      "1710.24s - Like you can imagine that\n",
      "what actually happens\n",
      "1711.8s - when you do these operations.\n",
      "1712.82s - Like first the GPU sends the\n",
      "data from like the memory\n",
      "1716.81s - to the compute units and then you know,\n",
      "1718.64s - turns it from a square to a triangle\n",
      "1720.38s - and then it sends it all the way back.\n",
      "1722.33s - And then you know, it sends the triangle\n",
      "1724.04s - from like the memory units\n",
      "to the compute units again\n",
      "1726.53s - where it does like another operation\n",
      "1727.79s - and then sends it all the way back.\n",
      "1729.71s - And then finally, you know,\n",
      "you guys get the idea,\n",
      "1731.27s - it's like sending the\n",
      "circle from the memory units\n",
      "1732.92s - to the compute units again\n",
      "1734.33s - and then it's like sending\n",
      "into all the way back.\n",
      "1736.91s - And so by default whenever\n",
      "you run any operations\n",
      "1739.1s - like in PyTorch like\n",
      "let's say you ran like add\n",
      "1740.9s - and then multiply and then cosine,\n",
      "1743.3s - this is exactly what would\n",
      "be happening on our GPU.\n",
      "1746.81s - And so you might think that\n",
      "this is a very like dumb thing\n",
      "1749.48s - to do and you would be correct.\n",
      "1753.32s - And that, you know, why are\n",
      "we like sending our triangle\n",
      "1755.39s - back from like the\n",
      "factory to the warehouse\n",
      "1757.85s - just to send the data\n",
      "from like the warehouse\n",
      "1759.89s - back to the factory again.\n",
      "1761.54s - And so a very like common\n",
      "operation for GPU compilers to do,\n",
      "1766.55s - and I'd say what I'd actually call\n",
      "1768.41s - like the most important optimization\n",
      "1770.15s - in a deep learning compiler by far,\n",
      "1771.92s - is called operator fusion.\n",
      "1773.6s - And so what an operator fusion\n",
      "does is that instead of like,\n",
      "1776.672s - you know, sending the data\n",
      "back and forth so much.\n",
      "1778.94s - we like do a single GPU kernel\n",
      "where you send the data once\n",
      "1783.29s - to the factory units, you\n",
      "do all of the operations\n",
      "1786.53s - and then you send the data back.\n",
      "1789.92s - Also, notably, this is also like an issue.\n",
      "1793.67s - This optimization is not\n",
      "really something you can do\n",
      "1795.44s - in eager mode, right?\n",
      "1796.64s - Because in eager mode I\n",
      "was, you know, mentioning\n",
      "1798.74s - like the executed model is very simple\n",
      "1800.75s - where you like run our operation\n",
      "1802.55s - and then it executes the operation.\n",
      "1804.14s - And now if we want to\n",
      "do this optimization,\n",
      "1806.66s - that program model is\n",
      "like no longer sufficient.\n",
      "1811.04s - And so there's actually\n",
      "like a lot of different ways\n",
      "1813.05s - to minimize memory movement.\n",
      "1815.27s - Although at the end of the day,\n",
      "1816.44s - like operator fusion is like\n",
      "the most important thing\n",
      "1818.69s - you can do for like a ML compiler.\n",
      "1821.33s - There's actually like a lot of decisions\n",
      "1823.1s - that go into operator fusion\n",
      "that like kind of enable it\n",
      "1826.31s - to be more or less effective.\n",
      "1828.47s - One of the kind of examples here\n",
      "1830.09s - is kind of these like re computation\n",
      "1831.53s - versus reuse trade-offs.\n",
      "1833.15s - If you guys are kind\n",
      "of familiar with maybe\n",
      "1834.65s - like register allocation type settings,\n",
      "1836.6s - you kind of often have a similar issue\n",
      "1839.0s - where like if you have a\n",
      "register you can choose\n",
      "1841.82s - to either like store it in global memory\n",
      "1845.03s - and then load from it later,\n",
      "1846.35s - or you can just choose to like recompute\n",
      "1848.33s - that value from like values\n",
      "that are ready in the registers.\n",
      "1851.96s - And so you kind of have\n",
      "a similar idea here\n",
      "1854.69s - where you oftentimes have cases\n",
      "where by doing some amount\n",
      "1859.19s - of re computation you can\n",
      "significantly reduce your memory.\n",
      "1863.09s - You can significantly reduce\n",
      "your number of memory accesses.\n",
      "1866.18s - And so in this way, like the recomputation\n",
      "1868.85s - can not only like reduce\n",
      "your like peak memory usage,\n",
      "1871.97s - it can also often like improve\n",
      "1873.35s - your actual runtime performance.\n",
      "1877.25s - And so I think one of the things\n",
      "1879.83s - to mention actually about like why.\n",
      "1882.38s - So this observation actually ends up being\n",
      "1883.67s - like quite important for\n",
      "deep learning performance\n",
      "1885.41s - like re computation versus reuse.\n",
      "1887.33s - And I think the reason\n",
      "why is that like the shape\n",
      "1889.58s - of machine learning programs,\n",
      "1891.26s - actually I think it looks quite unusual\n",
      "1893.09s - compared to like your typical\n",
      "program that you might have.\n",
      "1895.97s - Where like it's generally like\n",
      "a kind of a bit of an axiom\n",
      "1899.72s - in like programs that\n",
      "usually most intermediates\n",
      "1901.613s - that you have are very short-lived.\n",
      "1903.68s - So like, you know, your\n",
      "program generally consists\n",
      "1905.66s - of a lot of very short-lived intermediates\n",
      "1907.13s - that you know are created\n",
      "1908.45s - and they're very shortly destroyed.\n",
      "1910.61s - But in machine learning this\n",
      "is actually not the case.\n",
      "1913.34s - Because in machine learning,\n",
      "like the typical like model\n",
      "1916.34s - that you'll execute will\n",
      "first like run the model\n",
      "1919.4s - forward like, you know,\n",
      "layer zero to layer one,\n",
      "1921.59s - to layer two, to layer\n",
      "three, to layer four,\n",
      "1923.84s - and then initially run what's\n",
      "called the backwards pass,\n",
      "1926.39s - which will like run the layers in reverse.\n",
      "1929.27s - So then it'll go from like\n",
      "layer four to layer three,\n",
      "1931.19s - to layer two, to layer\n",
      "one and to layer zero.\n",
      "1933.74s - And in between the forward pass\n",
      "1935.6s - and the backwards pass you need to save\n",
      "1937.58s - what are called like\n",
      "intermediates or activations.\n",
      "1940.97s - And so these are like a lot of, like,\n",
      "1943.79s - you have a lot of them.\n",
      "1945.14s - And they're like often\n",
      "times like, you know,\n",
      "1947.42s - largely responsible for like running into\n",
      "1949.67s - like out of memory type errors.\n",
      "1951.98s - And so I think this is actually\n",
      "1953.39s - like kind of a pretty unusual\n",
      "like program structure\n",
      "1957.2s - in machine learning that's\n",
      "caused like, you know,\n",
      "1959.6s - back propagation and gradient dissent.\n",
      "1965.42s - And so finally, you know, the last thing\n",
      "1966.83s - you can be spending your\n",
      "time on is overhead.\n",
      "1969.23s - Where you can imagine that,\n",
      "1970.1s - you know, if a poor Gromit is not able\n",
      "1972.53s - to put down the train\n",
      "tracks faster than the train\n",
      "1975.44s - can like go on the train tracks,\n",
      "1977.06s - then sometimes a train\n",
      "is gonna be just stuck\n",
      "1979.55s - waiting for him to put down\n",
      "like the next train track.\n",
      "1982.28s - And so here I have like a profile trace\n",
      "1985.49s - where you can see that\n",
      "like the bottom line,\n",
      "1987.41s - which is a GPU trace is largely idle\n",
      "1990.17s - and it's mostly just\n",
      "idle waiting for the CPU\n",
      "1992.12s - to like schedule the next operation.\n",
      "1995.54s - And so there are a lot of like ways\n",
      "1996.83s - to actually address this nowadays.\n",
      "1998.69s - One of the most powerful\n",
      "is called CUDAgraphs,\n",
      "2000.91s - which is like an NVIDIA provided API,\n",
      "2004.214s - but you also have like other\n",
      "approaches like in a compiler\n",
      "2006.31s - for example, like codegenning\n",
      "like a lower overhead wrapper\n",
      "2009.25s - or something like this.\n",
      "2016.922s - So you know, I've talked\n",
      "about like ML compilers like,\n",
      "2021.13s - and you know like you know what\n",
      "they can do to your program\n",
      "2023.02s - and how they can be useful.\n",
      "2024.37s - But I think kind of like\n",
      "an interesting question\n",
      "2027.04s - that you often see like, you\n",
      "know, I talked a lot about\n",
      "2029.2s - how like, you know, we\n",
      "have like, you know,\n",
      "2031.96s - super massive infra build out\n",
      "2033.52s - and the programs are super simple.\n",
      "2035.14s - And we've seen like a lot of consolidation\n",
      "2037.33s - in terms of like what the\n",
      "architecture looks like.\n",
      "2039.58s - And so I think like a\n",
      "reasonable question is like,\n",
      "2041.83s - if you only have like one architecture\n",
      "2043.33s - and you're spending like\n",
      "billions of dollars to train it,\n",
      "2045.73s - why do you even need a compiler?\n",
      "2048.669s - You know, like why can't\n",
      "you just like, you know,\n",
      "2049.96s - assign some group of people\n",
      "to like optimize it by hand\n",
      "2053.44s - instead of like a leveraging a compiler?\n",
      "2058.12s - And I'm gonna say some\n",
      "like kind of or sorry,\n",
      "2062.47s - and the other thing I'll say about this\n",
      "2064.78s - actually is that like in\n",
      "practice a lot of times\n",
      "2067.93s - people do not use compilers\n",
      "like for kind of this reason.\n",
      "2072.1s - And so this section is gonna\n",
      "be talking a little bit about\n",
      "2074.11s - like why I think that's a case.\n",
      "2076.6s - And what are kind of\n",
      "some of the challenges\n",
      "2078.55s - when it comes to like using\n",
      "compilers in this setting.\n",
      "2082.66s - And yeah, so disclaimer, you know,\n",
      "2083.92s - I do really like compilers.\n",
      "2085.48s - I'm gonna say some kind of\n",
      "mean things about compilers\n",
      "2088.02s - in a bit, but you know, as like\n",
      "to establish my credibility,\n",
      "2093.685s - you know, I work on a team\n",
      "called Fighter's Compilers.\n",
      "2096.01s - And so there are like, to be\n",
      "clear like a lot of reasons\n",
      "2098.5s - why compilers can be very useful.\n",
      "2100.57s - In particular this kinda\n",
      "like notion of leverage,\n",
      "2102.85s - like being able to do the\n",
      "optimization once in the compiler\n",
      "2106.12s - and then having everybody be\n",
      "able to like take advantage\n",
      "2108.28s - of it.\n",
      "2109.96s - And also, you know, compilers\n",
      "are also just like very fun\n",
      "2112.36s - to work on.\n",
      "2114.07s - That being said, yeah,\n",
      "I'm gonna introduce you.\n",
      "2117.475s - You know, my new exciting library,\n",
      "2120.16s - Horace's exciting library\n",
      "abbreviated at HEL.\n",
      "2123.7s - And so it has a couple of cool features\n",
      "2125.38s - that you might be interested in.\n",
      "2127.03s - So the first feature it has is\n",
      "that it doesn't always work.\n",
      "2130.63s - And you know, to address that,\n",
      "2133.15s - it also has no documentation about\n",
      "2134.65s - why or when it will work\n",
      "except by reading my library\n",
      "2137.5s - as an implementation.\n",
      "2139.48s - And in exchange for that,\n",
      "when you update the library,\n",
      "2142.33s - it may totally change what code works\n",
      "2143.83s - and what code doesn't work.\n",
      "2145.12s - I.e. like no backwards\n",
      "compatibility or like guarantees\n",
      "2148.6s - on your like, you know, on\n",
      "whether your code works.\n",
      "2153.73s - And so are you interested\n",
      "in using my library?\n",
      "2157.99s - I guess, you know, most\n",
      "people would probably say no.\n",
      "2160.96s - And so I think one thing to note here\n",
      "2162.76s - that like if work means like\n",
      "has a desired performance\n",
      "2165.73s - and is applying the desired optimizations,\n",
      "2168.31s - this is kind of largely describing\n",
      "2169.72s - how compiler optimizations work.\n",
      "2171.58s - And that compiled optimizations\n",
      "don't always work.\n",
      "2175.36s - Often like there's no real documentation\n",
      "2177.85s - on when a compiler optimization\n",
      "will trigger and you know,\n",
      "2181.45s - when you update your compiler\n",
      "2183.37s - it may completely change when it does\n",
      "2186.19s - or does not apply these optimizations.\n",
      "2189.79s - And so there's like\n",
      "very influential article\n",
      "2194.342s - like about this compiler called ISPC.\n",
      "2196.69s - And they have this note here\n",
      "called auto vectorization\n",
      "2199.23s - is not a programming model.\n",
      "2201.22s - Where they note here\n",
      "is that like, you know,\n",
      "2203.38s - the problem with an auto vectorizer,\n",
      "2204.73s - which is kind of like a compiler...\n",
      "2206.08s - Or so the overall like\n",
      "framing of the article\n",
      "2208.36s - is that he wrote this\n",
      "compiler called like ISPC,\n",
      "2211.21s - which you can think of as like CUDA\n",
      "2212.74s - for intel SIMD instructions.\n",
      "2216.25s - And he kind of, you know, is\n",
      "constantly like, you know,\n",
      "2218.46s - as part of the article, he\n",
      "is constantly trying to fight\n",
      "2220.36s - against like the intel compiler team,\n",
      "2222.58s - which is like where he works.\n",
      "2223.87s - With the intel compiler team,\n",
      "2224.98s - wanted to kind of leverage\n",
      "auto vectorization\n",
      "2227.59s - to get vectorization done\n",
      "2228.76s - instead of introducing like\n",
      "a new program model in ISPC.\n",
      "2232.63s - And so he kind of, I think\n",
      "elucidates like what the problem\n",
      "2236.38s - with the auto vectoriser.\n",
      "2237.61s - Which is that the problem\n",
      "with the auto vectoriser\n",
      "2239.17s - is that as long as vectors can fail\n",
      "2241.21s - and it will then if you're programmer\n",
      "2242.89s - that actually cares about\n",
      "what code the compiler\n",
      "2244.75s - generates from your program,\n",
      "2245.95s - you need to deeply understand\n",
      "the auto vectoriser.\n",
      "2250.42s - Then when it fails to vectorize code,\n",
      "2251.92s - you wanna be vectorized, you need\n",
      "2253.54s - to either poke it in the right way\n",
      "2254.68s - or change your program in the right way\n",
      "2256.3s - so that it works for you again.\n",
      "2257.8s - And so this is like a very\n",
      "horrible way to program.\n",
      "2260.29s - And then, you know, if most of you are,\n",
      "2261.64s - or if any of you here are\n",
      "like very like into using\n",
      "2264.76s - SIMD instructions, you\n",
      "probably also do not trust\n",
      "2267.1s - the auto vectorize at all and\n",
      "you're mostly just, you know,\n",
      "2269.335s - writing intrinsics.\n",
      "2272.38s - And so the, you know, with\n",
      "the proper program model,\n",
      "2275.59s - ideally the user is able\n",
      "to like learn what does\n",
      "2278.71s - and does not work without,\n",
      "you know, needing to be tied\n",
      "2281.86s - to the implementation and then, you know,\n",
      "2283.81s - one compiler implements it\n",
      "2284.89s - and then the user can like\n",
      "learn to reliably rely\n",
      "2287.44s - on this optimization without\n",
      "needing to understand\n",
      "2290.08s - the compiler and only\n",
      "needing to understand\n",
      "2292.09s - the programming model.\n",
      "2293.53s - And so one I guess way you can phrase this\n",
      "2296.35s - is that like a compiler optimization\n",
      "2297.7s - that always works is just\n",
      "part of the programming model.\n",
      "2300.67s - Like for example, when you're\n",
      "writing like, you know,\n",
      "2303.01s - SIMD instructions, you\n",
      "know that as SIMD intrinsic\n",
      "2306.94s - will always get mapped\n",
      "to SIMD instruction.\n",
      "2309.04s - And so that's like part\n",
      "of your program model\n",
      "2310.9s - and not really an optimization\n",
      "that the compiler is doing.\n",
      "2316.48s - So I think, you know, to...\n",
      "2319.06s - Like when I see like the\n",
      "people kind of complaining\n",
      "2322.27s - about like Shoggoths\n",
      "and working with them,\n",
      "2325.24s - I kind of am often reminded\n",
      "times of a compiler.\n",
      "2328.24s - We can imagine that like\n",
      "a compiler is oftentimes\n",
      "2330.25s - it's like large piece\n",
      "of code that you know\n",
      "2332.77s - has been worked on by a lot\n",
      "of like very smart people\n",
      "2334.87s - and oftentimes has a lot\n",
      "of like tricky details\n",
      "2337.87s - and implementation details in it.\n",
      "2340.57s - And so ideally when\n",
      "you're doing a compiler,\n",
      "2342.52s - like only the program\n",
      "model ends up being exposed\n",
      "2344.68s - to the user.\n",
      "2345.513s - So like the actual compiler implementation\n",
      "2347.14s - ends up being completely hidden.\n",
      "2348.55s - And so the user only needs to deal\n",
      "2349.78s - with like the nice program model,\n",
      "2351.187s - but it's usually just like the language\n",
      "2352.84s - that the compiler is compiling from.\n",
      "2361.03s - Unfortunately, you know,\n",
      "when compilers fail\n",
      "2363.19s - and like, you know, they\n",
      "don't apply the optimizations\n",
      "2364.9s - that you want them to apply.\n",
      "2366.31s - The kind of entire thing becomes exposed.\n",
      "2368.71s - And so, you know, this\n",
      "kind of applies anytime\n",
      "2370.75s - that you're like you're\n",
      "wrestling with a compiler\n",
      "2373.27s - and you're trying to\n",
      "understand why the compiler\n",
      "2374.83s - like did or did not like inline\n",
      "my code or things like this.\n",
      "2380.62s - And so to give some examples of like cases\n",
      "2382.45s - where like very kind of\n",
      "nuanced details here.\n",
      "2385.99s - Can lead to compilers like having,\n",
      "2387.7s - like it can lead to\n",
      "compilers struggling a lot.\n",
      "2390.1s - One of them here is like\n",
      "numerics and machine learning.\n",
      "2392.95s - Where numerics can be like a kind of a,\n",
      "2395.02s - or like in general floating point,\n",
      "2396.79s - like arithmetic is like\n",
      "a very cursed thing\n",
      "2399.16s - to deal with generally speaking.\n",
      "2400.96s - And it's just gotten\n",
      "even worse with the fact\n",
      "2405.467s - that like everybody in the industry\n",
      "2406.33s - keeps on pushing our data types lower\n",
      "2408.13s - to lower and lower bits\n",
      "where like on the V100\n",
      "2410.95s - they kind of introduce\n",
      "like 16 bit operations\n",
      "2413.73s - is kind of the default.\n",
      "2415.06s - And on H100 they introduced\n",
      "eight bit operations\n",
      "2418.12s - and on the B100, you\n",
      "know, they're now pushing\n",
      "2419.83s - for like four bit operation,\n",
      "like four bit floats.\n",
      "2422.65s - And operations on four bit\n",
      "floating point numbers.\n",
      "2425.41s - I think it's reasonable to question like\n",
      "2427.6s - how is this even a floating\n",
      "point number at this point?\n",
      "2430.69s - But this is kind of what\n",
      "we're typically dealing with.\n",
      "2435.76s - And so as a result of like\n",
      "this kind of like low precision\n",
      "2438.07s - and the fact that numerics\n",
      "end up being so subtle,\n",
      "2440.14s - the algorithms have like very\n",
      "annoying numerical problems\n",
      "2442.63s - to deal with.\n",
      "2443.463s - And I think a good example for me\n",
      "2444.61s - that was like very frustrating\n",
      "was this kind of a NaN\n",
      "2447.64s - in like a FlashAttention implementation.\n",
      "2450.07s - Where the underlying cause\n",
      "here is something called an FMA\n",
      "2453.28s - or like a fuse multiply accumulate.\n",
      "2455.26s - It can actually be like disastrously bad\n",
      "2457.72s - when it comes to your numerics.\n",
      "2460.94s - So for folks who are unfamiliar with FMA,\n",
      "2463.03s - it's basically like it\n",
      "takes like A, B and C\n",
      "2465.28s - and it does like a signal operation\n",
      "2467.02s - that does A times B plus C.\n",
      "2469.75s - And so one of the ways that FMA differs\n",
      "2472.06s - from normal operations is that\n",
      "in addition to being faster,\n",
      "2476.26s - it's also usually computed\n",
      "2477.55s - in what people call like\n",
      "infinite precision internally.\n",
      "2480.37s - And what that means is that\n",
      "like the result of your FMA\n",
      "2483.85s - is like...\n",
      "2487.573s - Like the closest possible representation\n",
      "2489.73s - to like the true value of your FMA.\n",
      "2492.4s - And so this is different from\n",
      "like if you just did this\n",
      "2494.14s - operation separately.\n",
      "2495.64s - Where typically after your\n",
      "like multiply operation,\n",
      "2498.4s - you would have like a rounding term\n",
      "2500.68s - where it becomes a bit different.\n",
      "2503.8s - And so in this particular\n",
      "case we're computing exp\n",
      "2506.71s - of like a_i times B minus a maximum scale.\n",
      "2510.76s - Where max scale is like\n",
      "a singular scaler value.\n",
      "2515.02s - That's a result of taking the\n",
      "maximum across a_i times b.\n",
      "2518.86s - And so the main idea here is that exp\n",
      "2519.993s - is like a very numerically\n",
      "unstable operation.\n",
      "2522.73s - And so we want to make sure that\n",
      "2523.96s - like we're not taking any very large exp\n",
      "2526.6s - and so we're like, we're always,\n",
      "2528.04s - we're subtracting out the maximum value\n",
      "2529.9s - so that all the exponent, like\n",
      "the maximum size exponent,\n",
      "2533.38s - that you can get here is a zero.\n",
      "2536.92s - And you know, the compiler in its wisdom\n",
      "2540.52s - helpfully rewrites this as\n",
      "like FMA of ai b - max scale.\n",
      "2545.17s - Where you know it's smart here,\n",
      "2546.13s - it actually realizes that you know,\n",
      "2548.17s - it can rewrite the plus\n",
      "into like a plus and minus.\n",
      "2552.76s - But then, you know, if you\n",
      "actually look at the numerics\n",
      "2554.833s - that are going on here, max\n",
      "scale is like a rounding\n",
      "2558.1s - of a_i times b.\n",
      "2559.6s - and we actually end up\n",
      "computing this quantity\n",
      "2562.42s - instead of like expon of a_i\n",
      "times b minus the rounded value\n",
      "2567.22s - of a_i times b, which\n",
      "can be disastrously off.\n",
      "2570.76s - And so the end result\n",
      "here is that we NaNed\n",
      "2574.966s - with FMAs turned on and you\n",
      "don't NaN with FMAs turned off.\n",
      "2578.56s - Even though FMAs are like theoretically\n",
      "2581.29s - a precision improving optimization.\n",
      "2584.89s - The underlying cause here to summarize\n",
      "2586.57s - is basically that our numerical properties\n",
      "2589.18s - relied on two separate computations\n",
      "2592.09s - of the same value to be exactly identical.\n",
      "2595.57s - And in this case with FMAs,\n",
      "like you can apply them\n",
      "2598.9s - to only one branch but\n",
      "not the other branch.\n",
      "2601.12s - And this leads to like\n",
      "NaNs that in this case.\n",
      "2607.84s - Another thing that compilers\n",
      "oftentimes struggle with\n",
      "2609.88s - are what are called algebraic rewrites.\n",
      "2612.43s - So if you guys are familiar\n",
      "with this optimization\n",
      "2614.68s - called FlashAttention.\n",
      "2616.12s - Which kind of like fuses\n",
      "attention operators together,\n",
      "2619.63s - it actually relies on this\n",
      "like kind of subtle rewrite,\n",
      "2624.04s - which people often times\n",
      "call like online softmax.\n",
      "2627.22s - Which is that typically\n",
      "softmax is implemented\n",
      "2629.8s - with a couple of like global\n",
      "synchronizes I suppose.\n",
      "2634.78s - But you can actually\n",
      "rewrite softmax in a way\n",
      "2636.88s - that removes one of those\n",
      "global synchronizations.\n",
      "2641.05s - But this requires like\n",
      "some amount of math.\n",
      "2643.21s - It's not very hard math,\n",
      "2644.26s - but like compilers are\n",
      "generally very bad at math\n",
      "2647.11s - and so it's kind of difficult for them\n",
      "2648.58s - to actually come up with\n",
      "this rewrite by themselves.\n",
      "2653.05s - And so I think FlashAttention\n",
      "is like a very good example\n",
      "2656.05s - of like a case where\n",
      "you need to think about\n",
      "2659.59s - the program model you\n",
      "expose for FlashAttention.\n",
      "2662.17s - Where you know, before we\n",
      "had this like fused attention\n",
      "2665.35s - kernel came about, people\n",
      "just typically wrote attention\n",
      "2668.02s - like this, you know.\n",
      "2668.853s - You did like one matmul\n",
      "and then you did a softmax\n",
      "2671.29s - and then you did another matmul.\n",
      "2672.73s - And you know, this was\n",
      "like not super efficient,\n",
      "2674.98s - but it was as efficient as you could do.\n",
      "2676.9s - So people were happy with it.\n",
      "2678.61s - But unfortunately with\n",
      "FlashAttention we now wanna fuse\n",
      "2681.55s - like these three operations\n",
      "into a single kernel.\n",
      "2684.07s - And so like the difficulty\n",
      "is like, you know\n",
      "2687.58s - what API do we expose for FlashAttention?\n",
      "2693.22s - And so one way you might like tackle this\n",
      "2695.55s - is a pattern matching.\n",
      "2696.73s - So you can like, you know, use a compiler,\n",
      "2700.51s - you know, try to find\n",
      "the sequence of matmul\n",
      "2702.222s - sort by matmul.\n",
      "2703.33s - And then, you know, pattern\n",
      "match it into a like single\n",
      "2707.02s - FlashAttention operator.\n",
      "2708.55s - And so this is like a reasonable option.\n",
      "2712.18s - But the issue here is that\n",
      "it becomes very frustrating\n",
      "2714.52s - to debug, like for example,\n",
      "the user might like, you know,\n",
      "2717.58s - change how they write softmax\n",
      "like they might reimplement\n",
      "2720.453s - softmax with their like\n",
      "own softmax implementation.\n",
      "2723.34s - Instead of using your like vendor provided\n",
      "2725.23s - softmax implementation.\n",
      "2726.73s - And then all of a sudden your\n",
      "pattern like no longer applies\n",
      "2729.61s - and you know, they're sad\n",
      "because like the memory\n",
      "2732.46s - suddenly blows up and their\n",
      "code is like 3x slower\n",
      "2735.19s - and so this is like very\n",
      "frustrating for users to deal with.\n",
      "2740.71s - And so instead, you know,\n",
      "what PyTorch did in this case,\n",
      "2743.38s - is that we just said, okay,\n",
      "like these three operators\n",
      "2747.46s - now need to be fused into one operator.\n",
      "2749.38s - We're just gonna directly provide you\n",
      "2750.67s - that like single operator\n",
      "as a like, you know,\n",
      "2753.863s - a single API that you can call.\n",
      "2756.04s - And so this is one way\n",
      "typically that you can deal with\n",
      "2757.9s - with like program models is\n",
      "you can just kind of introduce\n",
      "2760.36s - a slightly high level\n",
      "thing that like does one,\n",
      "2763.12s - a very specific thing for the users,\n",
      "2765.64s - but this is also still\n",
      "kind of frustrating.\n",
      "2769.18s - And you might have some questions about\n",
      "2771.01s - like whether this is\n",
      "actually good enough to do.\n",
      "2773.44s - Because you know, when\n",
      "you consolidate multiple\n",
      "2775.96s - more primitive APIs in a\n",
      "single more monolithic API,\n",
      "2779.2s - you oftentimes run into issues\n",
      "where the monolithic API\n",
      "2781.5s - is now no longer able to\n",
      "represent all the things\n",
      "2784.54s - that users want to do.\n",
      "2787.9s - And so with attention,\n",
      "2788.86s - we do see that people kept on coming out\n",
      "2790.57s - with like new attention\n",
      "variants like, you know,\n",
      "2793.81s - Sliding Window Attention,\n",
      "Alibi, you know, PagedAttention,\n",
      "2796.42s - Neighborhood Attention,\n",
      "all this kind of stuff.\n",
      "2798.7s - Like, you know, you look on Twitter,\n",
      "2800.8s - and like a new four like attention papers\n",
      "2803.2s - come out every single\n",
      "week and as a result,\n",
      "2806.77s - like they kind of fuse the\n",
      "tender kernels that people have,\n",
      "2809.23s - I keep on accumulating\n",
      "like new cords like,\n",
      "2812.62s - you know, FlashAttention,\n",
      "like this one's from like\n",
      "2814.327s - the FlashAttention repo\n",
      "and now has like a dropout,\n",
      "2816.58s - a softmax scale, a causal one,\n",
      "a window size, a soft cap,\n",
      "2820.03s - alibi slopes and so on.\n",
      "2822.01s - And you know, some of\n",
      "these were like added\n",
      "2823.54s - in like the past couple\n",
      "months and you know,\n",
      "2825.61s - they just like keep on adding them.\n",
      "2826.99s - And once you've added\n",
      "like a core to an API\n",
      "2830.8s - or like to a program model,\n",
      "2831.82s - you can't remove the core anymore\n",
      "2833.71s - because now that's like, you know,\n",
      "2834.64s - breaking what users are rely upon.\n",
      "2838.36s - And even worse, like even\n",
      "though users like we've...\n",
      "2840.94s - People have been kind of\n",
      "aggressive about adding new cores,\n",
      "2843.58s - it still doesn't end up being enough.\n",
      "2847.63s - And you have like all sorts of users\n",
      "2848.463s - who are like constantly\n",
      "complaining that, you know,\n",
      "2850.36s - like nobody has implemented FlashAttention\n",
      "2852.49s - for their, you know,\n",
      "pet attention variant.\n",
      "2855.31s - Like there's no flash\n",
      "attention for prefixLM.\n",
      "2858.853s - This bottom one was from like\n",
      "a blog post from somebody\n",
      "2861.94s - who used to be at Google\n",
      "who was like complaining\n",
      "2863.59s - about the ecosystem outside of Google.\n",
      "2866.44s - And so basically the point of here\n",
      "2867.67s - is that like a single monolithic operator\n",
      "2869.38s - is not actually always sufficient.\n",
      "2872.47s - And so we're in a\n",
      "situation where compilers,\n",
      "2874.6s - like we don't really want\n",
      "to rely on a compiler\n",
      "2877.24s - to generate from scratch,\n",
      "2878.59s - but it's also painful to\n",
      "do modifications by hand.\n",
      "2881.23s - And so this might kind of seem\n",
      "like a like no win situation.\n",
      "2884.74s - Where you kind of must\n",
      "choose like one of them,\n",
      "2886.42s - you must choose either\n",
      "like unpredictability\n",
      "2889.24s - and you know, doing it\n",
      "as a compile optimization\n",
      "2891.61s - or you must choose like\n",
      "a single monolithic API\n",
      "2895.3s - that's like difficult\n",
      "for users to like modify.\n",
      "2901.06s - But you know, this is kind of where\n",
      "2902.56s - you kind of can be clever.\n",
      "2904.33s - And come up with like a new program model\n",
      "2906.79s - that wasn't either of the program models\n",
      "2908.47s - that users had before.\n",
      "2909.82s - So like one thing to notice here\n",
      "2911.5s - is that this kind of custom\n",
      "kernel that's difficult\n",
      "2913.24s - for a compiler to generate from scratch\n",
      "2915.04s - can actually be decomposed\n",
      "into like a handwritten\n",
      "2918.22s - slash like complicated\n",
      "FlashAttention kernel.\n",
      "2920.98s - And a bunch of like trivial\n",
      "modifications from users\n",
      "2924.22s - they can actually be like\n",
      "very mechanically generated.\n",
      "2929.35s - And so here we have like an API\n",
      "2931.18s - that we've introduced\n",
      "recently called Flex Attention\n",
      "2933.73s - and I really like FlexAttention.\n",
      "2936.34s - And I think we've seen like\n",
      "very positive reception\n",
      "2938.44s - from the community,\n",
      "including from I think users\n",
      "2941.26s - who like traditionally don't actually like\n",
      "2942.52s - use compilers that much.\n",
      "2943.96s - And so one of the reasons\n",
      "that FlexAttention is liked.\n",
      "2947.26s - Even though like it relies\n",
      "on Torch compile to work\n",
      "2950.11s - is that like it's\n",
      "guaranteed to always result\n",
      "2952.09s - in a single fused detention kernel\n",
      "2953.95s - and it's always guaranteed to have\n",
      "2955.18s - like the same memory properties\n",
      "2956.26s - as a fused attention kernel.\n",
      "2958.15s - And so this means that like the user\n",
      "2959.83s - now has a programming model\n",
      "2961.42s - that they can rely upon where\n",
      "they can kind of program\n",
      "2964.18s - against this programming model\n",
      "2965.53s - and like try a bunch of different variants\n",
      "2967.93s - that all like fit within\n",
      "this programming model.\n",
      "2970.69s - And the user does not need\n",
      "to understand how the,\n",
      "2972.703s - like actual API is\n",
      "implemented under the hood.\n",
      "2976.96s - So to give you guys like\n",
      "a bit of a like peak\n",
      "2981.071s - of how this API looks like.\n",
      "2982.36s - If you guys are familiar with\n",
      "like sliding window attention\n",
      "2984.64s - or causal attention, here\n",
      "you can kind of implement\n",
      "2987.79s - causal attention by checking whether\n",
      "2990.22s - like your query position is like greater\n",
      "2992.23s - than equal to your KV position.\n",
      "2994.09s - And then also checking\n",
      "whether the distance\n",
      "2995.77s - between them is like less\n",
      "than your sliding window size.\n",
      "2998.56s - And then you can just like,\n",
      "and these masks together\n",
      "3001.65s - to get your like sliding\n",
      "window causal attention.\n",
      "3005.67s - And so the way we kind\n",
      "of think about like,\n",
      "3007.922s - and so this is I think a good example\n",
      "3009.09s - of where compilers can\n",
      "provide a lot of value\n",
      "3012.69s - for users even in these\n",
      "like super like handcrafted\n",
      "3015.87s - scenarios where like previously prior\n",
      "3018.48s - to this attention API, you\n",
      "kind of have this situation\n",
      "3022.74s - where you had a bunch of these like,\n",
      "3024.481s - you had this like big cube of\n",
      "like, you know, like masking\n",
      "3027.45s - or like positional biases or\n",
      "whether it supported training\n",
      "3030.72s - or whether it was supported inference.\n",
      "3032.52s - And you know, a bunch of\n",
      "these dots were like filled in\n",
      "3034.59s - with users who had manually\n",
      "implemented attention kernels.\n",
      "3037.56s - But now with FlexAttention\n",
      "every single one of these\n",
      "3040.5s - like dots are like now filled in.\n",
      "3042.48s - And so it's now like\n",
      "consistently users can rely\n",
      "3045.96s - upon fuse attention kernels\n",
      "3047.7s - regardless of like the tension\n",
      "variant that they're using.\n",
      "3051.6s - And it gives them an\n",
      "example of like some stuff\n",
      "3053.28s - that users have been doing\n",
      "with a FlexAttention.\n",
      "3056.01s - You have on the left, like a\n",
      "bit of a fun attention mask\n",
      "3058.77s - from like some guy on Twitter\n",
      "3061.89s - where basically in this\n",
      "case he had like a bunch\n",
      "3063.81s - of like molecular graphs\n",
      "of like different sizes.\n",
      "3069.9s - And he was able to convert this\n",
      "into like an intention mask\n",
      "3073.11s - that FlexAttention supported.\n",
      "3075.63s - And so this is like a\n",
      "very weird mask that,\n",
      "3078.232s - you know, I definitely did not think about\n",
      "3079.53s - when developing FlexAttention.\n",
      "3081.21s - But when you've developed like\n",
      "I think a good program model\n",
      "3083.49s - for users, users are\n",
      "oftentimes able to do things\n",
      "3085.567s - that like you never considered doing\n",
      "3088.29s - when you developed abstraction.\n",
      "3093.81s - Another kind of analogy\n",
      "that I sometimes think about\n",
      "3096.96s - when it comes to like,\n",
      "3097.83s - you know, optimizations\n",
      "versus program models.\n",
      "3100.35s - Is there's kind of like a famous,\n",
      "3102.12s - I think quote from Grodicke,\n",
      "about like, you know,\n",
      "3104.91s - tackling math problems where he said\n",
      "3107.13s - that, you know, when you\n",
      "tackle like a math problem,\n",
      "3108.763s - you know, you imagine a math\n",
      "problem as like a walnut.\n",
      "3110.76s - And it is like, you know,\n",
      "when you're trying to open\n",
      "3112.32s - the walnut, you can either tackle it\n",
      "3113.58s - by like just hitting the walnut a bunch\n",
      "3115.86s - and like opening it up or\n",
      "you can kind of, you know,\n",
      "3118.5s - like soak the walnut in water\n",
      "3120.36s - and you know, kind of, you know,\n",
      "3121.65s - like establish like new like mental models\n",
      "3124.65s - for how to think about the\n",
      "problem until eventually,\n",
      "3126.81s - like the walnut just opens by itself\n",
      "3128.52s - after being soaked in water.\n",
      "3130.59s - And so I think about program\n",
      "models like very similarly.\n",
      "3134.7s - Where like, you know, if you think about\n",
      "3136.2s - like auto vectorization from\n",
      "like the Intel compiler folks\n",
      "3139.23s - perspective, you know, he\n",
      "had this kind of fun anecdote\n",
      "3141.99s - where like he said that the\n",
      "Intel people kept on asking\n",
      "3144.12s - like, what happens when the CUDA compiler\n",
      "3146.07s - fails to vectorize?\n",
      "3147.36s - And he was like, you\n",
      "know, absolutely baffled\n",
      "3148.74s - about, you know, what happened.\n",
      "3149.91s - Like he felt like this was just a thing\n",
      "3152.01s - he needed to understand to like, you know,\n",
      "3153.84s - understand, you know, how the Intel people\n",
      "3155.61s - needed to change their compiler\n",
      "to be more competitive.\n",
      "3158.7s - But the kind of like\n",
      "misunderstanding here is that like,\n",
      "3161.57s - it is almost a nonsensical question\n",
      "3163.32s - to even ask like when\n",
      "does a CUDA compiler fail\n",
      "3165.96s - to like make your code parallel?\n",
      "3168.12s - Because if you've\n",
      "written your code in CUDA\n",
      "3169.89s - and in the CUDA programming model,\n",
      "3171.45s - it like must be parallel.\n",
      "3173.97s - Like it is kind of like actionmatic\n",
      "3175.77s - part of the program model.\n",
      "3177.21s - Like there's no way you\n",
      "can write a CUDA program\n",
      "3179.55s - that does not execute\n",
      "across multiple cores.\n",
      "3184.65s - Like you can write CUDA programs\n",
      "3186.54s - that are like incorrect or deadlock,\n",
      "3188.76s - but they're still guaranteed to always run\n",
      "3190.2s - across multiple cores.\n",
      "3191.58s - And this is because the\n",
      "parallelism is inherent\n",
      "3194.1s - to the program modeling in\n",
      "CUDA while the parallelism\n",
      "3196.71s - is not inherent to the program\n",
      "model of auto vectorization.\n",
      "3205.92s - I think, yeah, I think\n",
      "kind of ML compilers\n",
      "3211.26s - and kind of, you know, how\n",
      "they apply to like, you know,\n",
      "3214.11s - just generally optimizing\n",
      "GPUs I think like is already\n",
      "3217.92s - quite difficult, but you\n",
      "have a lot more issues\n",
      "3220.41s - when it comes to like\n",
      "making GPUs run at scale.\n",
      "3223.32s - And so when you come to\n",
      "like, you know, getting GPUs\n",
      "3225.41s - to run a scale, I do\n",
      "think there are a couple\n",
      "3227.61s - of like kind of interesting differences\n",
      "3229.02s - between distributed ML programs\n",
      "3231.66s - versus like traditional\n",
      "distributed systems.\n",
      "3235.2s - And so one of the differences\n",
      "3236.16s - is that traditional distributed systems\n",
      "3237.99s - are oftentimes are just\n",
      "trying to scale a QPS.\n",
      "3240.87s - And so when you're like\n",
      "trying to scale QPS,\n",
      "3242.367s - you have a lot of like very\n",
      "small interchangeable queries\n",
      "3245.61s - and like the performance per hardware\n",
      "3247.35s - is not like that critical.\n",
      "3249.57s - And then oftentimes you're\n",
      "willing to like double\n",
      "3251.31s - your amount of hardware use\n",
      "3252.69s - in order to get like fall tolerance\n",
      "3254.28s - or like, you know, like higher\n",
      "uptime or things like this.\n",
      "3257.37s - On the other hand, in ML systems,\n",
      "3259.92s - we just basically have\n",
      "like a single query.\n",
      "3262.26s - Like, you know, you're just\n",
      "like training your ML job.\n",
      "3264.78s - We have frequent global synchronizations\n",
      "3266.58s - across all of our hardware and performance\n",
      "3270.18s - is like extremely important,\n",
      "3271.26s - you know, so much so\n",
      "that like there's no way\n",
      "3273.0s - we could tolerate like\n",
      "a 2x loss in performance\n",
      "3276.18s - for basically any reason.\n",
      "3280.92s - And so one way to kind of think about\n",
      "3282.93s - how you can paralyze programs.\n",
      "3284.79s - And this is actually\n",
      "like not specific to ML,\n",
      "3286.71s - this is kind of a general,\n",
      "3287.88s - like a way to think about\n",
      "paralyzing programs.\n",
      "3290.13s - Is that you can think\n",
      "that there's basically a,\n",
      "3292.05s - like you have like this\n",
      "cube of computation to do.\n",
      "3296.01s - Where one of the dimensions\n",
      "is like the batch dimension.\n",
      "3300.45s - So it's like different\n",
      "tasks that you can perform.\n",
      "3303.06s - You also have like the\n",
      "within task dimension,\n",
      "3305.34s - so like the different\n",
      "operations within a single task.\n",
      "3308.4s - And then finally you have\n",
      "like the time dimension,\n",
      "3310.59s - which is what your GPU is doing\n",
      "at any given point in time.\n",
      "3316.11s - And so you can think of data parallelism\n",
      "3317.34s - as basically just splitting\n",
      "along like the batch dimension.\n",
      "3320.61s - You can think about like task parallelism\n",
      "3322.35s - as splitting across a task dimension.\n",
      "3324.51s - And then you think about\n",
      "pipeline parallelism\n",
      "3325.77s - as kind of like splitting\n",
      "across the time dimension.\n",
      "3331.38s - Yeah, and so for data parallelism,\n",
      "3333.93s - like the basic idea I think\n",
      "is like pretty simple,\n",
      "3336.57s - which is just that we have like\n",
      "a bunch of different tasks.\n",
      "3338.76s - We have a bunch of parallelism,\n",
      "3340.17s - and so each GPU just\n",
      "handles a different task.\n",
      "3342.99s - And so because of that\n",
      "we just have like...\n",
      "3345.56s - We just put one task on\n",
      "each GPU and then run it.\n",
      "3348.48s - And this seems like very\n",
      "nice and very trivial,\n",
      "3351.0s - but the issue is that like we\n",
      "have a massive synchronization\n",
      "3353.52s - of parameters after every step.\n",
      "3355.62s - Because after every step\n",
      "that we do like the gradients\n",
      "3359.43s - need to be like synchronized\n",
      "across all of your hardware.\n",
      "3363.06s - And you can't just like\n",
      "avoid synchronizing\n",
      "3366.63s - your parameters because you\n",
      "also have this kind of like,\n",
      "3371.37s - mathy constraint from\n",
      "like the ML training side,\n",
      "3374.49s - which is that you simply can't\n",
      "train with like too large\n",
      "3376.56s - of a batch size or your model\n",
      "will not converge properly.\n",
      "3382.5s - There's also kind of another detail here\n",
      "3384.33s - where like you can't just like naively\n",
      "3385.74s - replicate your parameters.\n",
      "3387.21s - People oftentimes use like what's called\n",
      "3389.28s - a Fully-Sharded Data Parallel.\n",
      "3393.21s - The second kind of parallelism\n",
      "3394.043s - that you have is like task parallelism,\n",
      "3396.27s - which is commonly known\n",
      "as tensor parallelism.\n",
      "3398.22s - And in this case you\n",
      "basically just have two GPUs\n",
      "3400.26s - that split the same task.\n",
      "3402.06s - And so the main problem\n",
      "that you run into here\n",
      "3403.83s - that's kind of specific to ML\n",
      "is that like task parallelism\n",
      "3407.1s - because you're like running the same task\n",
      "3409.08s - does not oftentimes have like obvious ways\n",
      "3411.84s - to overlap your communication\n",
      "with your computation.\n",
      "3415.26s - But nevertheless, because the\n",
      "performance is so important,\n",
      "3418.2s - we still really want to\n",
      "overlap the communication.\n",
      "3420.87s - And so there ends up being like a lot\n",
      "3422.16s - of kind of involved things that people do\n",
      "3424.86s - to try to improve the overlap.\n",
      "3427.02s - One of them here is called\n",
      "like Async Tensor Parallelism.\n",
      "3430.2s - Where the general idea here\n",
      "3431.25s - is they have like a communication op\n",
      "3433.05s - followed by like a computation operation.\n",
      "3435.27s - And so usually, you know,\n",
      "3436.17s - while you're doing the communication,\n",
      "3437.82s - the CPU can't do any computation\n",
      "3439.71s - and so this is like, you know,\n",
      "wasted idle time on our GPU.\n",
      "3444.06s - But the observation here\n",
      "about Async Tensor Parallelism\n",
      "3446.84s - is you can actually kind\n",
      "of like mini pipeline\n",
      "3449.82s - your like communication and your matmul.\n",
      "3452.52s - Where even within like a\n",
      "single tensor operation,\n",
      "3455.4s - there's still oftentimes\n",
      "like a batch dimension\n",
      "3457.8s - that you can paralyze along.\n",
      "3459.36s - And so by like doing this\n",
      "kinda like micro pipelining\n",
      "3461.64s - of your like communication\n",
      "and computation,\n",
      "3464.64s - you can actually still enable overlap\n",
      "3467.31s - even though like we're\n",
      "doing tensor parallelism.\n",
      "3472.56s - Finally, the kind of\n",
      "last kind of parallelism\n",
      "3474.6s - that we have is a pipeline parallelism.\n",
      "3476.97s - Where the general idea\n",
      "here is that you assign\n",
      "3479.01s - like the first part of\n",
      "the task to the first GPU.\n",
      "3481.8s - And the second part of the\n",
      "task to the second GPU,\n",
      "3484.647s - this kind of differs\n",
      "from tensor parallelism\n",
      "3487.59s - or like tensor task parallelism in that\n",
      "3489.66s - in this case you do not\n",
      "run on the same task\n",
      "3492.84s - with both GPUs at the same time.\n",
      "3495.0s - And so in this way you\n",
      "can think of the task\n",
      "3497.1s - is actually being sharded\n",
      "across like the time dimension.\n",
      "3500.64s - And so there are a couple\n",
      "like additional wrinkles here\n",
      "3503.28s - about pipeline parallelism\n",
      "that are kind of unique to ML\n",
      "3506.52s - I think.\n",
      "3507.42s - So one of them is that, you know,\n",
      "3508.47s - once again the frequent\n",
      "massive synchronizations\n",
      "3510.84s - prevent us from like\n",
      "filling up the pipeline.\n",
      "3513.3s - But the second issue is\n",
      "that like back propagation,\n",
      "3515.76s - like the, you know, the\n",
      "forwards and backwards pattern\n",
      "3517.8s - that I mentioned before.\n",
      "3519.15s - Also adds like a lot of very fun wrinkles\n",
      "3521.61s - to your pipeline parallelism schedule.\n",
      "3523.38s - And then in this case this is kind of like\n",
      "3524.94s - a pipeline like you know,\n",
      "that people have designed\n",
      "3528.12s - where the blue boxes are\n",
      "like the forwards pass,\n",
      "3530.91s - the cyan boxes are like the backwards pass\n",
      "3533.82s - and then the green boxes\n",
      "are like the backwards pass\n",
      "3536.61s - split up even more.\n",
      "3538.68s - And in this case like the\n",
      "main kind of nuance here\n",
      "3542.16s - is that you can see like\n",
      "there's various points here\n",
      "3545.07s - where your pipeline can actually choose\n",
      "3546.87s - to either run a forwards micro batch\n",
      "3550.2s - or like a backwards micro batch.\n",
      "3551.79s - And so this kind of like\n",
      "choice is not something\n",
      "3554.25s - you need to deal with,\n",
      "3555.63s - in like a traditional\n",
      "pipeline parallelism setting.\n",
      "3557.94s - And it ends up leading to\n",
      "like a lot of different\n",
      "3561.27s - like optimizations that people\n",
      "do with pipeline parallelism.\n",
      "3565.38s - And so kind of like\n",
      "putting this all together,\n",
      "3567.75s - this is a diagram from the llama3 paper.\n",
      "3569.91s - Where they showed like\n",
      "what's kind of being done\n",
      "3572.31s - to train llama3.\n",
      "3573.988s - And so in this case you can\n",
      "see that we're combining\n",
      "3575.55s - like, you know, tensor\n",
      "parallelism, task parallelism,\n",
      "3577.98s - with data parallelism\n",
      "with pipeline parallelism.\n",
      "3581.4s - And there's also a\n",
      "fourth one thrown in here\n",
      "3583.02s - called context parallelism\n",
      "3584.79s - If you can think of that\n",
      "as just like another form\n",
      "3586.53s - of a task or tensor parallelism.\n",
      "3591.69s - Yeah, and so I think\n",
      "again here like compilers\n",
      "3595.2s - I think have historically\n",
      "struggled quite a bit\n",
      "3597.33s - when it comes to keeping up\n",
      "3598.44s - with the distributed\n",
      "optimizations that people do.\n",
      "3600.6s - And the main reason is that\n",
      "like compilers are dumb\n",
      "3602.79s - and humans are smart.\n",
      "3604.62s - And what I mean by this\n",
      "is that for any given set\n",
      "3606.75s - of parallelism schemes\n",
      "or like any given set\n",
      "3609.75s - of parallelism configs, it\n",
      "often is pretty feasible\n",
      "3612.3s - to create like automatic model\n",
      "3614.79s - for the compiler to determine\n",
      "how to automatically\n",
      "3616.95s - like paralyze your program.\n",
      "3619.23s - But the issue here is that\n",
      "actually that like most\n",
      "3621.66s - of the innovation of parallelism\n",
      "3623.49s - doesn't come from like searching\n",
      "3624.69s - within your existing search base.\n",
      "3626.52s - It usually comes from like\n",
      "expanding your search base\n",
      "3628.98s - along a new dimension.\n",
      "3630.48s - And so this is something that compilers\n",
      "3631.95s - are not like super great at.\n",
      "3634.2s - And it's been one of the\n",
      "struggles when people\n",
      "3636.48s - try to like, you know,\n",
      "automate parallelism\n",
      "3639.18s - with compilers or like general systems.\n",
      "3643.8s - And yeah.\n",
      "3647.46s - For example, like one of the\n",
      "digital like kind of wrinkles\n",
      "3649.59s - that have kind of shown up like,\n",
      "you know, at certain scales\n",
      "3652.98s - is a fault tolerance where you know,\n",
      "3655.08s - when you're running like,\n",
      "you know on like, you know,\n",
      "3656.76s - 10 or 20,000 GPUs like where\n",
      "we're doing these like,\n",
      "3660.66s - you know, globally full synchronizations\n",
      "3662.85s - across all of our GPUs.\n",
      "3664.62s - we have this issue where our GPUs can fail\n",
      "3666.57s - for like all sorts of reasons.\n",
      "3668.19s - Some of them might be like user related,\n",
      "3669.96s - some of them might be\n",
      "like hardware related,\n",
      "3671.52s - some of them might be networking related,\n",
      "3673.47s - but basically like a single failure\n",
      "3675.42s - takes down your entire run\n",
      "and this is like, you know,\n",
      "3677.64s - quite problematic.\n",
      "3678.87s - And so those tables from like a paper\n",
      "3680.46s - that Meta published about\n",
      "like training llama3\n",
      "3683.49s - and fault tolerance.\n",
      "3686.07s - and like, you know, I think\n",
      "like one kind of example\n",
      "3689.28s - of like how this ends up\n",
      "being like a interesting issue\n",
      "3692.19s - is that when you're training\n",
      "on like 16,000 GPU hours,\n",
      "3695.97s - you're only getting like a\n",
      "error once every 1.8 hours.\n",
      "3699.57s - But you know, now if you like\n",
      "scale it up to 131,000 GPUs,\n",
      "3703.32s - you now get a failure like\n",
      "every like 15 minutes or so.\n",
      "3706.35s - And so like this turns something\n",
      "3708.24s - that might not be that problematic\n",
      "3709.92s - at 16,000 scale or smaller into something\n",
      "3712.5s - that's like very problematic\n",
      "at a 131,000 GPU scale\n",
      "3716.13s - or even higher where you\n",
      "can imagine that, you know,\n",
      "3718.38s - now you have a situation\n",
      "where you might not be able\n",
      "3720.57s - to make even a single\n",
      "step before a single GPU\n",
      "3723.96s - in your entire fleet fails.\n",
      "3728.31s - So to kind of conclude, you know,\n",
      "3729.84s - I think ML has basically become\n",
      "the single most important\n",
      "3732.87s - computational workload in the\n",
      "world over the last decade.\n",
      "3736.89s - Maybe a bit biased,\n",
      "3738.78s - but I think by like the amount\n",
      "of like FLOPS and investment,\n",
      "3742.02s - I think you can make a strong case for it.\n",
      "3744.96s - But the characteristics\n",
      "I think of the workload\n",
      "3747.69s - both from a social POV, like, you know,\n",
      "3749.61s - the massive infrastructure required\n",
      "3750.75s - as well as the technical POV.\n",
      "3752.34s - I think kind of often does mean\n",
      "3753.6s - that a lot of the traditional approaches\n",
      "3755.07s - to building systems are\n",
      "compilers don't directly apply.\n",
      "3758.79s - Like you can't just like,\n",
      "you know, build a compiler\n",
      "3760.65s - and then, you know, have people\n",
      "who just optimize a compiler\n",
      "3762.93s - for like five years without\n",
      "thinking about, you know,\n",
      "3766.32s - how the workloads are\n",
      "evolving on top of them\n",
      "3768.84s - affect their compiler.\n",
      "3771.24s - And so I think to me\n",
      "3772.89s - like the kind of most interesting question\n",
      "3774.48s - about building a systems here.\n",
      "3775.92s - Isn't really about like building\n",
      "3777.09s - the right optimizations for systems.\n",
      "3779.07s - It's instead about like coming up\n",
      "3780.39s - with the right programming\n",
      "models for expressing\n",
      "3783.18s - like your systems and kind of coming up\n",
      "3785.43s - with the right programming\n",
      "models to enable people\n",
      "3787.77s - to kind of like do their own optimizations\n",
      "3790.23s - and kind of like build the next, you know,\n",
      "3792.72s - 100,000 GPU model.\n",
      "3794.76s - Thanks for coming to my\n",
      "talk. Hope you guys liked it.\n",
      "3799.177s - (audience applauds)\n"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "video_id = \"139UPjoq7Kw&t=25s&ab_channel=JaneStreet\"\n",
    "# video_id = 'uQhTuRlWMxw&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=13&ab_channel=3Blue1Brown'\n",
    "transcript = None\n",
    "\n",
    "try:\n",
    "    transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n",
    "    for script in transcript_list:\n",
    "        print(f\"Language: {script.language}\")\n",
    "        print(f\"Language: {script.language_code}\")\n",
    "        print(f\"Generated automatically: {script.is_generated}\")\n",
    "        print(f\"Translation available: {script.is_translatable}\")\n",
    "        print(\"-----\")\n",
    "        if 'en' in script.language_code and not script.is_generated:\n",
    "            transcript = script.fetch()\n",
    "    if transcript is None:\n",
    "        transcript = transcript_list.find_transcript(['en']).fetch()\n",
    "    for entry in transcript:\n",
    "        print(f\"{entry['start']}s - {entry['text']}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why No One Wants to Live in Canada - YouTube'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "video_url = \"https://www.youtube.com/watch?v=ZvSNcnG2eqY&t=82s&ab_channel=2and20\"\n",
    "r = requests.get(video_url)\n",
    "soup = BeautifulSoup(r.text)\n",
    "link = soup.find_all(name=\"title\")[0]\n",
    "title = str(link)\n",
    "title = title.replace(\"<title>\",\"\")\n",
    "title = title.replace(\"</title>\",\"\")\n",
    "title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Episode 475: Neal and Toby explain how CEOs from the biggest tech companies are trying to woo Donald Trump ahead of his presidency. Next up, how Broadcom road semiconductors all the way to $1 trillion. Then government officials sound off against the drones that have been flying around the northeast. Hot Wheels and those who hate daylight saving time are our winners or the weekend, and finally a look at the week ahead. #trump #tech #economy #ai #daylight #hotwheels #toys #books \\\\n\\\\n00:00 - Patterson bookstore bonus extravaganza\\\\n2:15 - Tech CEOs donate to Trump\\\\n6:40 - Broadcom quietly cashin in\\\\n10:45 - What is up with drones??\\\\n15:10 - Trump wants to end Daylight Saving Time\\\\n18:30 - Hot Wheels lead the way\\\\n21:30 - Week Ahead\\\\n\\\\nBuild your Range Rover Sport at landroverusa.com\\\\n\\\\nSubscribe to Morning Brew Daily for more of the news you need to start your day. Share the show with a friend, and leave us a review on your favorite podcast app.\\\\n\\\\n\\\\nListen to the podcast here: https://link.chtbl.com/MBD\\\\n\\\\nFollow us on Twitter: https://twitter.com/mbdailyshow\\\\nFollow us on Instagram: https://www.instagram.com/mbdailyshow/\\\\nFollow us on TikTok: http://www.tiktok.com/@mbdailyshow\\\\n\\\\nSign up for the Morning Brew Newsletter: https://www.morningbrew.com/daily/subscribe?utm_campaign=mbd_yt\\\\u0026utm_medium=multimedia\\\\u0026utm_source=youtube\\\\n\\\\nMorning Brew Daily, a daily talk show that covers the latest news on business, the economy, and everything else, with Neal Freyman and Toby Howell. Witty, informative and everything you need to start your day. Available on all podcasting platforms and Youtube.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern.findall(str(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 475: Neal and Toby explain how CEOs from the biggest tech companies are trying to woo Donald Trump ahead of his presidency. Next up, how Broadcom road semiconductors all the way to $1 trillion. Then government officials sound off against the drones that have been flying around the northeast. Hot Wheels and those who hate daylight saving time are our winners or the weekend, and finally a look at the week ahead. #trump #tech #economy #ai #daylight #hotwheels #toys #books \n",
      "\n",
      "00:00 - Patterson bookstore bonus extravaganza\n",
      "2:15 - Tech CEOs donate to Trump\n",
      "6:40 - Broadcom quietly cashin in\n",
      "10:45 - What is up with drones??\n",
      "15:10 - Trump wants to end Daylight Saving Time\n",
      "18:30 - Hot Wheels lead the way\n",
      "21:30 - Week Ahead\n",
      "\n",
      "Build your Range Rover Sport at landroverusa.com\n",
      "\n",
      "Subscribe to Morning Brew Daily for more of the news you need to start your day. Share the show with a friend, and leave us a review on your favorite podcast app.\n",
      "\n",
      "\n",
      "Listen to the podcast here: https://link.chtbl.com/MBD\n",
      "\n",
      "Follow us on Twitter: https://twitter.com/mbdailyshow\n",
      "Follow us on Instagram: https://www.instagram.com/mbdailyshow/\n",
      "Follow us on TikTok: http://www.tiktok.com/@mbdailyshow\n",
      "\n",
      "Sign up for the Morning Brew Newsletter: https://www.morningbrew.com/daily/subscribe?utm_campaign=mbd_yt\\u0026utm_medium=multimedia\\u0026utm_source=youtube\n",
      "\n",
      "Morning Brew Daily, a daily talk show that covers the latest news on business, the economy, and everything else, with Neal Freyman and Toby Howell. Witty, informative and everything you need to start your day. Available on all podcasting platforms and Youtube.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "video_url = 'https://www.youtube.com/watch?v=QlUcyaXiXPE&ab_channel=MorningBrewDaily'\n",
    "soup = BeautifulSoup(requests.get(video_url).content)\n",
    "pattern = re.compile('(?<=shortDescription\":\").*(?=\",\"isCrawlable)')\n",
    "description = pattern.findall(str(soup))[0].replace('\\\\n','\\n')\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00:00 - Patterson bookstore bonus extravaganza',\n",
       " '2:15 - Tech CEOs donate to Trump',\n",
       " '6:40 - Broadcom quietly cashin in',\n",
       " '10:45 - What is up with drones??',\n",
       " '15:10 - Trump wants to end Daylight Saving Time',\n",
       " '18:30 - Hot Wheels lead the way',\n",
       " '21:30 - Week Ahead']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapter_pattern = r\"(\\d{1,2}:\\d{2}(?::\\d{2})?)\\s+(.*)\"\n",
    "matches = re.findall(chapter_pattern, description)\n",
    "\n",
    "chapters = []\n",
    "for match in matches:\n",
    "    chapters.append(' '.join(match))\n",
    "chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_chapters(video_url):\n",
    "    soup = BeautifulSoup(requests.get(video_url).content)\n",
    "    pattern = re.compile('(?<=shortDescription\":\").*(?=\",\"isCrawlable)')\n",
    "    description = pattern.findall(str(soup))[0].replace('\\\\n','\\n')\n",
    "    return extract_chapters(description)\n",
    "\n",
    "def extract_chapters(description):\n",
    "    chapter_pattern = r\"(\\d{1,2}:\\d{2}(?::\\d{2})?)\\s+(.*)\"\n",
    "    matches = re.findall(chapter_pattern, description)\n",
    "\n",
    "    chapters = []\n",
    "    for match in matches:\n",
    "        chapters.append(' '.join(match))\n",
    "    return chapters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data.fetch_transcript import get_chapters\n",
    "# video_url = \"https://www.youtube.com/watch?v=ZvSNcnG2eqY&t=82s&ab_channel=2and20\"\n",
    "# video_url = 'https://www.youtube.com/watch?v=QlUcyaXiXPE&ab_channel=MorningBrewDaily'\n",
    "\n",
    "# no chapters\n",
    "video_url = 'https://www.youtube.com/watch?v=139UPjoq7Kw&t=25s&ab_channel=JaneStreet'\n",
    "\n",
    "get_chapters(video_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: English (United States)\n",
      "Language: en-US\n",
      "Generated automatically: False\n",
      "Translation available: True\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "link = 'https://www.youtube.com/watch?v=139UPjoq7Kw&t=25s&ab_channel=JaneStreet'\n",
    "from data.fetch_transcript import fetch_transcript\n",
    "\n",
    "output = fetch_transcript(link)\n",
    "transcript = output['transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"0s - - So it's my great pleasure\",\n",
       " '2s - to introduce our speaker tonight, Horace He.',\n",
       " '4s - Horace He graduated from Cornell in 2020,',\n",
       " '8s - and he works at Meta on the PyTorch team,',\n",
       " '11s - specifically at the intersection',\n",
       " '13s - of compilers and machine learning.',\n",
       " \"16s - If you've used things like torch.compile,\",\n",
       " '19s - which is a thing in PyTorch',\n",
       " '20s - that makes your model goes',\n",
       " '21s - like 2-4x faster in just one line of code.',\n",
       " \"24s - Or if you've used FlexAttention,\",\n",
       " '25s - which is something that lets researcher',\n",
       " '28s - design fast kernel for attention',\n",
       " '30s - without leaving the world of Python.',\n",
       " \"33s - He's the person responsible for both of those things.\",\n",
       " '36s - You should also check his blog, which is pretty awesome,',\n",
       " '38s - and in particular the blog post',\n",
       " '40s - \"Making Deep Learning go Brrrr From First Principles.\"',\n",
       " \"44s - And without further ado, I'll give it to Horace.\",\n",
       " '47s - - Thanks for the intro.',\n",
       " \"48s - Today I'm gonna give a talk about\",\n",
       " '49s - building machine learning systems',\n",
       " '51s - for a trillion, trillion floating point operations.',\n",
       " '54s - My name is Horace He,',\n",
       " \"55s - and I'm on the PyTorch compilers team at Meta.\",\n",
       " '59s - So, you know, I think we live in pretty unprecedented times',\n",
       " '62s - in terms of an infrastructure build out,',\n",
       " '65s - which is, I think, you know,',\n",
       " \"65s - nicely reflected in NVIDIA's stock price.\",\n",
       " '69s - I feel like, you know, basically every month,',\n",
       " '71s - we see like a different headline',\n",
       " '72s - about like a new nuclear power plant',\n",
       " '74s - from like Microsoft, or, you know,',\n",
       " '76s - like a massive 300K GPU data cluster from xAI.',\n",
       " '80s - You know, people like,',\n",
       " '81s - I feel like when Amazon first like built like a,',\n",
       " '84s - or like bought a nuclear power center,',\n",
       " '86s - it was like a big news.',\n",
       " '87s - But now practically like, you know,',\n",
       " '88s - every cool AI company',\n",
       " '90s - has their like own nuclear data center.',\n",
       " '95s - And you know, of course,',\n",
       " '96s - this like kind of really massive (indistinct) build out',\n",
       " '98s - has kind of also resulted in pretty ludicrous fundraisers',\n",
       " '101s - from startups, I think.',\n",
       " '102s - Like, I remember back in like 2016, like,',\n",
       " '104s - you know, if you like made it as a startup,',\n",
       " '107s - it would be like if you were worth a billion dollars, right?',\n",
       " '109s - It was like a unicorn.',\n",
       " '110s - It was like the mark of like really making it',\n",
       " '111s - as a startup beyond your wildest dreams.',\n",
       " '114s - But you know, in 2024, you know,',\n",
       " '115s - you gotta raise a billion dollars just like get started.',\n",
       " \"118s - You know, it's like just to play the game, (laughs)\",\n",
       " '120s - you need like a billion dollars, you know,',\n",
       " '122s - of which most of it goes to NVIDIA (chuckles).',\n",
       " '125s - (audience laughs)',\n",
       " \"128s - And so I think like, it's kind of crazy to think that,\",\n",
       " '132s - you know, all of this, like, you know,',\n",
       " '133s - billions of dollars is really just to do',\n",
       " '134s - like absolutely insane amount',\n",
       " '136s - of floating point operations.',\n",
       " \"138s - Here's like a nice chart from like Epoch AI\",\n",
       " '140s - showing like the growth of compute over time.',\n",
       " '144s - And like, these floating point operations',\n",
       " '147s - are really just like big matmuls',\n",
       " '149s - done like over and over, like, you know,',\n",
       " '151s - for millions of iterations over like months.',\n",
       " '154s - And nowadays,',\n",
       " '155s - like the leading edge models are currently trained',\n",
       " '157s - with about like one E26 floating point operations,',\n",
       " '160s - which is approximately 100 trillion',\n",
       " '164s - trillion floating point operations.',\n",
       " '166s - So like a trillion trillion, like a trillion TeraFLOPS',\n",
       " '171s - worth of floating point operations.',\n",
       " '173s - And so, you know, back, you know, another kind of effect',\n",
       " \"176s - that you might've noticed is like back prior to 2016,\",\n",
       " '179s - if you search on like Hacker news like ML,',\n",
       " \"181s - you'd often get like a lot of people asking about, you know,\",\n",
       " '183s - the ML family of languages.',\n",
       " '185s - But nowadays, you know, you search ML on a hacker news',\n",
       " '189s - and you get like a very different like type of article.',\n",
       " '195s - And so I think one of the things',\n",
       " '196s - that kind of is missed here is like, you know,',\n",
       " '198s - with all these like billions of dollars',\n",
       " '200s - and like, you know, yottaflop of operations,',\n",
       " \"202s - it's kind of easy to forget that, you know,\",\n",
       " '203s - like these operations needed to actually run somehow',\n",
       " '207s - on the like machines.',\n",
       " '210s - And so, you know, a modern stack might involve, you know,',\n",
       " '213s - like, you know, call like an LLM API',\n",
       " \"215s - and then it's called like PyTorch, like NCCL, Triton, CUDA\",\n",
       " '218s - like NVCC, like all these like different layers in the stack',\n",
       " '221s - that like somebody was involved in writing.',\n",
       " \"224s - And I'm often reminded of this like XK CD comic, you know,\",\n",
       " '228s - showing like the state of like modern digital infrastructure',\n",
       " '231s - and so, you know, you kind of, you know,',\n",
       " '233s - often have forget about all the infrastructure',\n",
       " '234s - that was involved for you to like get where you are,',\n",
       " \"238s - but like really we're just kind of building\",\n",
       " '239s - like layers on top of layers.',\n",
       " '243s - And so, you know, if you work in a systems, you know,',\n",
       " '245s - like I do and I suspect many of you do, you kind of,',\n",
       " '248s - I think oftentimes think about your work a little bit like,',\n",
       " '251s - like a benevolent dictator of sorts.',\n",
       " \"254s - Where kind of you imagine like what you're doing here\",\n",
       " '256s - is that like a lot of people build on top of your work.',\n",
       " '258s - And so if you can kind of, you know,',\n",
       " '260s - give like a small amount improvement to, you know,',\n",
       " '262s - like millions of people, you know,',\n",
       " '264s - your small improvements can lead to like, you know,',\n",
       " '266s - significant impacts on the world.',\n",
       " '268s - And so for example, I kind of imagine like Guido,',\n",
       " '271s - with Python, you just like, you know, kind of sits on top',\n",
       " '273s - of this cloud and you know, eventually like gives us like',\n",
       " '275s - NoGil or like faster CPython or like the Walrus Operator',\n",
       " '278s - and I feel like this is kind of oftentimes like',\n",
       " '281s - how I imagine like infrastructure work did.',\n",
       " \"284s - And it's kind of a lot of why I got into infra work\",\n",
       " '286s - in the first place.',\n",
       " '289s - And so on the other hand, I feel like if you work in ML,',\n",
       " '291s - things can sometimes feel a little bit different.',\n",
       " '293s - I originally came across this a post on threads',\n",
       " '296s - where this person said, you know, my main gripe',\n",
       " '298s - with like working on top of like LM APIs',\n",
       " \"300s - is that you're not really like,\",\n",
       " '301s - no one is like engineering anything.',\n",
       " \"303s - You're just like chanting like a prayer\",\n",
       " '306s - to this like manmade demon deity to like do your bidding.',\n",
       " '310s - And this like very similar to this kind of like',\n",
       " '312s - Shoggoth metaphor that has become like very popular',\n",
       " '314s - in deep learning circles.',\n",
       " '316s - Where the idea here is that like we really like',\n",
       " '318s - put all these like matmuls',\n",
       " '319s - and like computation into producing this like very weird',\n",
       " '323s - alien intelligence.',\n",
       " '324s - and then we like kind of like RLHF it, you know,',\n",
       " '328s - and like provide it in this like nice convenient interface',\n",
       " '330s - to people with like, you know, ChatGPT',\n",
       " '333s - or something like that.',\n",
       " '335s - But you know, I think if you think about it,',\n",
       " \"337s - I think we're kind of working with Shoggoths\",\n",
       " '339s - like all the way down.',\n",
       " \"342s - Like even like if you're working like in systems\",\n",
       " \"346s - and you're not calling like ML models,\",\n",
       " '348s - you still have this kinda like, you know,',\n",
       " '349s - massive amount of infrastructure that you presumably',\n",
       " \"351s - don't really understand written by people\",\n",
       " \"354s - you've probably never talked to.\",\n",
       " '355s - And like the end result know they try to expose',\n",
       " '358s - is like some kind of a simple interface',\n",
       " '360s - where you can just like import torch',\n",
       " '362s - and then, you know, hopefully run your code across like,',\n",
       " '364s - you know, a 100K GPUs.',\n",
       " '368s - And so I think as a result there are some,',\n",
       " '371s - I think, interesting ways in which I think ML models',\n",
       " '373s - that feel kind of different from regular systems.',\n",
       " '376s - One of those ways is that like ML models',\n",
       " '380s - are extremely simple and as a result',\n",
       " '384s - we have like very high expectations',\n",
       " '385s - from the performance of like the code.',\n",
       " '389s - I think it kind of like trace this all the way back',\n",
       " '391s - to this very nice article called the \"Bitter Lesson,\"',\n",
       " \"394s - which I'd really recommend reading if you guys\",\n",
       " \"395s - haven't come across it before.\",\n",
       " '397s - Where their main observation here was that like,',\n",
       " '400s - clever ideas in machine learning have basically',\n",
       " '403s - throughout like its 50 year history always lost out',\n",
       " '406s - to just simple ideas that like scaled really well',\n",
       " \"409s - with Moore's law.\",\n",
       " \"412s - And I'm not really like joking here when I say\",\n",
       " '414s - that like machine learning logic is exceedingly simple.',\n",
       " \"417s - There's like this cool project from someone\",\n",
       " '419s - called like Andre Pathy called llama2.c.',\n",
       " \"422s - And basically here he's like implemented\",\n",
       " '425s - like llama2 in about like 973 lines of C.',\n",
       " '428s - With like no other dependencies.',\n",
       " '430s - Like, you know, every single loop,',\n",
       " '431s - every single like matmul is just implemented from scratch.',\n",
       " \"434s - So with 973 lines, you can't like, you know,\",\n",
       " '436s - do it very fast, but it does run',\n",
       " '439s - and I think it does kind of indicate just like',\n",
       " '441s - how fundamentally simple the models',\n",
       " \"443s - that like we're spending all this compute end up being.\",\n",
       " '448s - And so the end result like',\n",
       " '449s - although like the problem themselves are extremely simple',\n",
       " '452s - and like very like easy to optimize in some sense.',\n",
       " '457s - The expectations are very high',\n",
       " '459s - for how well we can optimize these matmuls.',\n",
       " '461s - So one example here is that like the predominant metric',\n",
       " \"464s - for measuring your like model's performance in deep learning\",\n",
       " '468s - is called model FLOP utilization.',\n",
       " '470s - And so this is basically the like percentage',\n",
       " '472s - of the theoretical max flops that your GPU is able to do.',\n",
       " '477s - And so if you kind of think about this,',\n",
       " '479s - this is actually like a very absurd metric to hit.',\n",
       " '482s - Like if on a CPU, like you measured any of your code',\n",
       " '484s - by this metric, like you can only hit a 100%',\n",
       " '487s - if like at every single time every single core',\n",
       " '490s - of your CPU is always issuing max width SIMD instructions.',\n",
       " \"494s - That's like the only way you can hit a 100% utilization.\",\n",
       " '498s - And so, you know, if you take a look at any of the code',\n",
       " '500s - that you guys presumably write,',\n",
       " '502s - like almost no CPU code is like anywhere near like,',\n",
       " '505s - you know, a 100% flop.',\n",
       " \"506s - It's probably like way under like 1% most of the time.\",\n",
       " '510s - On the other hand, in like machine learning',\n",
       " \"512s - for like large scale training, we're typically often hitting\",\n",
       " '514s - around like 50% of like the peak flops.',\n",
       " '518s - And this is I think like this is kinda like a indicative',\n",
       " '522s - that like even though the overall problem',\n",
       " '523s - is like very simple, the kind of corresponding difficulty',\n",
       " '526s - just goes into like making your like models hit',\n",
       " '530s - like this, like very high perf barrier.',\n",
       " '535s - Another ind of a interesting observation',\n",
       " '537s - in like machine learning is that,',\n",
       " '540s - that has kind of exacerbated this a bit.',\n",
       " '542s - Is that the field has consolidated significantly',\n",
       " '544s - over the last five to 10 years.',\n",
       " '547s - So one of them is that like, you know,',\n",
       " '548s - like maybe 10 years ago you had like a lot more architecture',\n",
       " '551s - is a lot more variance and like different things',\n",
       " '553s - that people were trying.',\n",
       " '554s - But nowadays people really just like transformers are like',\n",
       " '558s - the dominant architecture for everything.',\n",
       " '559s - Like you have transformers for vision, you have transformers',\n",
       " '561s - for language, you have transformers for like, you know,',\n",
       " \"563s - audio, it's kind of like all transformers.\",\n",
       " '566s - And the other way like things have kind of changed',\n",
       " '568s - is that instead of like, you know, many different people',\n",
       " '572s - training SOTA models.',\n",
       " '573s - You oftentimes just have a few companies',\n",
       " '575s - that are training SOTA models.',\n",
       " \"576s - And so we've kind of gone from a bit of like a monopoly\",\n",
       " '579s - where like, you know, previously there was this like,',\n",
       " '581s - you know, one person providing the infra',\n",
       " '583s - and like many people using the infra,',\n",
       " '585s - to in some ways it feels a little bit more like monopsony',\n",
       " '588s - where you have like many people trying to provide infra',\n",
       " \"590s - and then you know, only one person's actually training\",\n",
       " '592s - the job at the end of the day.',\n",
       " '597s - And so as a kind of result, I think that like',\n",
       " \"600s - there's kind of two general ways to think about\",\n",
       " '602s - like getting performance from like your systems.',\n",
       " '606s - And so one of the ways generally is basically',\n",
       " '608s - like optimizations is like you have a compiler,',\n",
       " '610s - you like make the compiler faster, you know,',\n",
       " '613s - you improve like the performance',\n",
       " '614s - of everybody using your compiler.',\n",
       " '617s - and so the other way though',\n",
       " '618s - is kind of like programming models.',\n",
       " '620s - And so like this is kind of analogous',\n",
       " '621s - as opposed to like a system whose responsibility',\n",
       " '624s - is to like chop down a tree',\n",
       " \"626s - and then you know, you're just like optimizing\",\n",
       " '627s - how fast you can chop down the tree.',\n",
       " \"629s - And the other alternative is like you're providing tools\",\n",
       " '631s - for people to cut down the trees themselves.',\n",
       " '635s - And so to kind of like talk a little bit',\n",
       " \"637s - about programming models, I think it's like illustrative\",\n",
       " '640s - to kind of talk about how like ML frameworks',\n",
       " '642s - have kind of evolved in terms of what program model',\n",
       " \"645s - they've exposed to users.\",\n",
       " '647s - So originally like I think like 2010,',\n",
       " '651s - like 2011, 2012, the first like ML framework',\n",
       " '654s - that kind of got a lot of popularity was this framework',\n",
       " '656s - called Caffe.',\n",
       " '657s - And the way you like express neural networks in Caffe',\n",
       " '659s - is like a very declarative nature.',\n",
       " '662s - And by that I mean you like edited a protobuf file I think.',\n",
       " '666s - Where like the protobuf like specified all the things',\n",
       " '668s - you needed like care about your neural network.',\n",
       " '671s - And so as you can imagine, you know,',\n",
       " '672s - programming protobufs is like not very fun, you know,',\n",
       " \"675s - there's a lot of things you might want to do\",\n",
       " \"677s - that you can't do in protobufs.\",\n",
       " '678s - and so a like natural net thing that people did',\n",
       " '681s - was kind of these kind of graph builder type APIs.',\n",
       " '684s - And so this is kind of, you know, how TensorFlow 1',\n",
       " '686s - kinda of look like where the idea is that oh, you know,',\n",
       " '689s - like programming like, you know, programming protobufs',\n",
       " '691s - or like no human should need to write protobufs by hand.',\n",
       " '694s - And so ideally, you know,',\n",
       " '696s - you should just like write like a DSL of sorts',\n",
       " '699s - that allows you to generate the protobufs',\n",
       " '701s - like from this DSL.',\n",
       " '703s - However, like even this DSL still kind of has',\n",
       " '706s - like a lot of confusion in it.',\n",
       " '708s - Like it is not super clear that like given this DSL',\n",
       " '710s - like how code actually executes on your GPU.',\n",
       " '714s - And so kind of finally like around 2016 or 2017,',\n",
       " '718s - PyTorch started to become like really successful.',\n",
       " '720s - And kind of the like the feature',\n",
       " '723s - that like was most emblematic of PyTorch was basically',\n",
       " '726s - what was called imperative or eager execution.',\n",
       " '729s - And so what this means is that like,',\n",
       " '734s - or like, yeah, so PyTorch you know, was very successful',\n",
       " \"736s - and I think it's like worth talking about\",\n",
       " '737s - like why eager execution was so successful.',\n",
       " '740s - And I think that the main reason it was successful,',\n",
       " '742s - it just comes down to like what the programming model',\n",
       " '744s - of the execution looked like.',\n",
       " '746s - Where in a imperative slash like eager execution',\n",
       " '748s - is basically like, you know, you call a function,',\n",
       " '751s - the GPU runs a function',\n",
       " \"753s - and then the function finishes and you know, that's it.\",\n",
       " \"756s - Like, you know, like that's basically all you do.\",\n",
       " '757s - you call like torch.matmul and this is basically',\n",
       " '760s - the sequence of operations that happens but you know,',\n",
       " '763s - with kind of like a graph mode type approach',\n",
       " '765s - or you know, where kind of this pilot interjects.',\n",
       " '767s - You first like define the function,',\n",
       " '769s - the function gets converted into some like intermediate IR,',\n",
       " '773s - you know, a bunch of who knows what happens to your function',\n",
       " '776s - and then eventually like the function eventually executes',\n",
       " '779s - on the GPU.',\n",
       " '780s - And like the top one was like a very simple',\n",
       " '784s - execution model for people to understand.',\n",
       " \"787s - And I think another thing that's kind of interesting\",\n",
       " '789s - to notice about this is that this kind of also just,',\n",
       " '791s - like the top half also kind of describes',\n",
       " '794s - how Python executes.',\n",
       " '797s - And I think this is kind of a pretty illustrative to me',\n",
       " '799s - of like why Python has been so successful',\n",
       " '801s - in machine learning.',\n",
       " \"803s - It's basically that like, like a funny statement\",\n",
       " '806s - that you can make about Python is that like if you tried',\n",
       " '809s - to like train a model today, like you know,',\n",
       " '812s - I gave you like a day to like train a model,',\n",
       " '815s - it would run faster if you ran it in Python',\n",
       " '817s - compared to doing in C++.',\n",
       " '819s - And you might argue that this is like a unfair comparison.',\n",
       " '823s - Because you know, like you know all the infra like,',\n",
       " '825s - you know, all the frameworks that people built',\n",
       " '826s - are in Python.',\n",
       " '827s - But I think the reason why so many',\n",
       " '829s - of these frameworks have been built in Python',\n",
       " '831s - is that Python is like an exceedingly like',\n",
       " '833s - a simple language.',\n",
       " \"835s - And so it's also like a very global language.\",\n",
       " \"838s - And what I mean by like global is that like it's very easy\",\n",
       " '840s - for people to build their own infrastructure',\n",
       " '842s - on top of Python without really needing to fight',\n",
       " '845s - with like anything the Python language does',\n",
       " '847s - because the Python language itself does basically nothing.',\n",
       " \"851s - Like it, you know, it doesn't do any optimizations for you.\",\n",
       " '853s - It just like, you know, takes your function and runs it.',\n",
       " '857s - And so I think that PyTorch basically historically',\n",
       " '860s - is at like a very similar point in the design space.',\n",
       " '863s - Where PyTorch is like execution model is like so very simple',\n",
       " \"868s - and although this like doesn't really give you a lot\",\n",
       " \"870s - in terms of like, it doesn't like automatically\",\n",
       " \"872s - do a lot of things for you, it does mean that it's very easy\",\n",
       " '875s - for people to like build their own infrastructure',\n",
       " '878s - and frameworks on top of PyTorch.',\n",
       " '887s - I think another important detail to realize',\n",
       " '890s - especially about like PyTorch when it first came out.',\n",
       " '893s - Is that like this kind of unoptimized execution',\n",
       " \"896s - didn't actually even sacrifice any performance at all.\",\n",
       " '899s - Like, you know when people kind of benchmarked',\n",
       " '900s - PyTorch versus like, you know, TensorFlow at Cafe,',\n",
       " \"903s - PyTorch oftentimes wasn't even slower than those frameworks.\",\n",
       " \"907s - And I think there's like a two main reasons for this.\",\n",
       " '909s - So the first reason is that back in the day,',\n",
       " '912s - like you know about like 90% plus of your time',\n",
       " '914s - was spent in matmuls.',\n",
       " \"915s - And so there's basically nothing else you need to optimize.\",\n",
       " '918s - And so matmuls here are like matrix implications',\n",
       " \"920s - and so they're often provided by these like vendor libraries\",\n",
       " '922s - like cuBLAS or like QDNN.',\n",
       " \"925s - And like you know, they're provided by NVIDIA.\",\n",
       " \"927s - and they're like very hand optimized\",\n",
       " '929s - and so, you know, if 90% of your time is spent in matmuls,',\n",
       " '932s - then like what else can you even do to like optimize',\n",
       " '934s - the performance of your neural network?',\n",
       " '938s - And I think another kind of interesting like',\n",
       " \"943s - important piece here for like why PyTorch's performance\",\n",
       " '945s - was like quite good is that it had this like',\n",
       " '947s - Async execution model.',\n",
       " '950s - Where basically the idea here is that like you kind of have',\n",
       " '952s - like a parallel work queue on your GPU.',\n",
       " \"955s - And so what the CPU does is that it's only responsible\",\n",
       " '958s - for scheduling work on your work queue',\n",
       " '960s - and then you know, the GPU executes work from the work queue',\n",
       " '963s - and I think of it generally as like this GIF',\n",
       " '966s - is what usually comes to mind and basically you can imagine',\n",
       " '969s - that like the dog is like,',\n",
       " \"971s - or Gromit is like Python, you know, they're like, you know,\",\n",
       " '974s - trying to put down the train track in front of the train,',\n",
       " '976s - which is the GPU.',\n",
       " '977s - And so, you know, as long as like Gromit is able',\n",
       " '980s - to put down the train tracks faster than the train',\n",
       " '983s - actually rolls along the train tracks,',\n",
       " '985s - you can actually kind of view Python',\n",
       " '987s - as like having zero overhead.',\n",
       " \"989s - Like it doesn't provide any extra cost compared to,\",\n",
       " '993s - you know, if Python was like in a more efficient language',\n",
       " '995s - like C++.',\n",
       " '998s - And so in this way like you know,',\n",
       " '999s - eager execution not only had like a much easier',\n",
       " '1002s - to understand program model for users,',\n",
       " '1004s - it was also like basically just as fast',\n",
       " '1007s - as like non eager execution.',\n",
       " '1010s - Unfortunately, you know, good things that never last.',\n",
       " '1013s - And in 2017 like NVIDIA introduced',\n",
       " '1016s - what are called like tensor cores.',\n",
       " '1019s - And if you guys are unfamiliar with tensor cores,',\n",
       " \"1021s - they're basically like hardware units on the GPUs\",\n",
       " '1024s - that only do matmul operations.',\n",
       " \"1026s - Like, and I don't mean this like figuratively in the sense\",\n",
       " '1030s - that like people often say that GPUs are like',\n",
       " '1031s - well suited for matmuls.',\n",
       " '1033s - I mean this like very literally',\n",
       " \"1035s - in that like there's actually an assembly instruction\",\n",
       " '1037s - that just does like a mini matmul.',\n",
       " '1039s - And this is how you like interact with the tensor cores.',\n",
       " '1042s - And so if you look at this like plot of like the amount',\n",
       " '1045s - of like matmul flops versus non-matmul flops,',\n",
       " '1048s - you can really see like when NVIDIA realized',\n",
       " '1052s - that like deep learning was a big deal.',\n",
       " '1054s - Because like all, all of a sudden, you know,',\n",
       " '1055s - you kind of had this like massive like 10x gap',\n",
       " \"1058s - and so there's like a log scale.\",\n",
       " '1060s - And you had this kinda like massive like 10x gap',\n",
       " '1062s - between how fast matmuls were on the GPU',\n",
       " '1065s - and how fast like literally anything else',\n",
       " '1067s - you wanted to run on the GPU was.',\n",
       " '1071s - And so the end result is, you know, previously we said',\n",
       " '1073s - that like, you know, matmuls took like 90% of the time',\n",
       " '1076s - and so if the, if like, you know, NVIDIA has fed up matmuls',\n",
       " '1079s - by like 10x but then everything else like stayed',\n",
       " '1082s - the same amount of speed,',\n",
       " \"1083s - then all of a sudden, you know, like you're spending\",\n",
       " '1086s - a lot more of your time doing like non-matmul operations.',\n",
       " \"1090s - And so as a result we've kind of gotten like ML compilers\",\n",
       " '1095s - I think largely due to this change.',\n",
       " '1097s - And so one of the, I think the important details',\n",
       " '1099s - about ML compilers like, you know,',\n",
       " '1101s - in terms of like how they differ from the frameworks',\n",
       " '1102s - that came before is that ML frameworks still keep',\n",
       " '1106s - like the eager programming model and that like the code',\n",
       " '1109s - that you write like logically the program model',\n",
       " \"1111s - exposed to users is that you're still just writing\",\n",
       " '1113s - Python code and executes like line by line.',\n",
       " '1116s - The only difference now is that instead',\n",
       " '1117s - of actually executing line by line,',\n",
       " '1119s - we kind of captured into like a graph in some manner.',\n",
       " '1123s - And so Torch compile I think actually kind of does this',\n",
       " '1125s - in a pretty interesting way.',\n",
       " '1127s - And that torch compile actually like intercepts',\n",
       " '1131s - at like the Python by code interpreter level.',\n",
       " '1134s - Where Python kind of exposes these APIs',\n",
       " '1136s - where you can kind of insert',\n",
       " '1137s - your like own frame interpreter.',\n",
       " '1139s - And so this looks very much just like a traditional',\n",
       " '1141s - like you know, Git for like any kind of other VM.',\n",
       " '1144s - except this Git is kind of, you know,',\n",
       " '1145s - only meant for like PyTorch programs.',\n",
       " '1150s - And so if you kind of look at like, you know',\n",
       " '1152s - how like things have evolved over time.',\n",
       " '1155s - Originally you kind of had like frameworks like TensorFlow',\n",
       " '1157s - or Caffe before that.',\n",
       " '1159s - Where both the user like program model that users wrote',\n",
       " '1162s - was like a graph builder type attraction.',\n",
       " '1165s - But then the execution programming model',\n",
       " '1166s - was also a graph execution type extraction.',\n",
       " '1169s - And then like after that you kind of had PyTorch, you know,',\n",
       " '1172s - one type like stuff.',\n",
       " '1175s - Where the user program model now switched',\n",
       " '1176s - to like an eager style execution, but the execution program',\n",
       " '1179s - and the execution program model was also eager where like,',\n",
       " \"1182s - you know, each operator's executed one at a time.\",\n",
       " '1186s - But kind of now finally, you know,',\n",
       " '1187s - modern ML frameworks like,',\n",
       " '1189s - pretty much all ML frameworks nowadays',\n",
       " '1191s - use like an imperative eager programming model.',\n",
       " '1194s - But almost all ML frameworks now also have some way',\n",
       " '1197s - to like capture this program model into a graph of some kind',\n",
       " '1201s - so they can perform optimizations.',\n",
       " '1202s - And so this is like, you know, JAX.git, Tonnegrad,',\n",
       " '1205s - this is like MLX.',\n",
       " '1207s - They kind of all have their like different approaches',\n",
       " '1208s - for capturing the graph to optimize.',\n",
       " '1216s - And so I think next I kinda wanna talk about, you know,',\n",
       " \"1218s - we've kind of discussed how we've gotten to ML compilers\",\n",
       " '1222s - in the first place.',\n",
       " '1223s - And so I think next I wanna talk about like',\n",
       " '1225s - what ML compilers are actually doing for you',\n",
       " \"1228s - and what kind of optimizations they're performing.\",\n",
       " '1231s - And so generally speaking, the way I think about',\n",
       " '1234s - deep learning performance or like performance on GPUs',\n",
       " \"1236s - in general is that there's basically three things\",\n",
       " '1238s - you can be spending your time on.',\n",
       " '1240s - The first one is compute, so this is time on our GPU',\n",
       " '1243s - competing like actual floating point operations.',\n",
       " '1246s - The next one is a memory which is, you know,',\n",
       " '1248s - time spent transferring your tensors within a GPU.',\n",
       " '1252s - So this is like, you know, across various memory subsystems',\n",
       " '1255s - in your GPU.',\n",
       " '1256s - And so finally like overhead,',\n",
       " '1258s - which is like everything else like you know,',\n",
       " \"1259s - it's like time your GPU spending idle and so on.\",\n",
       " \"1263s - And so first we're gonna talk about compute.\",\n",
       " '1266s - And so I think to a sum of approximation you can say',\n",
       " '1270s - that all runtime on your GPU is either compute',\n",
       " \"1272s - or it's a shuffling data.\",\n",
       " '1274s - And that like, you know, data movement',\n",
       " '1277s - is like not like a real operation, right?',\n",
       " \"1280s - it's like a no op from like the theoretical point of view.\",\n",
       " \"1284s - All it's doing is it's moving data from one place\",\n",
       " \"1286s - where it's convenient to another place\",\n",
       " \"1287s - where it's convenient.\",\n",
       " '1288s - And so basically a floating point operation',\n",
       " '1290s - is like the only real thing a GPU can do.',\n",
       " '1294s - But you can actually, I think simplify this even more',\n",
       " '1296s - and say that in reality actually nowadays like all runtime',\n",
       " '1300s - is either like matmuls or essentially shuffling data.',\n",
       " '1303s - And so this is like, because if you look at the actual like',\n",
       " '1306s - flop chart on like an Edge 100 like GPU, you can see here',\n",
       " '1310s - that like the FP32 flops is like',\n",
       " '1313s - you only have 67 teraFLOPS of FB32 compute,',\n",
       " '1317s - but you actually have like a 1,00 teraFLOPS',\n",
       " '1319s - of a TF32 compute.',\n",
       " '1321s - Which is basically like matrix multiplication compute.',\n",
       " '1324s - And so what this essentially...',\n",
       " '1326s - And sometimes like what this means',\n",
       " \"1327s - is that if you're not doing measurable (indistinct)\",\n",
       " \"1329s - on your GPU, you're really only like getting like 7%\",\n",
       " '1334s - of like your peak FLOP utilization.',\n",
       " '1336s - And so like, you know by like the metric',\n",
       " '1337s - that I mentioned before like model FLOP utilization,',\n",
       " '1340s - even if your GPU was fully occupied doing stuff',\n",
       " \"1343s - that wasn't a matmul you could only ever get like 7%\",\n",
       " '1346s - of FLOP utilization, which is like much lower than,',\n",
       " '1349s - you know, our theoretical peak.',\n",
       " '1355s - I did have a brief interlude about like,',\n",
       " '1357s - I think an interesting case where like',\n",
       " '1359s - these kind of abstractions do break down even more.',\n",
       " '1362s - And so I do have like a kind of a fun question.',\n",
       " '1365s - Which is like, do the matrix contents',\n",
       " '1367s - affect your matrix multiplication performance?',\n",
       " '1370s - And so I think, you know, if you kind of like,',\n",
       " '1373s - you know, are familiar with like, you know,',\n",
       " '1375s - general performance, there are like a lot of things that...',\n",
       " '1378s - A lot of ways where like data can impact your performance,',\n",
       " '1380s - but in this case matmuls actually avoid a lot of them.',\n",
       " '1383s - So for example, they have identical memory access patterns',\n",
       " '1385s - regardless of like what data is in your tensor,',\n",
       " \"1388s - there's like no control flow in the matmul.\",\n",
       " \"1391s - And the GPU also don't have like the denormals.\",\n",
       " \"1393s - So like that's like not a possibility as well.\",\n",
       " '1397s - So if you like, you know,',\n",
       " '1398s - and in this case where like taking three tensors,',\n",
       " \"1400s - we're initializing them with like all zeros.\",\n",
       " '1403s - Like a tensor initialized from the Gaussian distribution',\n",
       " '1406s - and then a tensor initialized',\n",
       " '1408s - from like a uniform distribution,',\n",
       " '1409s - which is like from zero to one.',\n",
       " '1410s - And so funnily enough, if you benchmark this,',\n",
       " '1413s - you actually find that there is a performance difference',\n",
       " '1416s - depending on the actual data that is within your tensors.',\n",
       " \"1421s - And so there's this tweet from some of you guys might know,\",\n",
       " '1426s - a long time ago that really I think',\n",
       " '1428s - like when I first saw this.',\n",
       " '1429s - I actually was very much reminded of this tweet,',\n",
       " '1432s - where, you know, I thought I knew how like a GPU worked,',\n",
       " '1435s - but then I was like very confused about',\n",
       " '1437s - what could possibly be causing',\n",
       " '1439s - these like performance differences like, you know,',\n",
       " '1441s - between the different data.',\n",
       " '1443s - And so the actual cause here is something called',\n",
       " '1446s - like leakage power or dynamic power where I think, you know,',\n",
       " \"1449s - most of you're probably familiar that you know,\",\n",
       " '1452s - like when a CPU or GPU is under load,',\n",
       " '1454s - it uses more like power',\n",
       " '1456s - and at some point it can like throttle, you know,',\n",
       " \"1458s - it's like using the maximum amount of power can use\",\n",
       " \"1461s - or the max amount of like a heat it's allowed.\",\n",
       " '1465s - But the actual thing is that like, you know,',\n",
       " \"1466s - this power doesn't just like come from nowhere.\",\n",
       " \"1469s - It actually largely comes from what's called\",\n",
       " '1471s - like dynamic or switching power.',\n",
       " '1474s - And what this means is like every time a transistor',\n",
       " '1476s - like on the GPU switches from like zero to one',\n",
       " '1479s - or like, you know, high to low or low to high,',\n",
       " '1481s - it like loses a little bit of this power.',\n",
       " '1484s - And so the actual like power usage on your GPU',\n",
       " '1486s - is kind of like a like sum across the total amount',\n",
       " '1489s - of like switching that goes on in your GPU.',\n",
       " \"1493s - And so this is why like if you're multiplying\",\n",
       " '1495s - with all zeros, you can imagine',\n",
       " '1496s - that like your GPU ends up not,',\n",
       " \"1498s - like a lot of transistors don't end up switching at all.\",\n",
       " \"1501s - And so like it doesn't actually consume that much power\",\n",
       " \"1503s - and it's much less a throttle.\",\n",
       " '1506s - And so if you actually like look at this,',\n",
       " '1509s - you can actually get like very different performance',\n",
       " '1510s - for like all these different kind of fund distributions.',\n",
       " \"1513s - Like whether it's like, you know, the normal distribution\",\n",
       " \"1516s - or whether it's like a checkerboard type like pattern\",\n",
       " \"1519s - or you know, it's like sparse or ternary.\",\n",
       " '1521s - And the reason why is just like,',\n",
       " \"1523s - it's like this kind of abstract thing\",\n",
       " '1525s - where these different patterns lead to like more or less',\n",
       " '1527s - transistor flips and which leads to like more or less',\n",
       " '1531s - of power throttling, which leads to like more or less',\n",
       " '1534s - of performance.',\n",
       " '1535s - And so I remember actually one time somebody had told me',\n",
       " '1538s - a funny story where like they were like training',\n",
       " '1541s - the machine learning model and benchmarking performance.',\n",
       " '1544s - And then they like at some point their model would nad,',\n",
       " \"1547s - and then they'd be like wow my performance\",\n",
       " '1549s - just got way better (chuckles).',\n",
       " '1551s - And so I like wrote an article about this',\n",
       " '1553s - and they like messaged me and they were like, oh you know,',\n",
       " '1555s - that was very illustrative',\n",
       " '1557s - because you know, I was really confused',\n",
       " '1558s - why my performance would be getting better.',\n",
       " \"1560s - But that's why, you know, like if all your tensors are NaN,\",\n",
       " \"1563s - your transistors also don't need to do a lot of flipping\",\n",
       " \"1565s - and so you know, you'll measure like a better perf.\",\n",
       " \"1570s - So that's kind of compute.\",\n",
       " '1571s - And so the next thing that your GPU can be spending',\n",
       " '1574s - a lot of your time on is like memory,',\n",
       " '1576s - which is essentially a time spent transferring your tensors',\n",
       " '1579s - within a GPU.',\n",
       " '1585s - And so I think one thing to observe',\n",
       " '1587s - from this kind of empirical plot',\n",
       " '1590s - from a paper on like a data movement is that although like,',\n",
       " '1595s - so this paper kind of breaks down the operations',\n",
       " '1598s - on like I think a Bert type model.',\n",
       " '1601s - Into what it calls like tensor contractions,',\n",
       " '1603s - i.e memory locations.',\n",
       " '1605s - And then like, you know, normalization operations',\n",
       " '1607s - and element wise operations.',\n",
       " '1609s - And so you can see that although major (indistinct)',\n",
       " '1612s - are responsible for like 99.8% of your FLOPS,',\n",
       " \"1615s - they're only responsible for 61% of your runtime.\",\n",
       " '1618s - And so you know where like why are we spending like,',\n",
       " '1621s - you know, 40% of our runtime doing like operations',\n",
       " '1624s - like cumulatively only take 0.2% of our FLOPS?',\n",
       " \"1629s - The kind of a key thing here is what's called\",\n",
       " '1632s - like a memory bandwidth cost.',\n",
       " '1634s - Where the way I typically think about this,',\n",
       " \"1637s - is that like even like, and so here I'm talking like\",\n",
       " '1640s - all of your data already lives on the GPU, like, you know,',\n",
       " \"1642s - it's like, you know, it's like, you know,\",\n",
       " \"1643s - occupying your like GPU's VRAM.\",\n",
       " \"1648s - But the thing is that like your GPU's VRAM\",\n",
       " '1650s - is not where it like the compute units are located.',\n",
       " '1654s - And in order to actually do operations on a GPU,',\n",
       " '1656s - you need to move your data from like the VRAM',\n",
       " '1659s - to like where the compute units are located,',\n",
       " '1662s - which is like your SRAM or like compute units.',\n",
       " '1665s - And so usually I kind of think this of like',\n",
       " '1667s - as like a factory where you have like a factory',\n",
       " '1669s - with like not that much space.',\n",
       " '1671s - And then you have a warehouse located like much further away',\n",
       " '1675s - and so you have like a lot more space in your warehouse,',\n",
       " '1678s - but now in order to do any operations',\n",
       " '1680s - like on those like supplies,',\n",
       " '1683s - you need to move them from your warehouse',\n",
       " '1685s - to your factory and then back.',\n",
       " '1688s - And so this cost of like moving data around',\n",
       " '1690s - is called the memory bandwidth cost.',\n",
       " '1693s - And so this is actually like responsible',\n",
       " '1695s - for like a lot of like what your GPU',\n",
       " '1697s - is spending its time doing.',\n",
       " \"1699s - Where if you imagine like let's say we do like, you know,\",\n",
       " '1701s - three operations like on a GPU,',\n",
       " \"1704s - so maybe you're doing like an add,\",\n",
       " '1705s - and then like a, you know, RELU,',\n",
       " '1707s - and then like a sign operation.',\n",
       " '1710s - Like you can imagine that what actually happens',\n",
       " '1712s - when you do these operations.',\n",
       " '1713s - Like first the GPU sends the data from like the memory',\n",
       " '1717s - to the compute units and then you know,',\n",
       " '1719s - turns it from a square to a triangle',\n",
       " '1720s - and then it sends it all the way back.',\n",
       " '1722s - And then you know, it sends the triangle',\n",
       " '1724s - from like the memory units to the compute units again',\n",
       " '1727s - where it does like another operation',\n",
       " '1728s - and then sends it all the way back.',\n",
       " '1730s - And then finally, you know, you guys get the idea,',\n",
       " \"1731s - it's like sending the circle from the memory units\",\n",
       " '1733s - to the compute units again',\n",
       " \"1734s - and then it's like sending into all the way back.\",\n",
       " '1737s - And so by default whenever you run any operations',\n",
       " \"1739s - like in PyTorch like let's say you ran like add\",\n",
       " '1741s - and then multiply and then cosine,',\n",
       " '1743s - this is exactly what would be happening on our GPU.',\n",
       " '1747s - And so you might think that this is a very like dumb thing',\n",
       " '1749s - to do and you would be correct.',\n",
       " '1753s - And that, you know, why are we like sending our triangle',\n",
       " '1755s - back from like the factory to the warehouse',\n",
       " '1758s - just to send the data from like the warehouse',\n",
       " '1760s - back to the factory again.',\n",
       " '1762s - And so a very like common operation for GPU compilers to do,',\n",
       " \"1767s - and I'd say what I'd actually call\",\n",
       " '1768s - like the most important optimization',\n",
       " '1770s - in a deep learning compiler by far,',\n",
       " '1772s - is called operator fusion.',\n",
       " '1774s - And so what an operator fusion does is that instead of like,',\n",
       " '1777s - you know, sending the data back and forth so much.',\n",
       " '1779s - we like do a single GPU kernel where you send the data once',\n",
       " '1783s - to the factory units, you do all of the operations',\n",
       " '1787s - and then you send the data back.',\n",
       " '1790s - Also, notably, this is also like an issue.',\n",
       " '1794s - This optimization is not really something you can do',\n",
       " '1795s - in eager mode, right?',\n",
       " '1797s - Because in eager mode I was, you know, mentioning',\n",
       " '1799s - like the executed model is very simple',\n",
       " '1801s - where you like run our operation',\n",
       " '1803s - and then it executes the operation.',\n",
       " '1804s - And now if we want to do this optimization,',\n",
       " '1807s - that program model is like no longer sufficient.',\n",
       " \"1811s - And so there's actually like a lot of different ways\",\n",
       " '1813s - to minimize memory movement.',\n",
       " '1815s - Although at the end of the day,',\n",
       " '1816s - like operator fusion is like the most important thing',\n",
       " '1819s - you can do for like a ML compiler.',\n",
       " \"1821s - There's actually like a lot of decisions\",\n",
       " '1823s - that go into operator fusion that like kind of enable it',\n",
       " '1826s - to be more or less effective.',\n",
       " '1828s - One of the kind of examples here',\n",
       " '1830s - is kind of these like re computation',\n",
       " '1832s - versus reuse trade-offs.',\n",
       " '1833s - If you guys are kind of familiar with maybe',\n",
       " '1835s - like register allocation type settings,',\n",
       " '1837s - you kind of often have a similar issue',\n",
       " '1839s - where like if you have a register you can choose',\n",
       " '1842s - to either like store it in global memory',\n",
       " '1845s - and then load from it later,',\n",
       " '1846s - or you can just choose to like recompute',\n",
       " '1848s - that value from like values that are ready in the registers.',\n",
       " '1852s - And so you kind of have a similar idea here',\n",
       " '1855s - where you oftentimes have cases where by doing some amount',\n",
       " '1859s - of re computation you can significantly reduce your memory.',\n",
       " '1863s - You can significantly reduce your number of memory accesses.',\n",
       " '1866s - And so in this way, like the recomputation',\n",
       " '1869s - can not only like reduce your like peak memory usage,',\n",
       " '1872s - it can also often like improve',\n",
       " '1873s - your actual runtime performance.',\n",
       " '1877s - And so I think one of the things',\n",
       " '1880s - to mention actually about like why.',\n",
       " '1882s - So this observation actually ends up being',\n",
       " '1884s - like quite important for deep learning performance',\n",
       " '1885s - like re computation versus reuse.',\n",
       " '1887s - And I think the reason why is that like the shape',\n",
       " '1890s - of machine learning programs,',\n",
       " '1891s - actually I think it looks quite unusual',\n",
       " '1893s - compared to like your typical program that you might have.',\n",
       " \"1896s - Where like it's generally like a kind of a bit of an axiom\",\n",
       " '1900s - in like programs that usually most intermediates',\n",
       " '1902s - that you have are very short-lived.',\n",
       " '1904s - So like, you know, your program generally consists',\n",
       " '1906s - of a lot of very short-lived intermediates',\n",
       " '1907s - that you know are created',\n",
       " \"1908s - and they're very shortly destroyed.\",\n",
       " '1911s - But in machine learning this is actually not the case.',\n",
       " '1913s - Because in machine learning, like the typical like model',\n",
       " \"1916s - that you'll execute will first like run the model\",\n",
       " '1919s - forward like, you know, layer zero to layer one,',\n",
       " '1922s - to layer two, to layer three, to layer four,',\n",
       " \"1924s - and then initially run what's called the backwards pass,\",\n",
       " '1926s - which will like run the layers in reverse.',\n",
       " \"1929s - So then it'll go from like layer four to layer three,\",\n",
       " '1931s - to layer two, to layer one and to layer zero.',\n",
       " '1934s - And in between the forward pass',\n",
       " '1936s - and the backwards pass you need to save',\n",
       " '1938s - what are called like intermediates or activations.',\n",
       " '1941s - And so these are like a lot of, like,',\n",
       " '1944s - you have a lot of them.',\n",
       " \"1945s - And they're like often times like, you know,\",\n",
       " '1947s - largely responsible for like running into',\n",
       " '1950s - like out of memory type errors.',\n",
       " '1952s - And so I think this is actually',\n",
       " '1953s - like kind of a pretty unusual like program structure',\n",
       " \"1957s - in machine learning that's caused like, you know,\",\n",
       " '1960s - back propagation and gradient dissent.',\n",
       " '1965s - And so finally, you know, the last thing',\n",
       " '1967s - you can be spending your time on is overhead.',\n",
       " '1969s - Where you can imagine that,',\n",
       " '1970s - you know, if a poor Gromit is not able',\n",
       " '1973s - to put down the train tracks faster than the train',\n",
       " '1975s - can like go on the train tracks,',\n",
       " '1977s - then sometimes a train is gonna be just stuck',\n",
       " '1980s - waiting for him to put down like the next train track.',\n",
       " '1982s - And so here I have like a profile trace',\n",
       " '1985s - where you can see that like the bottom line,',\n",
       " '1987s - which is a GPU trace is largely idle',\n",
       " \"1990s - and it's mostly just idle waiting for the CPU\",\n",
       " '1992s - to like schedule the next operation.',\n",
       " '1996s - And so there are a lot of like ways',\n",
       " '1997s - to actually address this nowadays.',\n",
       " '1999s - One of the most powerful is called CUDAgraphs,',\n",
       " '2001s - which is like an NVIDIA provided API,',\n",
       " '2004s - but you also have like other approaches like in a compiler',\n",
       " '2006s - for example, like codegenning like a lower overhead wrapper',\n",
       " '2009s - or something like this.',\n",
       " \"2017s - So you know, I've talked about like ML compilers like,\",\n",
       " '2021s - and you know like you know what they can do to your program',\n",
       " '2023s - and how they can be useful.',\n",
       " '2024s - But I think kind of like an interesting question',\n",
       " '2027s - that you often see like, you know, I talked a lot about',\n",
       " '2029s - how like, you know, we have like, you know,',\n",
       " '2032s - super massive infra build out',\n",
       " '2034s - and the programs are super simple.',\n",
       " \"2035s - And we've seen like a lot of consolidation\",\n",
       " '2037s - in terms of like what the architecture looks like.',\n",
       " '2040s - And so I think like a reasonable question is like,',\n",
       " '2042s - if you only have like one architecture',\n",
       " \"2043s - and you're spending like billions of dollars to train it,\",\n",
       " '2046s - why do you even need a compiler?',\n",
       " \"2049s - You know, like why can't you just like, you know,\",\n",
       " '2050s - assign some group of people to like optimize it by hand',\n",
       " '2053s - instead of like a leveraging a compiler?',\n",
       " \"2058s - And I'm gonna say some like kind of or sorry,\",\n",
       " \"2062s - and the other thing I'll say about this\",\n",
       " '2065s - actually is that like in practice a lot of times',\n",
       " '2068s - people do not use compilers like for kind of this reason.',\n",
       " '2072s - And so this section is gonna be talking a little bit about',\n",
       " \"2074s - like why I think that's a case.\",\n",
       " '2077s - And what are kind of some of the challenges',\n",
       " '2079s - when it comes to like using compilers in this setting.',\n",
       " '2083s - And yeah, so disclaimer, you know,',\n",
       " '2084s - I do really like compilers.',\n",
       " \"2085s - I'm gonna say some kind of mean things about compilers\",\n",
       " '2088s - in a bit, but you know, as like to establish my credibility,',\n",
       " \"2094s - you know, I work on a team called Fighter's Compilers.\",\n",
       " '2096s - And so there are like, to be clear like a lot of reasons',\n",
       " '2098s - why compilers can be very useful.',\n",
       " '2101s - In particular this kinda like notion of leverage,',\n",
       " '2103s - like being able to do the optimization once in the compiler',\n",
       " '2106s - and then having everybody be able to like take advantage',\n",
       " '2108s - of it.',\n",
       " '2110s - And also, you know, compilers are also just like very fun',\n",
       " '2112s - to work on.',\n",
       " \"2114s - That being said, yeah, I'm gonna introduce you.\",\n",
       " '2117s - You know, my new exciting library,',\n",
       " \"2120s - Horace's exciting library abbreviated at HEL.\",\n",
       " '2124s - And so it has a couple of cool features',\n",
       " '2125s - that you might be interested in.',\n",
       " \"2127s - So the first feature it has is that it doesn't always work.\",\n",
       " '2131s - And you know, to address that,',\n",
       " '2133s - it also has no documentation about',\n",
       " '2135s - why or when it will work except by reading my library',\n",
       " '2138s - as an implementation.',\n",
       " '2139s - And in exchange for that, when you update the library,',\n",
       " '2142s - it may totally change what code works',\n",
       " \"2144s - and what code doesn't work.\",\n",
       " '2145s - I.e. like no backwards compatibility or like guarantees',\n",
       " '2149s - on your like, you know, on whether your code works.',\n",
       " '2154s - And so are you interested in using my library?',\n",
       " '2158s - I guess, you know, most people would probably say no.',\n",
       " '2161s - And so I think one thing to note here',\n",
       " '2163s - that like if work means like has a desired performance',\n",
       " '2166s - and is applying the desired optimizations,',\n",
       " '2168s - this is kind of largely describing',\n",
       " '2170s - how compiler optimizations work.',\n",
       " \"2172s - And that compiled optimizations don't always work.\",\n",
       " \"2175s - Often like there's no real documentation\",\n",
       " '2178s - on when a compiler optimization will trigger and you know,',\n",
       " '2181s - when you update your compiler',\n",
       " '2183s - it may completely change when it does',\n",
       " '2186s - or does not apply these optimizations.',\n",
       " \"2190s - And so there's like very influential article\",\n",
       " '2194s - like about this compiler called ISPC.',\n",
       " '2197s - And they have this note here called auto vectorization',\n",
       " '2199s - is not a programming model.',\n",
       " '2201s - Where they note here is that like, you know,',\n",
       " '2203s - the problem with an auto vectorizer,',\n",
       " '2205s - which is kind of like a compiler...',\n",
       " '2206s - Or so the overall like framing of the article',\n",
       " '2208s - is that he wrote this compiler called like ISPC,',\n",
       " '2211s - which you can think of as like CUDA',\n",
       " '2213s - for intel SIMD instructions.',\n",
       " '2216s - And he kind of, you know, is constantly like, you know,',\n",
       " '2218s - as part of the article, he is constantly trying to fight',\n",
       " '2220s - against like the intel compiler team,',\n",
       " '2223s - which is like where he works.',\n",
       " '2224s - With the intel compiler team,',\n",
       " '2225s - wanted to kind of leverage auto vectorization',\n",
       " '2228s - to get vectorization done',\n",
       " '2229s - instead of introducing like a new program model in ISPC.',\n",
       " '2233s - And so he kind of, I think elucidates like what the problem',\n",
       " '2236s - with the auto vectoriser.',\n",
       " '2238s - Which is that the problem with the auto vectoriser',\n",
       " '2239s - is that as long as vectors can fail',\n",
       " \"2241s - and it will then if you're programmer\",\n",
       " '2243s - that actually cares about what code the compiler',\n",
       " '2245s - generates from your program,',\n",
       " '2246s - you need to deeply understand the auto vectoriser.',\n",
       " '2250s - Then when it fails to vectorize code,',\n",
       " '2252s - you wanna be vectorized, you need',\n",
       " '2254s - to either poke it in the right way',\n",
       " '2255s - or change your program in the right way',\n",
       " '2256s - so that it works for you again.',\n",
       " '2258s - And so this is like a very horrible way to program.',\n",
       " '2260s - And then, you know, if most of you are,',\n",
       " '2262s - or if any of you here are like very like into using',\n",
       " '2265s - SIMD instructions, you probably also do not trust',\n",
       " \"2267s - the auto vectorize at all and you're mostly just, you know,\",\n",
       " '2269s - writing intrinsics.',\n",
       " '2272s - And so the, you know, with the proper program model,',\n",
       " '2276s - ideally the user is able to like learn what does',\n",
       " '2279s - and does not work without, you know, needing to be tied',\n",
       " '2282s - to the implementation and then, you know,',\n",
       " '2284s - one compiler implements it',\n",
       " '2285s - and then the user can like learn to reliably rely',\n",
       " '2287s - on this optimization without needing to understand',\n",
       " '2290s - the compiler and only needing to understand',\n",
       " '2292s - the programming model.',\n",
       " '2294s - And so one I guess way you can phrase this',\n",
       " '2296s - is that like a compiler optimization',\n",
       " '2298s - that always works is just part of the programming model.',\n",
       " \"2301s - Like for example, when you're writing like, you know,\",\n",
       " '2303s - SIMD instructions, you know that as SIMD intrinsic',\n",
       " '2307s - will always get mapped to SIMD instruction.',\n",
       " \"2309s - And so that's like part of your program model\",\n",
       " '2311s - and not really an optimization that the compiler is doing.',\n",
       " '2316s - So I think, you know, to...',\n",
       " '2319s - Like when I see like the people kind of complaining',\n",
       " '2322s - about like Shoggoths and working with them,',\n",
       " '2325s - I kind of am often reminded times of a compiler.',\n",
       " '2328s - We can imagine that like a compiler is oftentimes',\n",
       " \"2330s - it's like large piece of code that you know\",\n",
       " '2333s - has been worked on by a lot of like very smart people',\n",
       " '2335s - and oftentimes has a lot of like tricky details',\n",
       " '2338s - and implementation details in it.',\n",
       " \"2341s - And so ideally when you're doing a compiler,\",\n",
       " '2343s - like only the program model ends up being exposed',\n",
       " '2345s - to the user.',\n",
       " '2346s - So like the actual compiler implementation',\n",
       " '2347s - ends up being completely hidden.',\n",
       " '2349s - And so the user only needs to deal',\n",
       " '2350s - with like the nice program model,',\n",
       " \"2351s - but it's usually just like the language\",\n",
       " '2353s - that the compiler is compiling from.',\n",
       " '2361s - Unfortunately, you know, when compilers fail',\n",
       " \"2363s - and like, you know, they don't apply the optimizations\",\n",
       " '2365s - that you want them to apply.',\n",
       " '2366s - The kind of entire thing becomes exposed.',\n",
       " '2369s - And so, you know, this kind of applies anytime',\n",
       " \"2371s - that you're like you're wrestling with a compiler\",\n",
       " \"2373s - and you're trying to understand why the compiler\",\n",
       " '2375s - like did or did not like inline my code or things like this.',\n",
       " '2381s - And so to give some examples of like cases',\n",
       " '2382s - where like very kind of nuanced details here.',\n",
       " '2386s - Can lead to compilers like having,',\n",
       " '2388s - like it can lead to compilers struggling a lot.',\n",
       " '2390s - One of them here is like numerics and machine learning.',\n",
       " '2393s - Where numerics can be like a kind of a,',\n",
       " '2395s - or like in general floating point,',\n",
       " '2397s - like arithmetic is like a very cursed thing',\n",
       " '2399s - to deal with generally speaking.',\n",
       " \"2401s - And it's just gotten even worse with the fact\",\n",
       " '2405s - that like everybody in the industry',\n",
       " '2406s - keeps on pushing our data types lower',\n",
       " '2408s - to lower and lower bits where like on the V100',\n",
       " '2411s - they kind of introduce like 16 bit operations',\n",
       " '2414s - is kind of the default.',\n",
       " '2415s - And on H100 they introduced eight bit operations',\n",
       " \"2418s - and on the B100, you know, they're now pushing\",\n",
       " '2420s - for like four bit operation, like four bit floats.',\n",
       " '2423s - And operations on four bit floating point numbers.',\n",
       " \"2425s - I think it's reasonable to question like\",\n",
       " '2428s - how is this even a floating point number at this point?',\n",
       " \"2431s - But this is kind of what we're typically dealing with.\",\n",
       " '2436s - And so as a result of like this kind of like low precision',\n",
       " '2438s - and the fact that numerics end up being so subtle,',\n",
       " '2440s - the algorithms have like very annoying numerical problems',\n",
       " '2443s - to deal with.',\n",
       " '2443s - And I think a good example for me',\n",
       " '2445s - that was like very frustrating was this kind of a NaN',\n",
       " '2448s - in like a FlashAttention implementation.',\n",
       " '2450s - Where the underlying cause here is something called an FMA',\n",
       " '2453s - or like a fuse multiply accumulate.',\n",
       " '2455s - It can actually be like disastrously bad',\n",
       " '2458s - when it comes to your numerics.',\n",
       " '2461s - So for folks who are unfamiliar with FMA,',\n",
       " \"2463s - it's basically like it takes like A, B and C\",\n",
       " '2465s - and it does like a signal operation',\n",
       " '2467s - that does A times B plus C.',\n",
       " '2470s - And so one of the ways that FMA differs',\n",
       " '2472s - from normal operations is that in addition to being faster,',\n",
       " \"2476s - it's also usually computed\",\n",
       " '2478s - in what people call like infinite precision internally.',\n",
       " '2480s - And what that means is that like the result of your FMA',\n",
       " '2484s - is like...',\n",
       " '2488s - Like the closest possible representation',\n",
       " '2490s - to like the true value of your FMA.',\n",
       " '2492s - And so this is different from like if you just did this',\n",
       " '2494s - operation separately.',\n",
       " '2496s - Where typically after your like multiply operation,',\n",
       " '2498s - you would have like a rounding term',\n",
       " '2501s - where it becomes a bit different.',\n",
       " \"2504s - And so in this particular case we're computing exp\",\n",
       " '2507s - of like a_i times B minus a maximum scale.',\n",
       " '2511s - Where max scale is like a singular scaler value.',\n",
       " ...]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data.preprocess import clean_transcript, chunk_transcript\n",
    "\n",
    "cleaned = clean_transcript(transcript)\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0s - - So it\\'s my great pleasure 2s - to introduce our speaker tonight, Horace He. 4s - Horace He graduated from Cornell in 2020, 8s - and he works at Meta on the PyTorch team, 11s - specifically at the intersection 13s - of compilers and machine learning. 16s - If you\\'ve used things like torch.compile, 19s - which is a thing in PyTorch 20s - that makes your model goes 21s - like 2-4x faster in just one line of code. 24s - Or if you\\'ve used FlexAttention, 25s - which is something that lets researcher 28s - design fast kernel for attention 30s - without leaving the world of Python. 33s - He\\'s the person responsible for both of those things. 36s - You should also check his blog, which is pretty awesome, 38s - and in particular the blog post 40s - \"Making Deep Learning go Brrrr From First Principles.\" 44s - And without further ado, I\\'ll give it to Horace. 47s - - Thanks for the intro. 48s - Today I\\'m gonna give a talk about 49s - building machine learning systems 51s - for a trillion, trillion floating point operations. 54s - My name is Horace He, 55s - and I\\'m on the PyTorch compilers team at Meta. 59s - So, you know, I think we live in pretty unprecedented times 62s - in terms of an infrastructure build out, 65s - which is, I think, you know, 65s - nicely reflected in NVIDIA\\'s stock price. 69s - I feel like, you know, basically every month, 71s - we see like a different headline 72s - about like a new nuclear power plant 74s - from like Microsoft, or, you know, 76s - like a massive 300K GPU data cluster from xAI. 80s - You know, people like, 81s - I feel like when Amazon first like built like a, 84s - or like bought a nuclear power center, 86s - it was like a big news. 87s - But now practically like, you know, 88s - every cool AI company 90s - has their like own nuclear data center. 95s - And you know, of course, 96s - this like kind of really massive (indistinct) build out 98s - has kind of also resulted in pretty ludicrous fundraisers 101s - from startups, I think. 102s - Like, I remember back in like 2016, like, 104s - you know, if you like made it as a startup, 107s - it would be like if you were worth a billion dollars, right? 109s - It was like a unicorn. 110s - It was like the mark of like really making it 111s - as a startup beyond your wildest dreams. 114s - But you know, in 2024, you know, 115s - you gotta raise a billion dollars just like get started. 118s - You know, it\\'s like just to play the game, (laughs) 120s - you need like a billion dollars, you know, 122s - of which most of it goes to NVIDIA (chuckles). 125s - (audience laughs) 128s - And so I think like, it\\'s kind of crazy to think that, 132s - you know, all of this, like, you know, 133s - billions of dollars is really just to do 134s - like absolutely insane amount 136s - of floating point operations. 138s - Here\\'s like a nice chart from like Epoch AI 140s - showing like the growth of compute over time. 144s - And like, these floating point operations 147s - are really just like big matmuls 149s - done like over and over, like, you know, 151s - for millions of iterations over like months. 154s - And nowadays, 155s - like the leading edge models are currently trained 157s - with about like one E26 floating point operations, 160s - which is approximately 100 trillion 164s - trillion floating point operations. 166s - So like a trillion trillion, like a trillion TeraFLOPS 171s - worth of floating point operations. 173s - And so, you know, back, you know, another kind of effect 176s - that you might\\'ve noticed is like back prior to 2016, 179s - if you search on like Hacker news like ML, 181s - you\\'d often get like a lot of people asking about, you know, 183s - the ML family of languages. 185s - But nowadays, you know, you search ML on a hacker news 189s - and you get like a very different like type of article. 195s - And so I think one of the things 196s - that kind of is missed here is like, you know, 198s - with all these like billions of dollars 200s - and like, you know, yottaflop of operations, 202s - it\\'s kind of easy to forget that, you know, 203s - like these operations needed to actually run somehow 207s - on the like machines. 210s - And so, you know, a modern stack might involve, you know, 213s - like, you know, call like an LLM API 215s - and then it\\'s called like PyTorch, like NCCL, Triton, CUDA 218s - like NVCC, like all these like different layers in the stack 221s - that like somebody was involved in writing. 224s - And I\\'m often reminded of this like XK CD comic, you know, 228s - showing like the state of like modern digital infrastructure 231s - and so, you know, you kind of, you know, 233s - often have forget about all the infrastructure 234s - that was involved for you to like get where you are, 238s - but like really we\\'re just kind of building 239s - like layers on top of layers. 243s - And so, you know, if you work in a systems, you know, 245s - like I do and I suspect many of you do, you kind of, 248s - I think oftentimes think about your work a little bit like, 251s - like a benevolent dictator of sorts. 254s - Where kind of you imagine like what you\\'re doing here 256s - is that like a lot of people build on top of your work. 258s - And so if you can kind of, you know, 260s - give like a small amount improvement to, you know, 262s - like millions of people, you know, 264s - your small improvements can lead to like, you know, 266s - significant impacts on the world. 268s - And so for example, I kind of imagine like Guido, 271s - with Python, you just like, you know, kind of sits on top 273s - of this cloud and you know, eventually like gives us like 275s - NoGil or like faster CPython or like the Walrus Operator 278s - and I feel like this is kind of oftentimes like 281s - how I imagine like infrastructure work did. 284s - And it\\'s kind of a lot of why I got into infra work 286s - in the first place. 289s - And so on the other hand, I feel like if you work in ML, 291s - things can sometimes feel a little bit different. 293s - I originally came across this a post on threads 296s - where this person said, you know, my main gripe 298s - with like working on top of like LM APIs 300s - is that you\\'re not really like, 301s - no one is like engineering anything. 303s - You\\'re just like chanting like a prayer 306s - to this like manmade demon deity to like do your bidding. 310s - And this like very similar to this kind of like 312s - Shoggoth metaphor that has become like very popular 314s - in deep learning circles. 316s - Where the idea here is that like we really like 318s - put all these like matmuls 319s - and like computation into producing this like very weird 323s - alien intelligence. 324s - and then we like kind of like RLHF it, you know, 328s - and like provide it in this like nice convenient interface 330s - to people with like, you know, ChatGPT 333s - or something like that. 335s - But you know, I think if you think about it, 337s - I think we\\'re kind of working with Shoggoths 339s - like all the way down. 342s - Like even like if you\\'re working like in systems 346s - and you\\'re not calling like ML models, 348s - you still have this kinda like, you know, 349s - massive amount of infrastructure that you presumably 351s - don\\'t really understand written by people 354s - you\\'ve probably never talked to. 355s - And like the end result know they try to expose 358s - is like some kind of a simple interface 360s - where you can just like import torch 362s - and then, you know, hopefully run your code across like, 364s - you know, a 100K GPUs. 368s - And so I think as a result there are some, 371s - I think, interesting ways in which I think ML models 373s - that feel kind of different from regular systems. 376s - One of those ways is that like ML models 380s - are extremely simple and as a result 384s - we have like very high expectations 385s - from the performance of like the code. 389s - I think it kind of like trace this all the way back 391s - to this very nice article called the \"Bitter Lesson,\" 394s - which I\\'d really recommend reading if you guys 395s - haven\\'t come across it before. 397s - Where their main observation here was that like, 400s - clever ideas in machine learning have basically 403s - throughout like its 50 year history always lost out 406s - to just simple ideas that like scaled really well 409s - with Moore\\'s law. 412s - And I\\'m not really like joking here when I say 414s - that like machine learning logic is exceedingly simple. 417s - There\\'s like this cool project from someone 419s - called like Andre Pathy called llama2.c. 422s - And basically here he\\'s like implemented 425s - like llama2 in about like 973 lines of C. 428s - With like no other dependencies. 430s - Like, you know, every single loop, 431s - every single like matmul is just implemented from scratch. 434s - So with 973 lines, you can\\'t like, you know, 436s - do it very fast, but it does run 439s - and I think it does kind of indicate just like 441s - how fundamentally simple the models 443s - that like we\\'re spending all this compute end up being. 448s - And so the end result like 449s - although like the problem themselves are extremely simple 452s - and like very like easy to optimize in some sense. 457s - The expectations are very high 459s - for how well we can optimize these matmuls. 461s - So one example here is that like the predominant metric 464s - for measuring your like model\\'s performance in deep learning 468s - is called model FLOP utilization. 470s - And so this is basically the like percentage 472s - of the theoretical max flops that your GPU is able to do. 477s - And so if you kind of think about this, 479s - this is actually like a very absurd metric to hit. 482s - Like if on a CPU, like you measured any of your code 484s - by this metric, like you can only hit a 100% 487s - if like at every single time every single core 490s - of your CPU is always issuing max width SIMD instructions. 494s - That\\'s like the only way you can hit a 100% utilization. 498s - And so, you know, if you take a look at any of the code 500s - that you guys presumably write, 502s - like almost no CPU code is like anywhere near like, 505s - you know, a 100% flop. 506s - It\\'s probably like way under like 1% most of the time. 510s - On the other hand, in like machine learning 512s - for like large scale training, we\\'re typically often hitting 514s - around like 50% of like the peak flops. 518s - And this is I think like this is kinda like a indicative 522s - that like even though the overall problem 523s - is like very simple, the kind of corresponding difficulty 526s - just goes into like making your like models hit 530s - like this, like very high perf barrier. 535s - Another ind of a interesting observation 537s - in like machine learning is that, 540s - that has kind of exacerbated this a bit. 542s - Is that the field has consolidated significantly 544s - over the last five to 10 years. 547s - So one of them is that like, you know, 548s - like maybe 10 years ago you had like a lot more architecture 551s - is a lot more variance and like different things 553s - that people were trying. 554s - But nowadays people really just like transformers are like 558s - the dominant architecture for everything. 559s - Like you have transformers for vision, you have transformers 561s - for language, you have transformers for like, you know, 563s - audio, it\\'s kind of like all transformers. 566s - And the other way like things have kind of changed 568s - is that instead of like, you know, many different people 572s - training SOTA models. 573s - You oftentimes just have a few companies 575s - that are training SOTA models. 576s - And so we\\'ve kind of gone from a bit of like a monopoly 579s - where like, you know, previously there was this like, 581s - you know, one person providing the infra 583s - and like many people using the infra, 585s - to in some ways it feels a little bit more like monopsony 588s - where you have like many people trying to provide infra 590s - and then you know, only one person\\'s actually training 592s - the job at the end of the day. 597s - And so as a kind of result, I think that like 600s - there\\'s kind of two general ways to think about 602s - like getting performance from like your systems. 606s - And so one of the ways generally is basically 608s - like optimizations is like you have a compiler, 610s - you like make the compiler faster, you know, 613s - you improve like the performance 614s - of everybody using your compiler. 617s - and so the other way though 618s - is kind of like programming models. 620s - And so like this is kind of analogous 621s - as opposed to like a system whose responsibility 624s - is to like chop down a tree 626s - and then you know, you\\'re just like optimizing 627s - how fast you can chop down the tree. 629s - And the other alternative is like you\\'re providing tools 631s - for people to cut down the trees themselves. 635s - And so to kind of like talk a little bit 637s - about programming models, I think it\\'s like illustrative 640s - to kind of talk about how like ML frameworks 642s - have kind of evolved in terms of what program model 645s - they\\'ve exposed to users. 647s - So originally like I think like 2010, 651s - like 2011, 2012, the first like ML framework 654s - that kind of got a lot of popularity was this framework 656s - called Caffe. 657s - And the way you like express neural networks in Caffe 659s - is like a very declarative nature. 662s - And by that I mean you like edited a protobuf file I think. 666s - Where like the protobuf like specified all the things 668s - you needed like care about your neural network. 671s - And so as you can imagine, you know, 672s - programming protobufs is like not very fun, you know, 675s - there\\'s a lot of things you might want to do 677s - that you can\\'t do in protobufs. 678s - and so a like natural net thing that people did 681s - was kind of these kind of graph builder type APIs. 684s - And so this is kind of, you know, how TensorFlow 1 686s - kinda of look like where the idea is that oh, you know, 689s - like programming like, you know, programming protobufs 691s - or like no human should need to write protobufs by hand. 694s - And so ideally, you know, 696s - you should just like write like a DSL of sorts 699s - that allows you to generate the protobufs 701s - like from this DSL. 703s - However, like even this DSL still kind of has 706s - like a lot of confusion in it. 708s - Like it is not super clear that like given this DSL 710s - like how code actually executes on your GPU. 714s - And so kind of finally like around 2016 or 2017, 718s - PyTorch started to become like really successful. 720s - And kind of the like the feature 723s - that like was most emblematic of PyTorch was basically 726s - what was called imperative or eager execution. 729s - And so what this means is that like, 734s - or like, yeah, so PyTorch you know, was very successful 736s - and I think it\\'s like worth talking about 737s - like why eager execution was so successful. 740s - And I think that the main reason it was successful, 742s - it just comes down to like what the programming model 744s - of the execution looked like. 746s - Where in a imperative slash like eager execution 748s - is basically like, you know, you call a function, 751s - the GPU runs a function 753s - and then the function finishes and you know, that\\'s it. 756s - Like, you know, like that\\'s basically all you do. 757s - you call like torch.matmul and this is basically 760s - the sequence of operations that happens but you know, 763s - with kind of like a graph mode type approach 765s - or you know, where kind of this pilot interjects. 767s - You first like define the function, 769s - the function gets converted into some like intermediate IR, 773s - you know, a bunch of who knows what happens to your function 776s - and then eventually like the function eventually executes 779s - on the GPU. 780s - And like the top one was like a very simple 784s - execution model for people to understand. 787s - And I think another thing that\\'s kind of interesting 789s - to notice about this is that this kind of also just, 791s - like the top half also kind of describes 794s - how Python executes. 797s - And I think this is kind of a pretty illustrative to me 799s - of like why Python has been so successful 801s - in machine learning. 803s - It\\'s basically that like, like a funny statement 806s - that you can make about Python is that like if you tried 809s - to like train a model today, like you know, 812s - I gave you like a day to like train a model, 815s - it would run faster if you ran it in Python 817s - compared to doing in C++. 819s - And you might argue that this is like a unfair comparison. 823s - Because you know, like you know all the infra like, 825s - you know, all the frameworks that people built 826s - are in Python. 827s - But I think the reason why so many 829s - of these frameworks have been built in Python 831s - is that Python is like an exceedingly like 833s - a simple language. 835s - And so it\\'s also like a very global language. 838s - And what I mean by like global is that like it\\'s very easy 840s - for people to build their own infrastructure 842s - on top of Python without really needing to fight 845s - with like anything the Python language does 847s - because the Python language itself does basically nothing. 851s - Like it, you know, it doesn\\'t do any optimizations for you. 853s - It just like, you know, takes your function and runs it. 857s - And so I think that PyTorch basically historically 860s - is at like a very similar point in the design space. 863s - Where PyTorch is like execution model is like so very simple 868s - and although this like doesn\\'t really give you a lot 870s - in terms of like, it doesn\\'t like automatically 872s - do a lot of things for you, it does mean that it\\'s very easy 875s - for people to like build their own infrastructure 878s - and frameworks on top of PyTorch. 887s - I think another important detail to realize 890s - especially about like PyTorch when it first came out. 893s - Is that like this kind of unoptimized execution 896s - didn\\'t actually even sacrifice any performance at all. 899s - Like, you know when people kind of benchmarked 900s - PyTorch versus like, you know, TensorFlow at Cafe, 903s - PyTorch oftentimes wasn\\'t even slower than those frameworks. 907s - And I think there\\'s like a two main reasons for this. 909s - So the first reason is that back in the day, 912s - like you know about like 90% plus of your time 914s - was spent in matmuls. 915s - And so there\\'s basically nothing else you need to optimize. 918s - And so matmuls here are like matrix implications 920s - and so they\\'re often provided by these like vendor libraries 922s - like cuBLAS or like QDNN. 925s - And like you know, they\\'re provided by NVIDIA. 927s - and they\\'re like very hand optimized 929s - and so, you know, if 90% of your time is spent in matmuls, 932s - then like what else can you even do to like optimize 934s - the performance of your neural network? 938s - And I think another kind of interesting like 943s - important piece here for like why PyTorch\\'s performance 945s - was like quite good is that it had this like 947s - Async execution model. 950s - Where basically the idea here is that like you kind of have 952s - like a parallel work queue on your GPU. 955s - And so what the CPU does is that it\\'s only responsible 958s - for scheduling work on your work queue 960s - and then you know, the GPU executes work from the work queue 963s - and I think of it generally as like this GIF 966s - is what usually comes to mind and basically you can imagine 969s - that like the dog is like, 971s - or Gromit is like Python, you know, they\\'re like, you know, 974s - trying to put down the train track in front of the train, 976s - which is the GPU. 977s - And so, you know, as long as like Gromit is able 980s - to put down the train tracks faster than the train 983s - actually rolls along the train tracks, 985s - you can actually kind of view Python 987s - as like having zero overhead. 989s - Like it doesn\\'t provide any extra cost compared to, 993s - you know, if Python was like in a more efficient language 995s - like C++. 998s - And so in this way like you know, 999s - eager execution not only had like a much easier 1002s - to understand program model for users, 1004s - it was also like basically just as fast 1007s - as like non eager execution. 1010s - Unfortunately, you know, good things that never last. 1013s - And in 2017 like NVIDIA introduced 1016s - what are called like tensor cores. 1019s - And if you guys are unfamiliar with tensor cores, 1021s - they\\'re basically like hardware units on the GPUs 1024s - that only do matmul operations. 1026s - Like, and I don\\'t mean this like figuratively in the sense 1030s - that like people often say that GPUs are like 1031s - well suited for matmuls. 1033s - I mean this like very literally 1035s - in that like there\\'s actually an assembly instruction 1037s - that just does like a mini matmul. 1039s - And this is how you like interact with the tensor cores. 1042s - And so if you look at this like plot of like the amount 1045s - of like matmul flops versus non-matmul flops, 1048s - you can really see like when NVIDIA realized 1052s - that like deep learning was a big deal. 1054s - Because like all, all of a sudden, you know, 1055s - you kind of had this like massive like 10x gap 1058s - and so there\\'s like a log scale. 1060s - And you had this kinda like massive like 10x gap 1062s - between how fast matmuls were on the GPU 1065s - and how fast like literally anything else 1067s - you wanted to run on the GPU was. 1071s - And so the end result is, you know, previously we said 1073s - that like, you know, matmuls took like 90% of the time 1076s - and so if the, if like, you know, NVIDIA has fed up matmuls 1079s - by like 10x but then everything else like stayed 1082s - the same amount of speed, 1083s - then all of a sudden, you know, like you\\'re spending 1086s - a lot more of your time doing like non-matmul operations. 1090s - And so as a result we\\'ve kind of gotten like ML compilers 1095s - I think largely due to this change. 1097s - And so one of the, I think the important details 1099s - about ML compilers like, you know, 1101s - in terms of like how they differ from the frameworks 1102s - that came before is that ML frameworks still keep 1106s - like the eager programming model and that like the code 1109s - that you write like logically the program model 1111s - exposed to users is that you\\'re still just writing 1113s - Python code and executes like line by line. 1116s - The only difference now is that instead 1117s - of actually executing line by line, 1119s - we kind of captured into like a graph in some manner. 1123s - And so Torch compile I think actually kind of does this 1125s - in a pretty interesting way. 1127s - And that torch compile actually like intercepts 1131s - at like the Python by code interpreter level. 1134s - Where Python kind of exposes these APIs 1136s - where you can kind of insert 1137s - your like own frame interpreter. 1139s - And so this looks very much just like a traditional 1141s - like you know, Git for like any kind of other VM. 1144s - except this Git is kind of, you know, 1145s - only meant for like PyTorch programs. 1150s - And so if you kind of look at like, you know 1152s - how like things have evolved over time. 1155s - Originally you kind of had like frameworks like TensorFlow 1157s - or Caffe before that. 1159s - Where both the user like program model that users wrote 1162s - was like a graph builder type attraction. 1165s - But then the execution programming model 1166s - was also a graph execution type extraction. 1169s - And then like after that you kind of had PyTorch, you know, 1172s - one type like stuff. 1175s - Where the user program model now switched 1176s - to like an eager style execution, but the execution program 1179s - and the execution program model was also eager where like, 1182s - you know, each operator\\'s executed one at a time. 1186s - But kind of now finally, you know, 1187s - modern ML frameworks like, 1189s - pretty much all ML frameworks nowadays 1191s - use like an imperative eager programming model. 1194s - But almost all ML frameworks now also have some way 1197s - to like capture this program model into a graph of some kind 1201s - so they can perform optimizations. 1202s - And so this is like, you know, JAX.git, Tonnegrad, 1205s - this is like MLX. 1207s - They kind of all have their like different approaches 1208s - for capturing the graph to optimize. 1216s - And so I think next I kinda wanna talk about, you know, 1218s - we\\'ve kind of discussed how we\\'ve gotten to ML compilers 1222s - in the first place. 1223s - And so I think next I wanna talk about like 1225s - what ML compilers are actually doing for you 1228s - and what kind of optimizations they\\'re performing. 1231s - And so generally speaking, the way I think about 1234s - deep learning performance or like performance on GPUs 1236s - in general is that there\\'s basically three things 1238s - you can be spending your time on. 1240s - The first one is compute, so this is time on our GPU 1243s - competing like actual floating point operations. 1246s - The next one is a memory which is, you know, 1248s - time spent transferring your tensors within a GPU. 1252s - So this is like, you know, across various memory subsystems 1255s - in your GPU. 1256s - And so finally like overhead, 1258s - which is like everything else like you know, 1259s - it\\'s like time your GPU spending idle and so on. 1263s - And so first we\\'re gonna talk about compute. 1266s - And so I think to a sum of approximation you can say 1270s - that all runtime on your GPU is either compute 1272s - or it\\'s a shuffling data. 1274s - And that like, you know, data movement 1277s - is like not like a real operation, right? 1280s - it\\'s like a no op from like the theoretical point of view. 1284s - All it\\'s doing is it\\'s moving data from one place 1286s - where it\\'s convenient to another place 1287s - where it\\'s convenient. 1288s - And so basically a floating point operation 1290s - is like the only real thing a GPU can do. 1294s - But you can actually, I think simplify this even more 1296s - and say that in reality actually nowadays like all runtime 1300s - is either like matmuls or essentially shuffling data. 1303s - And so this is like, because if you look at the actual like 1306s - flop chart on like an Edge 100 like GPU, you can see here 1310s - that like the FP32 flops is like 1313s - you only have 67 teraFLOPS of FB32 compute, 1317s - but you actually have like a 1,00 teraFLOPS 1319s - of a TF32 compute. 1321s - Which is basically like matrix multiplication compute. 1324s - And so what this essentially... 1326s - And sometimes like what this means 1327s - is that if you\\'re not doing measurable (indistinct) 1329s - on your GPU, you\\'re really only like getting like 7% 1334s - of like your peak FLOP utilization. 1336s - And so like, you know by like the metric 1337s - that I mentioned before like model FLOP utilization, 1340s - even if your GPU was fully occupied doing stuff 1343s - that wasn\\'t a matmul you could only ever get like 7% 1346s - of FLOP utilization, which is like much lower than, 1349s - you know, our theoretical peak. 1355s - I did have a brief interlude about like, 1357s - I think an interesting case where like 1359s - these kind of abstractions do break down even more. 1362s - And so I do have like a kind of a fun question. 1365s - Which is like, do the matrix contents 1367s - affect your matrix multiplication performance? 1370s - And so I think, you know, if you kind of like, 1373s - you know, are familiar with like, you know, 1375s - general performance, there are like a lot of things that... 1378s - A lot of ways where like data can impact your performance, 1380s - but in this case matmuls actually avoid a lot of them. 1383s - So for example, they have identical memory access patterns 1385s - regardless of like what data is in your tensor, 1388s - there\\'s like no control flow in the matmul. 1391s - And the GPU also don\\'t have like the denormals. 1393s - So like that\\'s like not a possibility as well. 1397s - So if you like, you know, 1398s - and in this case where like taking three tensors, 1400s - we\\'re initializing them with like all zeros. 1403s - Like a tensor initialized from the Gaussian distribution 1406s - and then a tensor initialized 1408s - from like a uniform distribution, 1409s - which is like from zero to one. 1410s - And so funnily enough, if you benchmark this, 1413s - you actually find that there is a performance difference 1416s - depending on the actual data that is within your tensors. 1421s - And so there\\'s this tweet from some of you guys might know, 1426s - a long time ago that really I think 1428s - like when I first saw this. 1429s - I actually was very much reminded of this tweet, 1432s - where, you know, I thought I knew how like a GPU worked, 1435s - but then I was like very confused about 1437s - what could possibly be causing 1439s - these like performance differences like, you know, 1441s - between the different data. 1443s - And so the actual cause here is something called 1446s - like leakage power or dynamic power where I think, you know, 1449s - most of you\\'re probably familiar that you know, 1452s - like when a CPU or GPU is under load, 1454s - it uses more like power 1456s - and at some point it can like throttle, you know, 1458s - it\\'s like using the maximum amount of power can use 1461s - or the max amount of like a heat it\\'s allowed. 1465s - But the actual thing is that like, you know, 1466s - this power doesn\\'t just like come from nowhere. 1469s - It actually largely comes from what\\'s called 1471s - like dynamic or switching power. 1474s - And what this means is like every time a transistor 1476s - like on the GPU switches from like zero to one 1479s - or like, you know, high to low or low to high, 1481s - it like loses a little bit of this power. 1484s - And so the actual like power usage on your GPU 1486s - is kind of like a like sum across the total amount 1489s - of like switching that goes on in your GPU. 1493s - And so this is why like if you\\'re multiplying 1495s - with all zeros, you can imagine 1496s - that like your GPU ends up not, 1498s - like a lot of transistors don\\'t end up switching at all. 1501s - And so like it doesn\\'t actually consume that much power 1503s - and it\\'s much less a throttle. 1506s - And so if you actually like look at this, 1509s - you can actually get like very different performance 1510s - for like all these different kind of fund distributions. 1513s - Like whether it\\'s like, you know, the normal distribution 1516s - or whether it\\'s like a checkerboard type like pattern 1519s - or you know, it\\'s like sparse or ternary. 1521s - And the reason why is just like, 1523s - it\\'s like this kind of abstract thing 1525s - where these different patterns lead to like more or less 1527s - transistor flips and which leads to like more or less 1531s - of power throttling, which leads to like more or less 1534s - of performance. 1535s - And so I remember actually one time somebody had told me 1538s - a funny story where like they were like training 1541s - the machine learning model and benchmarking performance. 1544s - And then they like at some point their model would nad, 1547s - and then they\\'d be like wow my performance 1549s - just got way better (chuckles). 1551s - And so I like wrote an article about this 1553s - and they like messaged me and they were like, oh you know, 1555s - that was very illustrative 1557s - because you know, I was really confused 1558s - why my performance would be getting better. 1560s - But that\\'s why, you know, like if all your tensors are NaN, 1563s - your transistors also don\\'t need to do a lot of flipping 1565s - and so you know, you\\'ll measure like a better perf. 1570s - So that\\'s kind of compute. 1571s - And so the next thing that your GPU can be spending 1574s - a lot of your time on is like memory, 1576s - which is essentially a time spent transferring your tensors 1579s - within a GPU. 1585s - And so I think one thing to observe 1587s - from this kind of empirical plot 1590s - from a paper on like a data movement is that although like, 1595s - so this paper kind of breaks down the operations 1598s - on like I think a Bert type model. 1601s - Into what it calls like tensor contractions, 1603s - i.e memory locations. 1605s - And then like, you know, normalization operations 1607s - and element wise operations. 1609s - And so you can see that although major (indistinct) 1612s - are responsible for like 99.8% of your FLOPS, 1615s - they\\'re only responsible for 61% of your runtime. 1618s - And so you know where like why are we spending like, 1621s - you know, 40% of our runtime doing like operations 1624s - like cumulatively only take 0.2% of our FLOPS? 1629s - The kind of a key thing here is what\\'s called 1632s - like a memory bandwidth cost. 1634s - Where the way I typically think about this, 1637s - is that like even like, and so here I\\'m talking like 1640s - all of your data already lives on the GPU, like, you know, 1642s - it\\'s like, you know, it\\'s like, you know, 1643s - occupying your like GPU\\'s VRAM. 1648s - But the thing is that like your GPU\\'s VRAM 1650s - is not where it like the compute units are located. 1654s - And in order to actually do operations on a GPU, 1656s - you need to move your data from like the VRAM 1659s - to like where the compute units are located, 1662s - which is like your SRAM or like compute units. 1665s - And so usually I kind of think this of like 1667s - as like a factory where you have like a factory 1669s - with like not that much space. 1671s - And then you have a warehouse located like much further away 1675s - and so you have like a lot more space in your warehouse, 1678s - but now in order to do any operations 1680s - like on those like supplies, 1683s - you need to move them from your warehouse 1685s - to your factory and then back. 1688s - And so this cost of like moving data around 1690s - is called the memory bandwidth cost. 1693s - And so this is actually like responsible 1695s - for like a lot of like what your GPU 1697s - is spending its time doing. 1699s - Where if you imagine like let\\'s say we do like, you know, 1701s - three operations like on a GPU, 1704s - so maybe you\\'re doing like an add, 1705s - and then like a, you know, RELU, 1707s - and then like a sign operation. 1710s - Like you can imagine that what actually happens 1712s - when you do these operations. 1713s - Like first the GPU sends the data from like the memory 1717s - to the compute units and then you know, 1719s - turns it from a square to a triangle 1720s - and then it sends it all the way back. 1722s - And then you know, it sends the triangle 1724s - from like the memory units to the compute units again 1727s - where it does like another operation 1728s - and then sends it all the way back. 1730s - And then finally, you know, you guys get the idea, 1731s - it\\'s like sending the circle from the memory units 1733s - to the compute units again 1734s - and then it\\'s like sending into all the way back. 1737s - And so by default whenever you run any operations 1739s - like in PyTorch like let\\'s say you ran like add 1741s - and then multiply and then cosine, 1743s - this is exactly what would be happening on our GPU. 1747s - And so you might think that this is a very like dumb thing 1749s - to do and you would be correct. 1753s - And that, you know, why are we like sending our triangle 1755s - back from like the factory to the warehouse 1758s - just to send the data from like the warehouse 1760s - back to the factory again. 1762s - And so a very like common operation for GPU compilers to do, 1767s - and I\\'d say what I\\'d actually call 1768s - like the most important optimization 1770s - in a deep learning compiler by far, 1772s - is called operator fusion. 1774s - And so what an operator fusion does is that instead of like, 1777s - you know, sending the data back and forth so much. 1779s - we like do a single GPU kernel where you send the data once 1783s - to the factory units, you do all of the operations 1787s - and then you send the data back. 1790s - Also, notably, this is also like an issue. 1794s - This optimization is not really something you can do 1795s - in eager mode, right? 1797s - Because in eager mode I was, you know, mentioning 1799s - like the executed model is very simple 1801s - where you like run our operation 1803s - and then it executes the operation. 1804s - And now if we want to do this optimization, 1807s - that program model is like no longer sufficient. 1811s - And so there\\'s actually like a lot of different ways 1813s - to minimize memory movement. 1815s - Although at the end of the day, 1816s - like operator fusion is like the most important thing 1819s - you can do for like a ML compiler. 1821s - There\\'s actually like a lot of decisions 1823s - that go into operator fusion that like kind of enable it 1826s - to be more or less effective. 1828s - One of the kind of examples here 1830s - is kind of these like re computation 1832s - versus reuse trade-offs. 1833s - If you guys are kind of familiar with maybe 1835s - like register allocation type settings, 1837s - you kind of often have a similar issue 1839s - where like if you have a register you can choose 1842s - to either like store it in global memory 1845s - and then load from it later, 1846s - or you can just choose to like recompute 1848s - that value from like values that are ready in the registers. 1852s - And so you kind of have a similar idea here 1855s - where you oftentimes have cases where by doing some amount 1859s - of re computation you can significantly reduce your memory. 1863s - You can significantly reduce your number of memory accesses. 1866s - And so in this way, like the recomputation 1869s - can not only like reduce your like peak memory usage, 1872s - it can also often like improve 1873s - your actual runtime performance. 1877s - And so I think one of the things 1880s - to mention actually about like why. 1882s - So this observation actually ends up being 1884s - like quite important for deep learning performance 1885s - like re computation versus reuse. 1887s - And I think the reason why is that like the shape 1890s - of machine learning programs, 1891s - actually I think it looks quite unusual 1893s - compared to like your typical program that you might have. 1896s - Where like it\\'s generally like a kind of a bit of an axiom 1900s - in like programs that usually most intermediates 1902s - that you have are very short-lived. 1904s - So like, you know, your program generally consists 1906s - of a lot of very short-lived intermediates 1907s - that you know are created 1908s - and they\\'re very shortly destroyed. 1911s - But in machine learning this is actually not the case. 1913s - Because in machine learning, like the typical like model 1916s - that you\\'ll execute will first like run the model 1919s - forward like, you know, layer zero to layer one, 1922s - to layer two, to layer three, to layer four, 1924s - and then initially run what\\'s called the backwards pass, 1926s - which will like run the layers in reverse. 1929s - So then it\\'ll go from like layer four to layer three, 1931s - to layer two, to layer one and to layer zero. 1934s - And in between the forward pass 1936s - and the backwards pass you need to save 1938s - what are called like intermediates or activations. 1941s - And so these are like a lot of, like, 1944s - you have a lot of them. 1945s - And they\\'re like often times like, you know, 1947s - largely responsible for like running into 1950s - like out of memory type errors. 1952s - And so I think this is actually 1953s - like kind of a pretty unusual like program structure 1957s - in machine learning that\\'s caused like, you know, 1960s - back propagation and gradient dissent. 1965s - And so finally, you know, the last thing 1967s - you can be spending your time on is overhead. 1969s - Where you can imagine that, 1970s - you know, if a poor Gromit is not able 1973s - to put down the train tracks faster than the train 1975s - can like go on the train tracks, 1977s - then sometimes a train is gonna be just stuck 1980s - waiting for him to put down like the next train track. 1982s - And so here I have like a profile trace 1985s - where you can see that like the bottom line, 1987s - which is a GPU trace is largely idle 1990s - and it\\'s mostly just idle waiting for the CPU 1992s - to like schedule the next operation. 1996s - And so there are a lot of like ways 1997s - to actually address this nowadays. 1999s - One of the most powerful is called CUDAgraphs, 2001s - which is like an NVIDIA provided API, 2004s - but you also have like other approaches like in a compiler 2006s - for example, like codegenning like a lower overhead wrapper 2009s - or something like this. 2017s - So you know, I\\'ve talked about like ML compilers like, 2021s - and you know like you know what they can do to your program 2023s - and how they can be useful. 2024s - But I think kind of like an interesting question 2027s - that you often see like, you know, I talked a lot about 2029s - how like, you know, we have like, you know, 2032s - super massive infra build out 2034s - and the programs are super simple. 2035s - And we\\'ve seen like a lot of consolidation 2037s - in terms of like what the architecture looks like. 2040s - And so I think like a reasonable question is like, 2042s - if you only have like one architecture 2043s - and you\\'re spending like billions of dollars to train it, 2046s - why do you even need a compiler? 2049s - You know, like why can\\'t you just like, you know, 2050s - assign some group of people to like optimize it by hand 2053s - instead of like a leveraging a compiler? 2058s - And I\\'m gonna say some like kind of or sorry, 2062s - and the other thing I\\'ll say about this 2065s - actually is that like in practice a lot of times 2068s - people do not use compilers like for kind of this reason. 2072s - And so this section is gonna be talking a little bit about 2074s - like why I think that\\'s a case. 2077s - And what are kind of some of the challenges 2079s - when it comes to like using compilers in this setting. 2083s - And yeah, so disclaimer, you know, 2084s - I do really like compilers. 2085s - I\\'m gonna say some kind of mean things about compilers 2088s - in a bit, but you know, as like to establish my credibility, 2094s - you know, I work on a team called Fighter\\'s Compilers. 2096s - And so there are like, to be clear like a lot of reasons 2098s - why compilers can be very useful. 2101s - In particular this kinda like notion of leverage, 2103s - like being able to do the optimization once in the compiler 2106s - and then having everybody be able to like take advantage 2108s - of it. 2110s - And also, you know, compilers are also just like very fun 2112s - to work on. 2114s - That being said, yeah, I\\'m gonna introduce you. 2117s - You know, my new exciting library, 2120s - Horace\\'s exciting library abbreviated at HEL. 2124s - And so it has a couple of cool features 2125s - that you might be interested in. 2127s - So the first feature it has is that it doesn\\'t always work. 2131s - And you know, to address that, 2133s - it also has no documentation about 2135s - why or when it will work except by reading my library 2138s - as an implementation. 2139s - And in exchange for that, when you update the library, 2142s - it may totally change what code works 2144s - and what code doesn\\'t work. 2145s - I.e. like no backwards compatibility or like guarantees 2149s - on your like, you know, on whether your code works. 2154s - And so are you interested in using my library? 2158s - I guess, you know, most people would probably say no. 2161s - And so I think one thing to note here 2163s - that like if work means like has a desired performance 2166s - and is applying the desired optimizations, 2168s - this is kind of largely describing 2170s - how compiler optimizations work. 2172s - And that compiled optimizations don\\'t always work. 2175s - Often like there\\'s no real documentation 2178s - on when a compiler optimization will trigger and you know, 2181s - when you update your compiler 2183s - it may completely change when it does 2186s - or does not apply these optimizations. 2190s - And so there\\'s like very influential article 2194s - like about this compiler called ISPC. 2197s - And they have this note here called auto vectorization 2199s - is not a programming model. 2201s - Where they note here is that like, you know, 2203s - the problem with an auto vectorizer, 2205s - which is kind of like a compiler... 2206s - Or so the overall like framing of the article 2208s - is that he wrote this compiler called like ISPC, 2211s - which you can think of as like CUDA 2213s - for intel SIMD instructions. 2216s - And he kind of, you know, is constantly like, you know, 2218s - as part of the article, he is constantly trying to fight 2220s - against like the intel compiler team, 2223s - which is like where he works. 2224s - With the intel compiler team, 2225s - wanted to kind of leverage auto vectorization 2228s - to get vectorization done 2229s - instead of introducing like a new program model in ISPC. 2233s - And so he kind of, I think elucidates like what the problem 2236s - with the auto vectoriser. 2238s - Which is that the problem with the auto vectoriser 2239s - is that as long as vectors can fail 2241s - and it will then if you\\'re programmer 2243s - that actually cares about what code the compiler 2245s - generates from your program, 2246s - you need to deeply understand the auto vectoriser. 2250s - Then when it fails to vectorize code, 2252s - you wanna be vectorized, you need 2254s - to either poke it in the right way 2255s - or change your program in the right way 2256s - so that it works for you again. 2258s - And so this is like a very horrible way to program. 2260s - And then, you know, if most of you are, 2262s - or if any of you here are like very like into using 2265s - SIMD instructions, you probably also do not trust 2267s - the auto vectorize at all and you\\'re mostly just, you know, 2269s - writing intrinsics. 2272s - And so the, you know, with the proper program model, 2276s - ideally the user is able to like learn what does 2279s - and does not work without, you know, needing to be tied 2282s - to the implementation and then, you know, 2284s - one compiler implements it 2285s - and then the user can like learn to reliably rely 2287s - on this optimization without needing to understand 2290s - the compiler and only needing to understand 2292s - the programming model. 2294s - And so one I guess way you can phrase this 2296s - is that like a compiler optimization 2298s - that always works is just part of the programming model. 2301s - Like for example, when you\\'re writing like, you know, 2303s - SIMD instructions, you know that as SIMD intrinsic 2307s - will always get mapped to SIMD instruction. 2309s - And so that\\'s like part of your program model 2311s - and not really an optimization that the compiler is doing. 2316s - So I think, you know, to... 2319s - Like when I see like the people kind of complaining 2322s - about like Shoggoths and working with them, 2325s - I kind of am often reminded times of a compiler. 2328s - We can imagine that like a compiler is oftentimes 2330s - it\\'s like large piece of code that you know 2333s - has been worked on by a lot of like very smart people 2335s - and oftentimes has a lot of like tricky details 2338s - and implementation details in it. 2341s - And so ideally when you\\'re doing a compiler, 2343s - like only the program model ends up being exposed 2345s - to the user. 2346s - So like the actual compiler implementation 2347s - ends up being completely hidden. 2349s - And so the user only needs to deal 2350s - with like the nice program model, 2351s - but it\\'s usually just like the language 2353s - that the compiler is compiling from. 2361s - Unfortunately, you know, when compilers fail 2363s - and like, you know, they don\\'t apply the optimizations 2365s - that you want them to apply. 2366s - The kind of entire thing becomes exposed. 2369s - And so, you know, this kind of applies anytime 2371s - that you\\'re like you\\'re wrestling with a compiler 2373s - and you\\'re trying to understand why the compiler 2375s - like did or did not like inline my code or things like this. 2381s - And so to give some examples of like cases 2382s - where like very kind of nuanced details here. 2386s - Can lead to compilers like having, 2388s - like it can lead to compilers struggling a lot. 2390s - One of them here is like numerics and machine learning. 2393s - Where numerics can be like a kind of a, 2395s - or like in general floating point, 2397s - like arithmetic is like a very cursed thing 2399s - to deal with generally speaking. 2401s - And it\\'s just gotten even worse with the fact 2405s - that like everybody in the industry 2406s - keeps on pushing our data types lower 2408s - to lower and lower bits where like on the V100 2411s - they kind of introduce like 16 bit operations 2414s - is kind of the default. 2415s - And on H100 they introduced eight bit operations 2418s - and on the B100, you know, they\\'re now pushing 2420s - for like four bit operation, like four bit floats. 2423s - And operations on four bit floating point numbers. 2425s - I think it\\'s reasonable to question like 2428s - how is this even a floating point number at this point? 2431s - But this is kind of what we\\'re typically dealing with. 2436s - And so as a result of like this kind of like low precision 2438s - and the fact that numerics end up being so subtle, 2440s - the algorithms have like very annoying numerical problems 2443s - to deal with. 2443s - And I think a good example for me 2445s - that was like very frustrating was this kind of a NaN 2448s - in like a FlashAttention implementation. 2450s - Where the underlying cause here is something called an FMA 2453s - or like a fuse multiply accumulate. 2455s - It can actually be like disastrously bad 2458s - when it comes to your numerics. 2461s - So for folks who are unfamiliar with FMA, 2463s - it\\'s basically like it takes like A, B and C 2465s - and it does like a signal operation 2467s - that does A times B plus C. 2470s - And so one of the ways that FMA differs 2472s - from normal operations is that in addition to being faster, 2476s - it\\'s also usually computed 2478s - in what people call like infinite precision internally.']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_transcript(cleaned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
